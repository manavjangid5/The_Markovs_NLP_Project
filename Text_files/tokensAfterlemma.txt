Undergraduate
Topics
in
Computer
Science
UTiCS
delivers
high-quality
instructional
content
for
under-
graduate
studying
in
all
area
of
computing
and
information
science
From
core
foundational
and
theoreti-
cal
material
to
final-year
topic
and
application
UTiCS
book
take
a
fresh
concise
and
modern
approach
and
are
ideal
for
self-study
or
for
a
one-
or
two-semester
course
The
text
are
all
authored
by
established
expert
in
their
field
reviewed
by
an
international
advisory
board
and
contain
numerous
example
and
problem
Many
include
fully
worked
solution
Department
of
Computer
Science
and
Engineering
IIT
Delhi
India
Ian
Mackie
E´
cole
Polytechnique
France
and
University
of
Sussex
UK
Samson
Abramsky
University
of
Oxford
UK
Chris
Hankin
Imperial
College
London
UK
Dexter
Kozen
Cornell
University
USA
Andrew
Pitts
University
of
Cambridge
UK
Hanne
Riis
Nielson
Technical
University
of
Denmark
Denmark
Steven
Skiena
Stony
Brook
University
USA
David
Zhang
The
Hong
Kong
Polytechnic
University
Hong
Kong
Undergraduate
Topics
in
Computer
Science
ISSN
1863-7310
ISBN
978-1-84800-301-9
e-ISBN
978-1-84800-302-
10.1007/978-1-84800-302-6
A
catalogue
record
for
this
book
is
available
from
the
British
Library
Library
of
Congress
Control
Number
2008933221
Apart
from
any
fair
dealing
for
the
purpose
of
research
or
private
study
or
criticism
or
review
a
permitted
under
the
Copyright
Designs
and
Patents
Act
1988
this
publication
may
only
be
reproduced
stored
or
transmitted
in
any
form
or
by
any
mean
with
the
prior
permission
in
writing
of
the
publisher
or
in
the
case
of
reprographic
reproduction
in
accordance
with
the
term
of
license
issued
by
the
Copyright
Licensing
Agency
Enquiries
concerning
reproduction
outside
those
term
should
be
sent
to
the
publisher
The
use
of
registered
name
trademark
etc.
in
this
publication
doe
not
imply
even
in
the
absence
of
a
specific
statement
that
such
name
are
exempt
from
the
relevant
law
and
regulation
and
therefore
free
for
general
use
The
publisher
make
no
representation
express
or
implied
with
regard
to
the
accuracy
of
the
information
contained
in
this
book
and
can
not
accept
any
legal
responsibility
or
liability
for
any
error
or
omission
that
may
be
made
An
introductory
course
on
Software
Engineering
remains
one
of
the
hardest
subject
to
teach
largely
because
of
the
wide
range
of
topic
the
area
encom-
pass
I
have
believed
for
some
time
that
we
often
tend
to
teach
too
many
concept
and
topic
in
an
introductory
course
resulting
in
shallow
knowledge
and
little
insight
on
application
of
these
concept
And
Software
Engineering
is
finally
about
application
of
concept
to
efficiently
engineer
good
software
solution
I
believe
that
an
introductory
course
on
Software
Engineering
should
focus
on
imparting
to
student
the
knowledge
and
skill
that
are
needed
to
successfully
execute
a
commercial
project
of
a
few
person-months
effort
while
employing
proper
practice
and
technique
It
is
worth
pointing
out
that
a
vast
majority
of
the
project
executed
in
the
industry
today
fall
in
this
scope—executed
by
a
small
team
over
a
few
month
I
also
believe
that
by
carefully
selecting
the
concept
and
topic
we
can
in
the
course
of
a
semester
achieve
this
This
is
the
motivation
of
this
book
The
goal
of
this
book
is
to
introduce
to
the
student
a
limited
number
of
concept
and
practice
which
will
achieve
the
following
two
objective
Teach
the
student
the
skill
needed
to
execute
a
smallish
commercial
project
Provide
the
student
necessary
conceptual
background
for
undertaking
ad-
vanced
study
in
software
engineering
through
course
or
on
their
own
I
have
included
in
this
book
only
those
concept
that
I
believe
are
founda-
tional
and
through
which
the
two
objective
mentioned
above
can
be
met
Ad-
vanced
topic
have
been
consciously
left
out
As
executing
a
software
project
requires
skill
in
two
dimensions—engineering
and
project
management—this
book
focus
on
key
task
in
these
two
dimension
and
discus
concept
and
technique
that
can
be
applied
to
effectively
execute
these
task
The
book
is
organized
in
a
simple
manner
with
one
chapter
for
each
of
the
key
task
in
a
project
For
engineering
these
task
are
requirement
analy-
si
and
specification
architecture
design
module
level
design
coding
and
unit
testing
and
testing
For
project
management
the
key
task
are
project
plan-
ning
and
project
monitoring
and
control
but
both
are
discussed
together
in
one
chapter
on
project
planning
a
even
monitoring
ha
to
be
planned
In
addi-
tion
the
book
contains
one
chapter
that
clearly
defines
the
problem
domain
of
Software
Engineering
and
another
chapter
that
discus
the
central
concept
of
software
process
which
integrates
the
different
task
executed
in
a
project
Each
chapter
open
with
some
introduction
and
then
clearly
list
the
chapter
goal
or
what
the
reader
can
expect
to
learn
from
the
chapter
For
the
task
covered
in
the
chapter
the
important
concept
are
first
discussed
followed
by
a
discussion
of
the
output
of
the
task
the
desired
quality
property
of
the
output
and
some
practical
method
and
notation
for
performing
the
task
The
explanation
are
supported
by
example
and
the
key
learning
are
summarized
in
the
end
for
the
reader
The
chapter
end
with
some
self-assessment
exercise
The
book
is
primarily
intented
for
an
introductory
course
on
Software
Engi-
neering
in
any
undergraduate
or
graduate
program
It
is
targeted
for
student
who
know
programming
but
have
not
had
a
formal
exposure
to
software
engi-
neering
The
book
can
also
be
used
by
professional
who
are
in
a
similar
state—know
some
programming
but
want
to
be
introduced
to
the
systematic
approach
of
software
engineering
Though
the
book
is
self-contained
some
teaching
support
and
supplemental
resource
are
available
through
a
website
The
URL
is
The
resource
available
on
the
site
include
The
powerpoint
presentation
for
each
chapter
in
ppt
format
so
instructor
can
change
them
to
suit
their
style
Various
template
for
different
output
in
a
project
that
can
be
used
for
the
student
project
in
the
course
A
case
study
with
most
of
the
major
output
of
the
project
Some
practice
exercise
for
unit
testing
and
inspection
I
would
like
to
express
my
gratitude
to
my
editor
Wayne
Wheeler
who
con-
ceived
this
idea
of
a
concise
introductory
book
and
created
this
opportunity
I
would
also
like
to
express
my
thanks
to
my
wife
Shikha
and
my
daughter
Sumedha
and
Sunanda
for
once
again
bearing
with
my
mood
and
odd
hour
Ask
any
student
who
ha
had
some
programming
experience
the
following
question
You
are
given
a
problem
for
which
you
have
to
build
a
software
system
that
most
student
feel
will
be
approximately
10,000
line
of
say
C
or
Java
code
If
you
are
working
full
time
on
it
how
long
will
it
take
you
to
build
this
system
The
answer
of
student
is
generally
And
given
the
program-
ming
expertise
of
the
student
there
is
a
good
chance
that
they
will
be
able
to
build
the
software
and
demo
it
to
the
professor
within
With
the
productivity
of
the
student
will
be
5000
line
of
code
LOC
per
person-month
Now
let
u
take
an
alternative
scenario—we
act
a
client
and
pose
the
same
problem
to
a
company
that
is
in
the
business
of
developing
software
for
client
Though
there
is
no
standard
productivity
figure
and
it
varies
a
lot
it
is
fair
to
say
a
productivity
figure
of
1000
LOC
per
person-month
is
quite
respectable
though
it
can
be
a
low
a
100
LOC
per
person-month
for
embedded
system
With
this
productivity
a
team
of
professional
in
a
software
organization
will
take
10
person-months
to
build
this
software
system
Why
this
difference
in
productivity
in
the
two
scenario
Why
is
it
that
the
same
student
who
can
produce
software
at
a
productivity
of
a
few
thousand
LOC
per
month
while
in
college
end
up
producing
only
about
a
thousand
LOC
per
month
when
working
in
a
company
The
answer
of
course
is
that
two
different
thing
are
being
built
in
the
two
scenario
In
the
first
a
student
system
is
being
built
which
is
primarily
meant
for
demonstration
purpose
and
is
not
expected
to
be
used
later
Because
it
is
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
not
to
be
used
nothing
of
significance
depends
on
the
software
and
the
presence
of
bug
and
lack
of
quality
is
not
a
major
concern
Neither
are
the
other
quality
issue
like
usability
maintainability
portability
etc
On
the
other
hand
an
industrial-strength
software
system
is
built
to
solve
some
problem
of
a
client
and
is
used
by
the
client
’
s
organization
for
operating
some
part
of
business
and
a
malfunction
of
such
a
system
can
have
huge
impact
in
term
of
financial
or
business
loss
inconvenience
to
user
or
loss
of
property
and
life
Consequently
the
software
system
need
to
be
of
high
quality
with
respect
to
property
like
reliability
usability
portability
etc
This
need
for
high
quality
and
to
satisfy
the
the
end
user
ha
a
major
impact
on
the
way
software
is
developed
and
it
cost
The
rule
of
thumb
Brooks
give
suggests
that
the
industrial-strength
software
may
cost
about
10
time
the
student
software
16
The
software
industry
is
largely
interested
in
developing
industrial-strength
software
and
the
area
of
software
engineering
focus
on
how
to
build
such
system
That
is
the
problem
domain
for
software
engineering
is
industrial-
strength
software
In
the
rest
of
the
book
when
we
use
the
term
software
we
mean
industrial-strength
software
In
the
remainder
of
this
chapter
we
will
learn
That
quality
cost
and
schedule
are
the
main
force
that
drive
a
industrial-
strength
software
project
How
cost
and
productivity
are
defined
and
measured
for
such
a
project
and
how
quality
of
software
is
characterized
and
measured
That
large
scale
and
change
are
important
attribute
of
the
problem
domain
and
solution
approach
have
to
handle
them
Though
the
need
for
high
quality
distinguishes
industrial
strength
software
from
others
cost
and
schedule
are
other
major
driving
force
for
such
software
In
the
industrial-strength
software
domain
there
are
three
basic
force
at
play—cost
schedule
and
quality
The
software
should
be
produced
at
reasonable
cost
in
a
reasonable
time
and
should
be
of
good
quality
These
three
parameter
often
drive
and
define
a
software
project
Industrial-strength
software
is
very
expensive
primarily
due
to
the
fact
that
software
development
is
extremely
labor-intensive
To
get
an
idea
of
the
cost
involved
let
u
consider
the
current
state
of
practice
in
the
industry
Lines
of
code
LOC
or
thousand
of
line
of
code
KLOC
delivered
is
by
far
the
most
commonly
used
measure
of
software
size
in
the
industry
As
the
main
cost
of
producing
software
is
the
manpower
employed
the
cost
of
developing
software
is
generally
measured
in
term
of
person-months
of
effort
spent
in
development
And
productivity
is
frequently
measured
in
the
industry
in
term
of
LOC
or
KLOC
per
person-month
The
productivity
in
the
software
industry
for
writing
fresh
code
generally
range
from
few
hundred
to
about
1000+
LOC
per
person-month
This
produc-
tivity
is
over
the
entire
development
cycle
not
just
the
coding
task
Software
company
often
charge
the
client
for
whom
they
are
developing
the
software
be-
tween
$
3000
-
$
15,000
per
person-month
With
a
productivity
of
1000
LOC
per
person-month
it
mean
that
each
line
of
delivered
code
cost
between
$
$
15
And
even
small
project
can
easily
end
up
with
software
of
50,000
LOC
With
this
productivity
such
a
software
project
will
cost
between
$
150,000
and
Schedule
is
another
important
factor
in
many
project
Business
trend
are
dictating
that
the
time
to
market
of
a
product
should
be
reduced
that
is
the
cycle
time
from
concept
to
delivery
should
be
small
For
software
this
mean
that
it
need
to
be
developed
faster
and
within
the
specified
time
Unfortunately
the
history
of
software
is
full
of
case
where
project
have
been
substantially
late
Clearly
therefore
reducing
the
cost
and
the
cycle
time
for
software
de-
velopment
are
central
goal
of
software
engineering
Productivity
in
term
of
output
KLOC
per
person-month
can
adequately
capture
both
cost
and
sched-
ule
concern
If
productivity
is
higher
it
should
be
clear
that
the
cost
in
term
of
person-months
will
be
lower
the
same
work
can
now
be
done
with
fewer
person-months
Similarly
if
productivity
is
higher
the
potential
of
developing
the
software
in
le
time
improves—a
team
of
higher
productivity
will
finish
a
job
in
le
time
than
a
same-size
team
with
lower
productivity
The
actual
time
the
project
will
take
of
course
depends
also
on
the
number
of
people
al-
located
to
the
project
Hence
pursuit
of
higher
productivity
is
a
basic
driving
force
behind
software
engineering
and
a
major
reason
for
using
the
different
tool
and
technique
Besides
cost
and
schedule
the
other
major
factor
driving
software
engi-
neering
is
quality
Today
quality
is
one
of
the
main
mantra
and
business
strategy
are
designed
around
it
Unfortunately
a
large
number
of
instance
have
occurred
regarding
the
unreliability
of
software—the
software
often
doe
not
do
what
it
is
supposed
to
do
or
doe
something
it
is
not
supposed
to
do
Clearly
developing
high-quality
software
is
another
fundamental
goal
of
soft-
ware
engineering
However
while
cost
is
generally
well
understood
the
concept
of
quality
in
the
context
of
software
need
further
elaboration
The
international
standard
on
software
product
quality
55
suggests
that
software
quality
comprises
six
main
attribute
a
shown
in
Figure
These
attribute
can
be
defined
a
follows
Functionality
The
capability
to
provide
function
which
meet
stated
and
implied
need
when
the
software
is
used
Reliability
The
capability
to
provide
failure-free
service
Usability
The
capability
to
be
understood
learned
and
used
Efficiency
The
capability
to
provide
appropriate
performance
relative
to
the
amount
of
resource
used
Maintainability
The
capability
to
be
modified
for
purpose
of
making
cor-
rections
improvement
or
adaptation
Portability
The
capability
to
be
adapted
for
different
specified
environ-
ments
without
applying
action
or
mean
other
than
those
provided
for
this
purpose
in
the
product
With
multiple
dimension
to
quality
different
project
may
emphasize
dif-
ferent
attribute
and
a
global
single
number
for
quality
is
not
possible
How-
ever
despite
the
fact
that
there
are
many
quality
attribute
reliability
is
gen-
erally
accepted
to
be
the
main
quality
criterion
As
unreliability
of
software
is
due
to
the
presence
of
defect
in
the
software
one
measure
of
quality
is
the
number
of
defect
in
the
delivered
software
per
unit
size
generally
taken
to
be
thousand
of
line
of
code
or
KLOC
With
this
a
the
major
quality
criterion
the
quality
objective
is
to
reduce
the
number
of
defect
per
KLOC
a
much
a
possible
Current
best
practice
in
software
engineering
have
been
able
to
reduce
the
defect
density
to
le
than
To
determine
the
quality
of
a
software
product
we
need
to
determine
the
number
of
defect
in
the
software
that
wa
delivered
This
number
is
clearly
not
known
at
delivery
time
and
may
never
be
known
One
approach
to
measure
quality
is
to
log
the
defect
found
in
or
after
delivery
and
define
quality
with
respect
to
these
defect
This
mean
that
quality
of
delivered
software
can
only
be
determined
The
defect
density
can
however
also
be
estimated
from
past
data
of
similar
projects—if
similar
approach
are
being
used
then
it
is
expected
that
the
current
project
will
have
similar
defect
density
a
the
past
project
It
should
be
pointed
out
that
to
use
this
definition
of
quality
what
a
defect
is
must
be
clearly
defined
A
defect
could
be
some
problem
in
the
software
that
cause
the
software
to
crash
or
a
problem
that
cause
an
output
to
be
not
properly
aligned
or
one
that
misspells
some
word
etc
The
exact
definition
of
what
is
considered
a
defect
will
clearly
depend
on
the
project
or
the
standard
the
organization
developing
the
project
us
typically
it
is
the
latter
Besides
reliability
another
quality
attribute
of
great
interest
is
maintain-
ability
Once
the
software
is
delivered
and
deployed
it
enters
the
maintenance
phase
Why
is
maintenance
needed
for
software
when
software
ha
no
physical
component
that
can
degrade
with
age
Software
need
to
be
maintained
be-
cause
of
the
residual
defect
remaining
in
the
system
It
is
commonly
believed
that
the
state
of
the
art
today
is
limited
and
developing
software
with
zero
de-
fect
density
is
not
possible
These
defect
once
discovered
need
to
be
removed
leading
to
what
is
called
corrective
maintenance
Maintenance
is
also
needed
to
change
the
delivered
software
to
satisfy
the
enhanced
need
of
the
user
and
the
environment
leading
to
adaptive
maintenance
Over
the
life
of
a
software
system
maintenance
cost
can
far
exceed
the
cost
of
original
development
The
maintenance-to-development-cost
ratio
ha
been
variously
suggested
a
80:20
70:30
or
60:40
Due
to
this
high
cost
maintainability
attribute
of
delivered
software
is
of
high
interest—it
is
clearly
desirable
to
have
software
system
that
are
easier
to
maintain
Though
cost
schedule
and
quality
are
the
main
driving
force
for
a
project
in
our
problem
domain
of
industry
strength
software
there
are
some
other
char-
acteristics
of
the
problem
domain
that
also
influence
the
solution
approach
employed
We
focus
on
two
such
characteristics—scale
and
change
Most
industrial-strength
software
system
tend
to
be
large
and
complex
requiring
ten
of
thousand
of
line
of
code
Sizes
of
some
of
the
well-known
software
product
are
given
in
Table
As
can
be
expected
development
of
a
large
system
requires
a
different
set
of
method
compared
to
developing
a
small
system
a
the
method
that
are
used
for
developing
small
system
often
do
not
scale
up
to
large
system
An
example
will
illustrate
this
point
Consider
the
problem
of
counting
people
in
a
room
versus
taking
a
census
of
a
country
Both
are
essentially
counting
problem
But
the
method
used
for
counting
people
in
a
room
will
just
not
work
when
taking
a
census
A
different
set
of
method
will
have
to
be
used
for
conducting
a
census
and
the
census
problem
will
require
considerably
more
management
organization
and
validation
in
addition
to
counting
Similarly
method
that
one
can
use
to
develop
program
of
a
few
hundred
line
can
not
be
expected
to
work
when
software
of
a
few
hundred
thousand
line
need
to
be
developed
A
different
set
of
method
must
be
used
for
developing
large
software
Any
software
project
involves
the
use
of
engineering
and
project
manage-
ment
In
small
project
informal
method
for
development
and
management
can
be
used
However
for
large
project
both
have
to
be
much
more
rigorous
a
illustrated
in
Figure
In
other
word
to
successfully
execute
a
project
a
proper
method
for
engineering
the
system
ha
to
be
employed
and
the
project
ha
to
be
tightly
managed
to
make
sure
that
cost
schedule
and
quality
are
under
control
Large
scale
is
a
key
characteristic
of
the
problem
domain
and
the
solution
approach
should
employ
tool
and
technique
that
have
the
ability
to
build
large
software
system
Change
is
another
characteristic
of
the
problem
domain
which
the
ap-
proaches
for
development
must
handle
As
the
complete
set
of
requirement
for
the
system
is
generally
not
known
often
can
not
be
known
at
the
start
of
the
project
or
stated
a
development
proceeds
and
time
pass
additional
requirement
are
identified
which
need
to
be
incorporated
in
the
software
be-
ing
developed
This
need
for
change
requires
that
method
for
development
embrace
change
and
accommodate
it
efficiently
Change
request
can
be
quite
disruptive
to
a
project
and
if
not
handled
properly
can
consume
up
to
30
to
40
%
of
the
development
cost
14
As
discussed
above
software
ha
to
be
changed
even
after
it
ha
been
de-
ployed
Though
traditionally
change
in
software
during
maintenance
have
been
distinguished
from
change
that
occur
while
the
development
is
taking
place
these
line
are
blurring
a
fundamentally
the
change
in
both
of
these
scenario
are
similar—existing
source
code
need
to
be
changed
due
to
some
change
in
the
requirement
or
due
to
some
defect
that
need
to
be
removed
Overall
a
the
world
change
faster
software
ha
to
change
faster
even
while
under
development
Changes
in
requirement
are
therefore
a
characteris-
tic
of
the
problem
domain
In
today
’
s
world
approach
that
can
not
accept
and
accommodate
change
are
of
little
use—they
can
solve
only
those
few
problem
that
are
change
resistant
The
problem
domain
for
software
engineering
is
industrial-strength
software
This
software
is
meant
to
solve
some
problem
of
some
set
of
user
and
is
expected
to
be
of
high
quality
In
this
problem
domain
cost
schedule
and
quality
are
basic
driving
force
Hence
method
and
tool
that
will
be
used
for
solving
problem
in
this
domain
must
ensure
high
productivity
and
high
quality
Productivity
is
measured
a
amount
of
output
per
unit
of
input
resource
In
software
output
can
be
measured
in
term
of
line
of
code
delivered
and
a
human
time
is
the
main
resource
input
can
be
measured
a
person-months
Productivity
can
therefore
be
measured
a
line
of
code
delivered
per
person-
month
Software
quality
ha
many
attribute
which
include
functionality
reliabil-
ity
usability
efficiency
maintainability
and
portability
Reliability
is
often
considered
a
the
main
quality
attribute
and
a
unreliability
in
software
is
due
to
defect
in
the
software
quality
can
be
characterized
by
number
of
defect
per
thousand
line
of
code
The
problem
in
this
domain
often
tend
to
be
very
large
and
where
the
need
of
the
customer
change
fast
Hence
the
technique
used
for
developing
industrial-strength
software
should
be
such
that
they
are
capable
of
building
large
software
system
and
have
the
capability
to
handle
change
What
are
the
main
difference
between
a
student
software
and
industrial-strength
software
If
developing
a
program
for
solving
a
problem
requires
effort
E
it
is
estimated
that
an
industrial-strength
software
for
solving
that
problem
will
require
10E
effort
Where
do
you
think
this
extra
effort
cost
is
spent
What
measurement
will
you
take
in
a
project
to
measure
the
productivity
and
how
will
you
determine
the
productivity
from
these
measure
What
are
the
different
attribute
of
software
quality
If
for
an
accounting
soft-
ware
we
are
most
interested
in
ensuring
that
the
software
doe
not
make
any
computation
mistake
then
which
of
the
quality
attribute
should
we
be
most
concerned
about
What
are
some
of
the
project
management
task
that
you
will
do
differently
for
a
large
project
a
compared
to
a
small
project
How
will
your
execution
of
these
task
change
Suppose
change
are
to
be
made
to
a
software
system
that
is
in
operation
Why
will
change
to
such
a
system
cost
a
lot
more
than
just
making
change
to
the
source
code
file
Now
that
we
have
a
better
understanding
of
the
problem
domain
that
software
engineering
deal
with
let
u
orient
our
discussion
to
software
engineering
itself
Software
engineering
is
defined
a
the
systematic
approach
to
the
development
operation
maintenance
and
retirement
of
software
52
We
have
seen
that
besides
delivering
software
high
quality
low
cost
and
low
cycle
time
are
also
goal
which
software
engineering
must
achieve
In
other
word
the
systematic
approach
must
help
achieve
a
high
quality
and
productiv-
ity
Q
&
P
In
software
the
three
main
factor
that
influence
Q
&
P
are
people
process
and
technology
That
is
the
final
quality
delivered
and
productivity
achieved
depends
on
the
skill
of
the
people
involved
in
the
software
project
the
process
people
use
to
perform
the
different
task
in
the
project
and
the
tool
they
use
As
it
is
people
who
ultimately
develop
and
deliver
and
productivity
is
measured
with
respect
to
people
’
s
effort
a
the
basic
input
the
main
job
of
process
is
to
help
people
achieve
higher
Q
&
P
by
specifying
what
task
to
do
and
how
to
do
them
Tools
are
aid
that
help
people
perform
some
of
the
task
more
efficiently
and
with
fewer
error
It
should
therefore
be
clear
that
to
satisfy
the
objective
of
delivering
software
with
high
Q
&
P
process
form
the
core
Consequently
in
software
engineering
the
focus
is
primarily
on
process
which
are
referred
to
a
the
systematic
approach
in
the
definition
given
above
It
is
this
focus
on
process
that
distinguishes
software
engineering
from
most
other
computing
discipline
Many
other
computing
discipline
focus
on
some
type
of
product—operating
system
database
etc.—while
software
engineering
focus
on
the
process
for
producing
the
product
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
As
process
form
the
heart
of
software
engineering
with
tool
and
tech-
nology
providing
support
to
efficiently
execute
the
process
this
book
focus
primarily
on
process
In
this
chapter
we
will
discus
Role
of
a
process
and
a
process
model
in
a
project
Various
component
process
in
the
software
process
and
the
key
role
of
the
development
process
and
the
project
management
process
Various
model
for
the
development
process—waterfall
prototyping
itera-
tive
RUP
timeboxing
and
XP
The
overall
structure
of
the
project
management
process
and
it
key
phase
A
process
is
a
sequence
of
step
performed
for
a
given
purpose
52
As
men-
tioned
earlier
while
developing
industrial
strength
software
the
purpose
is
to
develop
software
to
satisfy
the
need
of
some
user
or
client
a
shown
in
Figure
A
software
project
is
one
instance
of
this
problem
and
the
development
process
is
what
is
used
to
achieve
this
purpose
So
for
a
project
it
development
process
play
a
key
role—it
is
by
following
the
process
the
desired
end
goal
of
delivering
the
software
is
achieved
However
a
discussed
earlier
it
is
not
sufficient
to
just
reach
the
final
goal
of
having
the
desired
software
but
we
want
that
the
project
be
done
at
low
cost
and
in
low
cycle
time
and
deliver
high-quality
software
The
role
of
process
increase
due
to
these
additional
goal
and
though
many
process
can
achieve
the
basic
goal
of
developing
software
in
Figure
to
achieve
high
Q
&
P
we
need
some
optimum
process
It
is
this
goal
that
make
designing
a
process
a
challenge
We
must
distinguish
process
specification
or
description
from
the
process
itself
A
process
is
a
dynamic
entity
which
capture
the
action
performed
Process
specification
on
the
other
hand
is
a
description
of
process
which
pre-
sumably
can
be
followed
in
some
project
to
achieve
the
goal
for
which
the
In
a
project
a
process
specification
may
be
used
a
the
process
the
project
plan
to
follow
The
actual
process
is
what
is
actually
done
in
the
project
Note
that
the
actual
process
can
be
different
from
the
planned
process
and
ensuring
that
the
specified
process
is
being
followed
is
a
nontrivial
problem
However
in
this
book
we
will
assume
that
the
planned
and
actual
process
are
the
same
and
will
not
distinguish
between
the
two
and
will
use
the
term
process
to
refer
to
both
A
process
model
specifies
a
general
process
which
is
optimum
for
a
class
of
project
That
is
in
the
situation
for
which
the
model
is
applicable
using
the
process
model
a
the
project
’
s
process
will
lead
to
the
goal
of
developing
software
with
high
Q
&
P
A
process
model
is
essentially
a
compilation
of
best
practice
into
a
recipe
for
success
in
the
project
In
other
word
a
process
is
a
mean
to
reach
the
goal
of
high
quality
low
cost
and
low
cycle
time
and
a
process
model
provides
a
process
structure
that
is
well
suited
for
a
class
of
project
A
process
is
often
specified
at
a
high
level
a
a
sequence
of
stage
The
sequence
of
step
for
a
stage
is
the
process
for
that
stage
and
is
often
referred
to
a
a
subprocess
of
the
process
As
defined
above
a
process
is
the
sequence
of
step
executed
to
achieve
a
goal
Since
many
different
goal
may
have
to
be
satisfied
while
developing
soft-
ware
multiple
process
are
needed
Many
of
these
do
not
concern
software
engineering
though
they
do
impact
software
development
These
could
be
con-
sidered
nonsoftware
process
Business
process
social
process
and
training
process
are
all
example
of
process
that
come
under
this
These
process
also
affect
the
software
development
activity
but
are
beyond
the
purview
of
software
engineering
The
process
that
deal
with
the
technical
and
management
issue
of
soft-
ware
development
are
collectively
called
the
software
process
As
a
software
project
will
have
to
engineer
a
solution
and
properly
manage
the
project
there
are
clearly
two
major
component
in
a
software
process—a
development
pro-
ce
and
a
project
management
process
The
development
process
specifies
all
the
engineering
activity
that
need
to
be
performed
whereas
the
management
process
specifies
how
to
plan
and
control
these
activity
so
that
cost
sched-
ule
quality
and
other
objective
are
met
Effective
development
and
project
management
process
are
the
key
to
achieving
the
objective
of
delivering
the
desired
software
satisfying
the
user
need
while
ensuring
high
productivity
and
quality
During
the
project
many
product
are
produced
which
are
typically
com-
posed
of
many
item
for
example
the
final
source
code
may
be
composed
of
many
source
file
These
item
keep
evolving
a
the
project
proceeds
creating
many
version
on
the
way
As
development
process
generally
do
not
focus
on
evolution
and
change
to
handle
them
another
process
called
software
configu-
ration
control
process
is
often
used
The
objective
of
this
component
process
is
to
primarily
deal
with
managing
change
so
that
the
integrity
of
the
product
is
not
violated
despite
change
These
three
constituent
process
focus
on
the
project
and
the
product
and
can
be
considered
a
comprising
the
product
engineering
process
a
their
main
objective
is
to
produce
the
desired
product
If
the
software
process
can
be
viewed
a
a
static
entity
then
these
three
component
process
will
suffice
However
a
software
process
itself
is
a
dynamic
entity
a
it
must
change
to
adapt
to
our
increased
understanding
about
software
development
and
availability
of
newer
technology
and
tool
Due
to
this
a
process
to
manage
the
software
process
is
needed
The
basic
objective
of
the
process
management
process
is
to
improve
the
software
process
By
improvement
we
mean
that
the
capability
of
the
process
to
produce
quality
good
at
low
cost
is
improved
For
this
the
current
software
process
is
studied
frequently
by
studying
the
project
that
have
been
done
using
the
process
The
whole
process
of
understanding
the
current
process
analyzing
it
property
determining
how
to
improve
and
then
affecting
the
improvement
is
dealt
with
by
the
process
management
process
The
relationship
between
these
major
component
process
is
shown
in
Fig-
ure
These
component
process
are
distinct
not
only
in
the
type
of
activity
performed
in
them
but
typically
also
in
the
people
who
perform
the
activity
specified
by
the
process
In
a
typical
project
development
activity
are
per-
formed
by
programmer
designer
tester
etc
the
project
management
pro-
ce
activity
are
performed
by
the
project
management
configuration
control
process
activity
are
performed
by
a
group
generally
called
the
configuration
controller
and
the
process
management
process
activity
are
performed
by
the
software
engineering
process
group
SEPG
In
this
book
we
will
focus
primarily
on
process
relating
to
product
engineering
particularly
the
development
and
project
management
process
Much
of
the
book
discus
the
different
phase
of
a
development
process
and
the
subprocesses
or
methodology
used
for
executing
these
phase
For
the
rest
of
the
book
we
will
use
the
term
software
process
to
mean
product
engineering
process
unless
specified
otherwise
For
the
software
development
process
the
goal
is
to
produce
a
high-quality
software
product
It
therefore
focus
on
activity
directly
related
to
production
of
the
software
for
example
design
coding
and
testing
As
the
development
process
specifies
the
major
development
and
quality
control
activity
that
need
to
be
performed
in
the
project
it
form
the
core
of
the
software
process
The
management
process
is
often
decided
based
on
the
development
process
A
project
’
s
development
process
defines
the
task
the
project
should
per-
form
and
the
order
in
which
they
should
be
done
A
process
limit
the
degree
of
freedom
for
a
project
by
specifying
what
type
of
activity
must
be
under-
taken
and
in
what
order
such
that
the
shortest
or
the
most
efficient
path
is
obtained
from
the
user
need
to
the
software
satisfying
these
need
The
process
drive
a
project
and
heavily
influence
the
outcome
As
discussed
earlier
a
process
model
specifies
a
general
process
usually
a
a
set
of
stage
in
which
a
project
should
be
divided
the
order
in
which
the
stage
should
be
executed
and
any
other
constraint
and
condition
on
the
execution
of
stage
The
basic
premise
behind
a
process
model
is
that
in
the
situation
for
which
the
model
is
applicable
using
the
process
model
a
the
project
’
s
process
will
lead
to
low
cost
high
quality
reduced
cycle
time
or
provide
other
benefit
In
other
word
the
process
model
provides
generic
guideline
for
developing
a
suitable
process
for
a
project
Due
to
the
importance
of
the
development
process
various
model
have
been
proposed
In
this
section
we
will
discus
some
of
the
major
model
The
simplest
process
model
is
the
waterfall
model
which
state
that
the
phase
are
organized
in
a
linear
order
The
model
wa
originally
proposed
by
Royce
74
though
variation
of
the
model
have
evolved
depending
on
the
nature
of
activity
and
the
flow
of
control
between
them
In
this
model
a
project
be-
gin
with
feasibility
analysis
Upon
successfully
demonstrating
the
feasibility
of
a
project
the
requirement
analysis
and
project
planning
begin
The
design
start
after
the
requirement
analysis
is
complete
and
coding
begin
after
the
design
is
complete
Once
the
programming
is
completed
the
code
is
integrated
and
testing
is
done
Upon
successful
completion
of
testing
the
system
is
in-
stalled
After
this
the
regular
operation
and
maintenance
of
the
system
take
place
The
model
is
shown
in
Figure
The
basic
idea
behind
the
phase
is
separation
of
concerns—each
phase
deal
with
a
distinct
and
separate
set
of
concern
By
doing
this
the
large
and
complex
task
of
building
the
software
is
broken
into
smaller
task
which
by
themselves
are
still
quite
complex
of
specifying
requirement
doing
design
etc
Separating
the
concern
and
focusing
on
a
select
few
in
a
phase
give
a
better
handle
to
the
engineer
and
manager
in
dealing
with
the
complexity
of
the
problem
The
requirement
analysis
phase
is
mentioned
a
analysis
and
planning.
Planning
is
a
critical
activity
in
software
development
A
good
plan
is
based
on
the
requirement
of
the
system
and
should
be
done
before
later
phase
begin
However
in
practice
detailed
requirement
are
not
necessary
for
planning
Consequently
planning
usually
overlap
with
the
requirement
analysis
and
a
plan
is
ready
before
the
later
phase
begin
This
plan
is
an
additional
input
to
all
the
later
phase
Linear
ordering
of
activity
ha
some
important
consequence
First
to
clearly
identify
the
end
of
a
phase
and
the
beginning
of
the
next
some
cer-
tification
mechanism
ha
to
be
employed
at
the
end
of
each
phase
This
is
usually
done
by
some
verification
and
validation
mean
that
will
ensure
that
the
output
of
a
phase
is
consistent
with
it
input
which
is
the
output
of
the
previous
phase
and
that
the
output
of
the
phase
is
consistent
with
the
overall
requirement
of
the
system
The
consequence
of
the
need
for
certification
is
that
each
phase
must
have
some
defined
output
that
can
be
evaluated
and
certified
That
is
when
the
ac-
tivities
of
a
phase
are
completed
there
should
be
some
product
that
is
produced
by
that
phase
The
output
of
the
earlier
phase
are
often
called
work
product
and
are
usually
in
the
form
of
document
like
the
requirement
document
or
design
document
For
the
coding
phase
the
output
is
the
code
Though
the
set
of
document
that
should
be
produced
in
a
project
is
dependent
on
how
the
process
is
implemented
the
following
document
generally
form
a
reasonable
set
that
should
be
produced
in
each
project
One
of
the
main
advantage
of
the
waterfall
model
is
it
simplicity
It
is
concep-
tually
straightforward
and
divide
the
large
task
of
building
a
software
system
into
a
series
of
cleanly
divided
phase
each
phase
dealing
with
a
separate
logi-
cal
concern
It
is
also
easy
to
administer
in
a
contractual
setup—as
each
phase
is
completed
and
it
work
product
produced
some
amount
of
money
is
given
by
the
customer
to
the
developing
organization
The
waterfall
model
although
widely
used
ha
some
strong
limitation
It
assumes
that
the
requirement
of
a
system
can
be
frozen
i.e.
baselined
before
the
design
begin
This
is
possible
for
system
designed
to
automate
an
existing
manual
system
But
for
new
system
determining
the
require-
ments
is
difficult
a
the
user
doe
not
even
know
the
requirement
Hence
having
unchanging
requirement
is
unrealistic
for
such
project
Freezing
the
requirement
usually
requires
choosing
the
hardware
because
it
form
a
part
of
the
requirement
specification
A
large
project
might
take
a
few
year
to
complete
If
the
hardware
is
selected
early
then
due
to
the
speed
at
which
hardware
technology
is
changing
it
is
likely
that
the
final
software
will
use
a
hardware
technology
on
the
verge
of
becoming
obsolete
This
is
clearly
not
desirable
for
such
expensive
software
system
It
follows
the
big
bang
approach—the
entire
software
is
delivered
in
one
shot
at
the
end
This
entail
heavy
risk
a
the
user
doe
not
know
until
the
very
end
what
they
are
getting
Furthermore
if
the
project
run
out
of
money
in
the
middle
then
there
will
be
no
software
That
is
it
ha
the
all
or
nothing
value
proposition
It
encourages
requirement
bloating
Since
all
requirement
must
be
spec-
ified
at
the
start
and
only
what
is
specified
will
be
delivered
it
encourages
the
user
and
other
stakeholder
to
add
even
those
feature
which
they
think
might
be
needed
which
finally
may
not
get
used
It
is
a
document-driven
process
that
requires
formal
document
at
the
end
of
each
phase
Despite
these
limitation
the
waterfall
model
ha
been
the
most
widely
used
process
model
It
is
well
suited
for
routine
type
of
project
where
the
require-
ments
are
well
understood
That
is
if
the
developing
organization
is
quite
fa-
miliar
with
the
problem
domain
and
the
requirement
for
the
software
are
quite
clear
the
waterfall
model
work
well
and
may
be
the
most
efficient
process
The
goal
of
a
prototyping-based
development
process
is
to
counter
the
first
limitation
of
the
waterfall
model
The
basic
idea
here
is
that
instead
of
freezing
the
requirement
before
any
design
or
coding
can
proceed
a
throwaway
proto-
type
is
built
to
help
understand
the
requirement
This
prototype
is
developed
based
on
the
currently
known
requirement
Development
of
the
prototype
ob-
viously
undergoes
design
coding
and
testing
but
each
of
these
phase
is
not
done
very
formally
or
thoroughly
By
using
this
prototype
the
client
can
get
an
actual
feel
of
the
system
which
can
enable
the
client
to
better
understand
the
requirement
of
the
desired
system
This
result
in
more
stable
requirement
that
change
le
frequently
Prototyping
is
an
attractive
idea
for
complicated
and
large
system
for
which
there
is
no
manual
process
or
existing
system
to
help
determine
the
requirement
In
such
situation
letting
the
client
play
with
the
prototype
provides
invaluable
and
intangible
input
that
help
determine
the
requirement
for
the
system
It
is
also
an
effective
method
of
demonstrating
the
feasibility
of
a
certain
approach
This
might
be
needed
for
novel
system
where
it
is
not
clear
that
constraint
can
be
met
or
that
algorithm
can
be
developed
to
implement
the
requirement
In
both
situation
the
risk
associated
with
the
project
are
being
reduced
through
the
use
of
prototyping
The
process
model
of
the
prototyping
approach
is
shown
in
Figure
A
development
process
using
throwaway
prototyping
typically
proceeds
a
follows
40
The
development
of
the
prototype
typically
start
when
the
prelim-
inary
version
of
the
requirement
specification
document
ha
been
developed
At
this
stage
there
is
a
reasonable
understanding
of
the
system
and
it
need
and
which
need
are
unclear
or
likely
to
change
After
the
prototype
ha
been
developed
the
end
user
and
client
are
given
an
opportunity
to
use
and
ex-
plore
the
prototype
Based
on
their
experience
they
provide
feedback
to
the
developer
regarding
the
prototype
what
is
correct
what
need
to
be
modified
what
is
missing
what
is
not
needed
etc
Based
on
the
feedback
the
prototype
is
modified
to
incorporate
some
of
the
suggested
change
that
can
be
done
easily
and
then
the
user
and
the
client
are
again
allowed
to
use
the
system
This
cycle
repeat
until
in
the
judgment
of
the
prototype
developer
and
an-
alysts
the
benefit
from
further
changing
the
system
and
obtaining
feedback
is
outweighed
by
the
cost
and
time
involved
in
making
the
change
and
obtaining
the
feedback
Based
on
the
feedback
the
initial
requirement
are
modified
to
produce
the
final
requirement
specification
which
is
then
used
to
develop
the
production
quality
system
For
prototyping
for
the
purpose
of
requirement
analysis
to
be
feasible
it
cost
must
be
kept
low
Consequently
only
those
feature
are
included
in
the
prototype
that
will
have
a
valuable
return
from
the
user
experience
Excep-
tion
handling
recovery
and
conformance
to
some
standard
and
format
are
typically
not
included
in
prototype
In
prototyping
a
the
prototype
is
to
be
discarded
there
is
no
point
in
implementing
those
part
of
the
requirement
that
are
already
well
understood
Hence
the
focus
of
the
development
is
to
include
those
feature
that
are
not
properly
understood
And
the
development
approach
is
quick
and
dirty
with
the
focus
on
quick
development
rather
than
quality
Because
the
prototype
is
to
be
thrown
away
only
minimal
documenta-
tion
need
to
be
produced
during
prototyping
For
example
design
document
a
test
plan
and
a
test
case
specification
are
not
needed
during
the
development
of
the
prototype
Another
important
cost-cutting
measure
is
to
reduce
testing
Because
testing
consumes
a
major
part
of
development
expenditure
during
reg-
ular
software
development
this
ha
a
considerable
impact
in
reducing
cost
By
using
these
type
of
cost-cutting
method
it
is
possible
to
keep
the
cost
of
the
prototype
to
le
than
a
few
percent
of
the
total
development
cost
And
the
return
from
this
extra
cost
can
be
substantial
First
the
experience
of
developing
the
prototype
will
reduce
the
cost
of
the
actual
software
develop-
ment
Second
a
requirement
will
be
more
stable
now
due
to
the
feedback
from
the
prototype
there
will
be
fewer
change
in
the
requirement
Consequently
the
cost
incurred
due
to
change
in
the
requirement
will
be
substantially
re-
duced
Third
the
quality
of
final
software
is
likely
to
be
far
superior
a
the
experience
engineer
have
obtained
while
developing
the
prototype
will
enable
them
to
create
a
better
design
write
better
code
and
do
better
testing
And
fi-
nally
developing
a
prototype
mitigates
many
risk
that
exist
in
a
project
where
requirement
are
not
well
known
Overall
prototyping
is
well
suited
for
project
where
requirement
are
hard
to
determine
and
the
confidence
in
the
stated
requirement
is
low
In
such
project
where
requirement
are
not
properly
understood
in
the
beginning
using
the
prototyping
process
model
can
be
the
most
effective
method
for
developing
the
software
It
is
also
an
excellent
technique
for
reducing
some
type
of
risk
associated
with
a
project
The
iterative
development
process
model
counter
the
third
and
fourth
limita-
tions
of
the
waterfall
model
and
try
to
combine
the
benefit
of
both
proto-
typing
and
the
waterfall
model
The
basic
idea
is
that
the
software
should
be
developed
in
increment
each
increment
adding
some
functional
capability
to
the
system
until
the
full
system
is
implemented
The
iterative
enhancement
model
4
is
an
example
of
this
approach
In
the
first
step
of
this
model
a
simple
initial
implementation
is
done
for
a
subset
of
the
overall
problem
This
subset
is
one
that
contains
some
of
the
key
aspect
of
the
problem
that
are
easy
to
understand
and
implement
and
which
form
a
useful
and
usable
system
A
project
control
list
is
created
that
contains
in
order
all
the
task
that
must
be
performed
to
obtain
the
final
implementation
This
project
control
list
give
an
idea
of
how
far
along
the
project
is
at
any
given
step
from
the
final
system
Each
step
consists
of
removing
the
next
task
from
the
list
designing
the
implementation
for
the
selected
task
coding
and
testing
the
implementation
performing
an
analysis
of
the
partial
system
obtained
after
this
step
and
updat-
ing
the
list
a
a
result
of
the
analysis
These
three
phase
are
called
the
design
phase
implementation
phase
and
analysis
phase
The
process
is
iterated
until
the
project
control
list
is
empty
at
which
time
the
final
implementation
of
the
system
will
be
available
The
iterative
enhancement
model
is
shown
in
Figure
The
project
control
list
guide
the
iteration
step
and
keep
track
of
all
task
that
must
be
done
Based
on
the
analysis
one
of
the
task
in
the
list
can
include
redesign
of
defective
component
or
redesign
of
the
entire
system
However
redesign
of
the
system
will
generally
occur
only
in
the
initial
step
In
the
later
step
the
design
would
have
stabilized
and
there
is
le
chance
of
redesign
Each
entry
in
the
list
is
a
task
that
should
be
performed
in
one
step
of
the
iterative
enhancement
process
and
should
be
simple
enough
to
be
completely
understood
Selecting
task
in
this
manner
will
minimize
the
chance
of
error
and
reduce
the
redesign
work
The
design
and
implementation
phase
of
each
step
can
be
performed
in
a
top-down
manner
or
by
using
some
other
technique
Though
there
are
clear
benefit
of
iterative
development
particularly
in
allowing
changing
requirement
not
having
the
all-or-nothing
risk
etc.
there
are
some
cost
associated
with
iterative
development
also
For
example
a
the
requirement
for
future
iteration
are
not
known
the
design
of
a
system
may
not
be
too
robust
Also
change
may
have
to
be
made
to
the
existing
system
to
accommodate
requirement
of
the
future
iteration
leading
to
extra
rework
and/or
discarding
of
work
done
earlier
Overall
it
may
not
offer
the
best
technical
solution
but
the
benefit
may
outweigh
the
cost
in
many
project
Another
common
approach
for
iterative
development
is
to
do
the
require-
ments
and
the
architecture
design
in
a
standard
waterfall
or
prototyping
ap-
proach
but
deliver
the
software
iteratively
That
is
the
building
of
the
system
which
is
the
most
time
and
effort-consuming
task
is
done
iteratively
though
most
of
the
requirement
are
specified
upfront
We
can
view
this
approach
a
having
one
iteration
delivering
the
requirement
and
the
architecture
plan
and
then
further
iteration
delivering
the
software
in
increment
At
the
start
of
each
delivery
iteration
which
requirement
will
be
implemented
in
this
release
are
decided
and
then
the
design
is
enhanced
and
code
developed
to
implement
the
requirement
The
iteration
end
with
delivery
of
a
working
software
system
providing
some
value
to
the
end
user
Selecting
of
requirement
for
an
iteration
is
done
primarily
based
on
the
value
the
requirement
provides
to
the
end
user
and
how
critical
they
are
for
supporting
other
requirement
This
approach
is
shown
in
Figure
The
advantage
of
this
approach
is
that
a
the
requirement
are
mostly
known
upfront
an
overall
view
of
the
system
is
available
and
a
proper
archi-
tecture
can
be
designed
which
can
remain
relatively
stable
With
this
hopefully
rework
in
development
iteration
will
diminish
At
the
same
time
the
value
to
the
end
customer
is
delivered
iteratively
so
it
doe
not
have
the
all-or-nothing
risk
Also
since
the
delivery
is
being
done
incrementally
and
planning
and
execution
of
each
iteration
is
done
separately
feedback
from
an
iteration
can
be
incorporated
in
the
next
iteration
Even
new
requirement
that
may
get
un-
covered
can
also
be
incorporated
Hence
this
model
of
iterative
development
also
provides
some
of
the
benefit
of
the
model
discussed
above
The
iterative
approach
is
becoming
extremely
popular
despite
some
diffi-
culties
in
using
it
in
this
context
There
are
a
few
key
reason
for
it
increasing
popularity
First
and
foremost
in
today
’
s
world
client
do
not
want
to
invest
too
much
without
seeing
return
In
the
current
business
scenario
it
is
preferable
to
see
return
continuously
of
the
investment
made
The
iterative
model
per-
mit
this—after
each
iteration
some
working
software
is
delivered
and
the
risk
to
the
client
is
therefore
limited
Second
a
business
are
changing
rapidly
today
they
never
really
know
the
complete
requirement
for
the
software
and
there
is
a
need
to
constantly
add
new
capability
to
the
software
to
adapt
the
business
to
changing
situation
Iterative
process
allows
this
Third
each
iteration
provides
a
working
system
for
feedback
which
help
in
developing
stable
requirement
for
the
next
iteration
Below
we
will
describe
some
other
process
model
all
of
them
using
some
iterative
approach
Rational
Unified
Process
RUP
51
63
is
another
iterative
process
model
that
wa
designed
by
Rational
now
part
of
IBM
Though
it
is
a
general
pro-
ce
model
it
wa
designed
for
object-oriented
development
using
the
Unified
Modeling
Language
UML
We
will
discus
these
topic
in
a
later
chapter
RUP
proposes
that
development
of
software
be
divided
into
cycle
each
cycle
delivering
a
fully
working
system
Generally
each
cycle
is
executed
a
a
separate
project
whose
goal
is
to
deliver
some
additional
capability
to
an
exist-
ing
system
built
by
the
previous
cycle
Hence
for
a
project
the
process
for
a
cycle
form
the
overall
process
Each
cycle
itself
is
broken
into
four
consecutive
phase
Each
phase
ha
a
distinct
purpose
and
completion
of
each
phase
is
a
well-
defined
milestone
in
the
project
with
some
clearly
defined
output
The
purpose
of
the
inception
phase
is
to
establish
the
goal
and
scope
of
the
project
and
completion
of
this
phase
is
the
lifecycle
objective
milestone
This
milestone
should
specify
the
vision
and
high-level
capability
of
the
eventual
system
what
business
benefit
it
is
expected
to
provide
some
key
illustrative
use
case
of
the
system
key
risk
of
the
project
and
a
basic
plan
of
the
project
regarding
the
cost
and
schedule
Based
on
the
output
of
this
phase
a
go/no-go
decision
may
be
taken
And
if
the
project
is
to
proceed
then
this
milestone
represents
that
there
is
a
shared
vision
among
the
stakeholder
and
they
agree
to
the
project
it
vision
benefit
cost
usage
etc
In
the
elaboration
phase
the
architecture
of
the
system
is
designed
based
on
the
detailed
requirement
analysis
The
completion
of
this
phase
is
the
life-
cycle
architecture
milestone
At
the
end
of
this
phase
it
is
expected
that
most
of
the
requirement
have
been
identified
and
specified
and
the
architecture
of
the
system
ha
been
designed
and
specified
in
a
manner
that
it
address
the
technical
risk
identified
in
the
earlier
phase
In
addition
a
high-level
project
plan
for
the
project
ha
been
prepared
showing
the
remaining
phase
and
iter-
ations
in
those
and
the
current
perception
of
risk
By
the
end
of
this
phase
the
critical
engineering
decision
regarding
the
choice
of
technology
architec-
ture
etc
have
been
taken
and
a
detailed
understanding
of
the
project
exists
Outputs
of
this
milestone
allow
technical
evaluation
of
the
proposed
solution
a
well
a
a
better
informed
decision
about
cost-benefit
analysis
of
the
project
In
the
construction
phase
the
software
is
built
and
tested
This
phase
result
in
the
software
product
to
be
delivered
along
with
associated
user
and
other
manual
and
successfully
completing
this
phase
result
in
the
initial
operational
capability
milestone
being
achieved
The
purpose
of
the
transition
phase
is
to
move
the
software
from
the
devel-
opment
environment
to
the
client
’
s
environment
where
it
is
to
be
hosted
This
is
a
complex
task
which
can
require
additional
testing
conversion
of
old
data
for
this
software
to
work
training
of
personnel
etc
The
successful
execution
of
this
phase
result
in
achieving
the
milestone
product
release
The
different
phase
and
milestone
in
RUP
are
shown
in
Figure
Though
these
phase
are
consecutive
each
phase
itself
may
have
multiple
iteration
with
each
iteration
delivering
to
an
internal
or
external
customer
some
well-defined
output
which
is
often
a
part
of
the
final
deliverable
of
that
phase
’
s
milestone
Generally
it
is
expected
that
the
construction
phase
will
be
broken
into
multiple
iteration
each
iteration
producing
a
working
system
which
can
be
used
for
feedback
evaluation
beta-testing
etc
Though
itera-
tions
in
construction
are
done
often
and
it
is
clear
what
an
iteration
in
this
phase
delivers
iteration
may
be
done
meaningfully
in
other
phase
a
well
For
example
in
the
elaboration
phase
the
first
iteration
may
just
specify
the
overall
architecture
and
high-level
requirement
while
the
second
iteration
may
be
done
to
thrash
out
the
detail
As
another
example
there
may
be
multiple
iteration
to
transition
the
developed
software
with
each
iteration
making
live
some
part
or
some
feature
of
the
developed
software
RUP
ha
carefully
chosen
the
phase
name
so
a
not
to
confuse
them
with
the
engineering
task
that
are
to
be
done
in
the
project
a
in
RUP
the
en-
gineering
task
and
phase
are
separate
Different
engineering
activity
may
be
performed
in
a
phase
to
achieve
it
milestone
RUP
group
the
activity
into
different
subprocesses
which
it
call
core
process
workflow
These
sub-
process
correspond
to
the
task
of
performing
requirement
analysis
doing
design
implementing
the
design
testing
project
management
etc
Some
of
the
subprocesses
are
shown
in
Table
One
key
difference
of
RUP
from
other
model
is
that
it
ha
separated
the
phase
from
the
task
and
allows
multiple
of
these
subprocesses
to
function
within
a
phase
In
waterfall
or
waterfall-based
iterative
model
a
phase
within
a
process
wa
linked
to
a
particular
task
performed
by
some
process
like
re-
quirements
design
etc
In
RUP
these
task
are
separated
from
the
stage
and
it
allows
for
example
during
construction
execution
of
the
requirement
process
That
is
it
allows
some
part
of
the
requirement
activity
be
done
even
in
construction
something
the
waterfall
did
not
allow
So
a
project
if
it
so
wish
may
do
detailed
requirement
only
for
some
feature
during
the
elabora-
tion
phase
and
may
do
detailing
of
other
requirement
while
the
construction
is
going
on
maybe
the
first
iteration
of
it
This
not
only
allows
a
project
a
greater
degree
of
flexibility
in
planning
when
the
different
task
should
be
done
it
also
capture
the
reality
of
the
situation—it
is
often
not
possible
to
specify
all
requirement
at
the
start
and
it
is
best
to
start
the
project
with
some
requirement
and
work
out
the
detail
later
Though
a
subprocess
may
be
active
in
many
phase
a
can
be
expected
the
volume
of
work
or
the
effort
being
spent
on
the
subprocess
will
vary
with
phase
For
example
it
is
expected
that
a
lot
more
effort
will
be
spent
in
the
requirement
subprocess
during
elaboration
and
le
will
be
spent
in
construc-
tion
and
still
le
if
any
will
be
spent
in
transition
Similarly
the
model
ha
the
development
process
active
in
elaboration
which
allows
a
project
to
build
a
prototype
during
the
elaboration
phase
to
help
it
requirement
activity
if
needed
However
most
of
the
implementation
doe
happen
in
the
construction
phase
The
effort
spent
in
a
subprocess
in
different
phase
will
of
course
de-
pend
on
the
project
However
a
general
pattern
is
indicated
in
Table
by
specifying
if
the
level
of
effort
for
the
phase
is
high
medium
low
etc
Overall
RUP
provides
a
flexible
process
model
which
follows
an
iterative
approach
not
only
at
a
top
level
through
cycle
but
also
encourages
iterative
approach
during
each
of
the
phase
in
a
cycle
And
in
phase
it
allows
the
different
task
to
be
done
a
per
the
need
of
the
project
To
speed
up
development
parallelism
between
the
different
iteration
can
be
employed
That
is
a
new
iteration
commences
before
the
system
produced
by
the
current
iteration
is
released
and
hence
development
of
a
new
release
happens
in
parallel
with
the
development
of
the
current
release
By
starting
an
iteration
before
the
previous
iteration
ha
completed
it
is
possible
to
reduce
the
average
delivery
time
for
iteration
However
to
support
parallel
execution
each
iteration
ha
to
be
structured
properly
and
team
have
to
be
organized
suitably
The
timeboxing
model
proposes
an
approach
for
these
60
59
In
the
timeboxing
model
the
basic
unit
of
development
is
a
time
box
which
is
of
fixed
duration
Since
the
duration
is
fixed
a
key
factor
in
selecting
the
requirement
or
feature
to
be
built
in
a
time
box
is
what
can
be
fit
into
the
time
box
This
is
in
contrast
to
regular
iterative
approach
where
the
functionality
is
selected
and
then
the
time
to
deliver
is
determined
Timeboxing
change
the
perspective
of
development
and
make
the
schedule
a
nonnegotiable
and
a
high-priority
commitment
Each
time
box
is
divided
into
a
sequence
of
stage
like
in
the
waterfall
model
Each
stage
performs
some
clearly
defined
task
for
the
iteration
and
produce
a
clearly
defined
output
The
model
also
requires
that
the
duration
of
each
stage
that
is
the
time
it
take
to
complete
the
task
of
that
stage
is
approximately
the
same
Furthermore
the
model
requires
that
there
be
a
dedicated
team
for
each
stage
That
is
the
team
for
a
stage
performs
only
task
of
that
stage—tasks
for
other
stage
are
performed
by
their
respective
team
This
is
quite
different
from
other
iterative
model
where
the
implicit
assumption
is
that
the
same
team
performs
all
the
different
task
of
the
project
or
the
iteration
Having
time-boxed
iteration
with
stage
of
equal
duration
and
having
ded-
icated
team
render
itself
to
pipelining
of
different
iteration
Pipelining
is
a
concept
from
hardware
in
which
different
instruction
are
executed
in
paral-
lel
with
the
execution
of
a
new
instruction
starting
once
the
first
stage
of
the
previous
instruction
is
finished
To
illustrate
the
use
of
this
model
consider
a
time
box
consisting
of
three
stage
requirement
specification
build
and
deployment
The
require-
ment
stage
is
executed
by
it
team
of
analyst
and
end
with
a
prioritized
list
of
requirement
to
be
built
in
this
iteration
along
with
a
high-level
design
The
build
team
develops
the
code
for
implementing
the
requirement
and
performs
the
testing
The
tested
code
is
then
handed
over
to
the
deployment
team
which
performs
predeployment
test
and
then
installs
the
system
for
production
use
These
three
stage
are
such
that
they
can
be
done
in
approximately
equal
time
in
an
iteration
With
a
time
box
of
three
stage
the
project
proceeds
a
follows
When
the
requirement
team
ha
finished
requirement
for
timebox-1
the
requirement
are
given
to
the
build
team
for
building
the
software
The
requirement
team
then
go
on
and
start
preparing
the
requirement
for
timebox-2
When
the
build
for
timebox-
the
code
is
handed
over
to
the
deployment
team
and
the
build
team
move
on
to
build
code
for
requirement
for
timebox-
2
and
the
requirement
team
move
on
to
doing
requirement
for
timebox-3
This
pipelined
execution
of
the
timeboxing
process
is
shown
in
Figure
59
With
a
three-stage
time
box
at
most
three
iteration
can
be
concurrently
in
progress
If
the
time
box
is
of
size
T
day
then
the
first
software
delivery
will
occur
after
T
day
The
subsequent
delivery
however
will
take
place
after
every
T/
For
example
if
the
time
box
duration
T
is
and
each
stage
duration
is
the
first
delivery
is
made
The
second
delivery
is
made
after
1
the
third
after
1
and
so
on
Contrast
this
with
a
linear
execution
of
iteration
in
which
the
first
delivery
will
be
made
after
the
second
after
1
the
third
after
2
and
so
on
There
are
three
team
working
on
the
project—the
requirement
team
the
build
team
and
the
deployment
team
The
teamwise
activity
for
the
3-stage
pipeline
discussed
above
is
shown
in
Figure
59
It
should
be
clear
that
the
duration
of
each
iteration
ha
not
been
reduced
The
total
work
done
in
a
time
box
and
the
effort
spent
in
it
also
remains
the
same—the
same
amount
of
software
is
delivered
at
the
end
of
each
iteration
a
the
time
box
undergoes
the
same
stage
If
the
effort
and
time
spent
in
each
iteration
also
remains
the
same
then
what
is
the
cost
of
reducing
the
delivery
time
The
real
cost
of
this
reduced
time
is
in
the
resource
used
in
this
model
With
timeboxing
there
are
dedicated
team
for
different
stage
and
the
total
team
size
for
the
project
is
the
sum
of
team
of
different
stage
This
is
the
main
difference
from
the
situation
where
there
is
a
single
team
which
performs
all
the
stage
and
the
entire
team
work
on
the
same
iteration
Hence
the
timeboxing
provides
an
approach
for
utilizing
additional
man-
power
to
reduce
the
delivery
time
It
is
well
known
that
with
standard
method
of
executing
project
we
can
not
compress
the
cycle
time
of
a
project
substan-
tially
by
adding
more
manpower
However
through
the
timeboxing
model
we
can
use
more
manpower
in
a
manner
such
that
by
parallel
execution
of
different
stage
we
are
able
to
deliver
software
quicker
In
other
word
it
provides
a
way
of
shortening
delivery
time
through
the
use
of
additional
manpower
Timeboxing
is
well
suited
for
project
that
require
a
large
number
of
fea-
tures
to
be
developed
in
a
short
time
around
a
stable
architecture
using
stable
technology
These
feature
should
be
such
that
there
is
some
flexibility
in
grouping
them
for
building
a
meaningful
system
in
an
iteration
that
provides
value
to
the
user
The
main
cost
of
this
model
is
the
increased
complexity
of
project
management
and
managing
the
product
being
developed
a
multiple
development
are
concurrently
active
Also
the
impact
of
unusual
situation
in
an
iteration
can
be
quite
disruptive
Further
detail
about
the
model
a
well
a
a
detailed
example
of
applying
the
model
on
a
real
commercial
project
are
given
in
60
59
Agile
development
approach
evolved
in
the
1990s
a
a
reaction
to
documen-
tation
and
bureaucracy-based
process
particularly
the
waterfall
approach
Agile
approach
are
based
on
some
common
principle
some
of
which
are
x
Working
software
is
the
key
measure
of
progress
in
a
project
For
progress
in
a
project
therefore
software
should
be
developed
and
deliv-
ered
rapidly
in
small
increment
Even
late
change
in
the
requirement
should
be
entertained
small-increment
model
of
development
help
in
accommodating
them
Face-to-face
communication
is
preferred
over
documentation
Continuous
feedback
and
involvement
of
customer
is
necessary
for
developing
good-quality
software
Simple
design
which
evolves
and
improves
with
time
is
a
better
approach
than
doing
an
elaborate
design
up
front
for
handling
all
possible
scenario
The
delivery
date
are
decided
by
empowered
team
of
talented
individual
and
are
not
dictated
Many
detailed
agile
methodology
have
been
proposed
some
of
which
are
widely
used
now
Extreme
programming
XP
is
one
of
the
most
popular
and
well-known
approach
in
the
family
of
agile
method
Like
all
agile
approach
it
belief
that
change
are
inevitable
and
rather
than
treating
change
a
un-
desirable
development
should
embrace
change
And
to
accommodate
change
the
development
process
ha
to
be
lightweight
and
quick
to
respond
For
this
it
develops
software
iteratively
and
avoids
reliance
on
detailed
and
multiple
document
which
are
hard
to
maintain
Instead
it
relies
on
face-to-face
com-
munication
simplicity
and
feedback
to
ensure
that
the
desired
change
are
quickly
and
correctly
reflected
in
the
program
Here
we
briefly
discus
the
development
process
of
XP
a
a
representative
of
an
agile
process
An
extreme
programming
project
start
with
user
story
which
are
short
a
few
sentence
description
of
what
scenario
the
customer
and
user
would
like
the
system
to
support
They
are
different
from
traditional
requirement
specification
primarily
in
details—user
story
do
not
contain
detailed
require-
ments
which
are
to
be
uncovered
only
when
the
story
is
to
be
implemented
therefore
allowing
the
detail
to
be
decided
a
late
a
possible
Each
story
is
written
on
a
separate
card
so
they
can
be
flexibly
grouped
The
empowered
development
team
estimate
how
long
it
will
take
to
imple-
ment
a
user
story
The
estimate
are
rough
generally
stated
in
week
Using
these
estimate
and
the
story
release
planning
is
done
which
defines
which
story
are
to
be
built
in
which
system
release
and
the
date
of
these
release
Frequent
and
small
release
are
encouraged
and
for
a
release
iteration
are
employed
Acceptance
test
are
also
built
from
the
story
which
are
used
to
test
the
software
before
the
release
Bugs
found
during
the
acceptance
testing
for
an
iteration
can
form
work
item
for
the
next
iteration
This
overall
process
is
shown
in
Figure
Development
is
done
in
iteration
each
iteration
lasting
no
more
than
a
few
week
An
iteration
start
with
iteration
planning
in
which
the
story
to
be
implemented
in
this
iteration
are
selected—high-value
and
high-risk
story
are
considered
a
higher
priority
and
implemented
in
early
iteration
Failed
acceptance
test
in
previous
iteration
also
have
to
be
handled
Details
of
the
story
are
obtained
in
the
iteration
for
doing
the
development
The
development
approach
used
in
an
iteration
ha
some
unique
practice
First
it
envisages
that
development
is
done
by
pair
of
programmer
called
pair
programming
and
which
we
will
discus
further
in
Chapter
7
instead
of
individual
programmer
Second
it
suggests
that
for
building
a
code
unit
automated
unit
test
be
written
first
before
the
actual
code
is
written
and
then
the
code
should
be
written
to
pas
the
test
This
approach
is
referred
to
a
test-driven
development
in
contrast
to
regular
code-first
development
in
which
programmer
first
write
code
and
then
think
of
how
to
test
it
We
will
discus
test-driven
development
further
in
Chapter
7
As
functionality
of
the
unit
increase
the
unit
test
are
enhanced
first
and
then
the
code
is
enhanced
to
pas
the
new
set
of
unit
test
Third
a
it
encourages
simple
solution
a
well
a
change
it
is
expected
that
the
design
of
the
solution
devised
earlier
may
at
some
point
become
unsuitable
for
further
development
To
handle
this
situation
it
suggests
that
refactoring
be
done
to
improve
the
design
and
then
use
the
refactored
code
for
further
development
During
refactoring
no
new
functionality
is
added
only
the
design
of
the
existing
program
is
improved
Refactoring
will
be
discussed
further
in
Chapter
7
Fourth
it
encourages
frequent
integration
of
different
unit
To
avoid
too
many
change
in
the
base
code
happening
together
only
one
pair
at
a
time
can
release
their
change
and
integrate
into
the
common
code
base
The
process
within
an
iteration
is
shown
This
is
a
very
simplified
description
of
XP
There
are
many
other
rule
in
XP
relating
to
issue
like
right
of
programmer
and
customer
communi-
cation
between
the
team
member
and
use
of
metaphor
trust
and
visibility
to
all
stakeholder
collective
ownership
of
code
in
which
any
pair
can
change
any
code
team
management
building
quick
spike
solution
to
resolve
diffi-
cult
technical
and
architectural
issue
or
to
explore
some
approach
how
bug
are
to
be
handled
how
what
can
be
done
within
an
iteration
is
to
be
esti-
mated
from
the
progress
made
in
the
previous
iteration
how
meeting
are
to
be
conducted
how
a
day
in
the
development
should
start
etc
The
website
is
a
good
source
on
these
a
well
a
other
as-
pects
of
XP
XP
and
other
agile
method
are
suitable
for
situation
where
the
volume
and
pace
of
requirement
change
is
high
and
where
requirement
risk
are
con-
siderable
Because
of
it
reliance
on
strong
communication
between
all
the
team
member
it
is
effective
when
team
are
collocated
and
of
modest
size
of
up
to
about
20
member
And
a
it
envisages
strong
involvement
of
the
customer
in
the
development
a
well
a
in
planning
the
delivery
date
it
work
well
when
the
customer
is
willing
to
be
heavily
involved
during
the
entire
development
working
a
a
team
member
We
have
seen
many
different
development
process
model
What
is
the
need
for
the
different
model
As
mentioned
earlier
while
developing
industrial
strength
software
the
purpose
is
not
only
to
develop
software
to
satisfy
the
need
of
some
user
or
client
but
we
want
that
the
project
be
done
in
low
cost
and
cycle
time
and
deliver
high-quality
software
In
addition
there
could
be
other
constraint
in
a
project
that
the
project
may
need
to
satisfy
Hence
given
the
constraint
of
the
project
we
would
like
to
employ
the
process
model
that
is
likely
to
maximize
the
chance
of
delivering
the
software
and
achieve
the
highest
Q
&
P
Hence
selecting
a
suitable
development
process
model
for
a
project
is
a
key
decision
that
a
project
manager
ha
to
take
Let
u
illustrate
this
by
a
few
example
Suppose
a
small
team
of
developer
ha
been
entrusted
with
the
task
of
building
a
small
auction
site
for
a
local
university
The
university
administration
is
willing
to
spend
some
time
at
the
start
to
help
develop
the
requirement
but
it
is
expected
that
their
availability
will
be
limited
later
The
team
ha
been
given
and
an
extension
of
the
deadline
seems
very
improbable
It
also
seems
that
the
auction
site
will
have
some
feature
that
are
essential
but
will
also
have
some
feature
that
are
desirable
but
without
which
the
system
can
function
reasonably
well
With
these
constraint
it
is
clear
that
a
waterfall
model
is
not
suitable
for
this
project
a
the
all
or
nothing
risk
that
it
entail
is
unacceptable
due
to
the
inflexible
deadline
The
iterative
enhancement
model
where
each
iteration
doe
a
complete
waterfall
is
also
not
right
a
it
requires
requirement
analysis
for
each
iteration
and
the
user
and
client
are
not
available
later
However
the
iterative
delivery
approach
in
which
the
complete
requirement
are
done
in
the
first
iteration
but
delivery
is
done
in
iteration
seems
well
suited
with
delivery
being
done
in
two
or
three
iteration
a
time
is
short
From
the
requirement
the
project
team
can
decide
what
functionality
is
essential
to
have
in
a
working
system
and
include
it
in
the
first
iteration
The
other
desirable
feature
can
be
planned
for
the
second
iteration
With
this
approach
the
chance
of
completing
the
first
iteration
before
the
final
deadline
increase
That
is
with
this
model
the
chance
of
delivering
a
working
system
increase
RUP
a
it
allows
iteration
in
each
phase
is
also
a
suitable
model
Consider
another
example
where
the
customer
are
in
a
highly
competitive
environment
where
requirement
depend
on
what
the
competition
is
doing
and
delivering
functionality
regularly
is
highly
desirable
Furthermore
to
reduce
cost
the
customer
want
to
outsource
a
much
project
work
a
possible
to
another
team
in
another
country
For
this
project
clearly
waterfall
is
not
suitable
a
requirement
are
not
even
known
at
the
start
Iterative
enhancement
also
may
not
work
a
it
may
not
be
able
to
deliver
rapidly
XP
will
be
hard
to
apply
a
it
requires
that
the
entire
team
including
the
customer
be
collocated
For
this
project
the
timeboxing
model
seems
to
fit
the
best
The
whole
project
can
employ
three
teams—one
of
analyst
who
will
work
with
the
customer
to
determine
the
requirement
one
to
do
the
development
which
could
be
in
some
low-cost
destination
and
the
third
to
do
the
deployment
which
will
be
where
the
site
is
hosted
By
suitably
staffing
the
team
the
duration
of
each
of
the
three
phases—analysis
and
design
build
and
deployment—can
be
made
approximately
equal
Then
Consider
another
project
where
a
university
want
to
automate
the
reg-
istration
process
It
already
ha
a
database
of
course
and
pre-requisites
and
a
database
of
student
record
In
this
project
a
the
requirement
are
well
understood
since
registration
have
been
happening
manually
the
waterfall
model
seems
to
be
the
optimum
While
the
selection
of
the
development
process
decides
the
phase
and
task
to
be
done
it
doe
not
specify
thing
like
how
long
each
phase
should
last
or
how
many
resource
should
be
assigned
to
a
phase
or
how
a
phase
should
be
monitored
And
quality
and
productivity
in
the
project
will
also
depend
critically
on
these
decision
To
meet
the
cost
quality
and
schedule
objective
resource
have
to
be
properly
allocated
to
each
activity
for
the
project
and
progress
of
different
activity
ha
to
be
monitored
and
corrective
action
taken
when
needed
All
these
activity
are
part
of
the
project
management
process
Hence
a
project
management
process
is
necessary
to
ensure
that
the
engineer-
ing
process
end
up
meeting
the
real-world
objective
of
cost
schedule
and
quality
The
project
management
process
specifies
all
activity
that
need
to
be
done
by
the
project
management
to
ensure
that
cost
and
quality
objective
are
met
Its
basic
task
is
to
ensure
that
once
a
development
process
is
chosen
it
is
implemented
optimally
That
is
the
basic
task
is
to
plan
the
detailed
implementation
of
the
process
for
the
particular
project
and
then
ensure
that
the
plan
is
properly
executed
For
a
large
project
a
proper
management
process
is
essential
for
success
The
activity
in
the
management
process
for
a
project
can
be
grouped
broadly
into
three
phase
planning
monitoring
and
control
and
termination
analysis
Project
management
begin
with
planning
which
is
perhaps
the
most
critical
project
management
activity
The
goal
of
this
phase
is
to
develop
a
plan
for
software
development
following
which
the
objective
of
the
project
can
be
met
successfully
and
efficiently
A
software
plan
is
usually
produced
before
the
development
activity
begin
and
is
updated
a
development
proceeds
and
data
about
progress
of
the
project
becomes
available
During
planning
the
major
activity
are
cost
estimation
schedule
and
milestone
determination
project
staffing
quality
control
plan
and
controlling
and
monitoring
plan
Project
planning
is
undoubtedly
the
single
most
important
management
activity
and
it
form
the
basis
for
monitoring
and
control
We
will
devote
one
full
chapter
Project
monitoring
and
control
phase
of
the
management
process
is
the
longest
in
term
of
duration
it
encompasses
most
of
the
development
process
It
includes
all
activity
the
project
management
ha
to
perform
while
the
development
is
going
on
to
ensure
that
project
objective
are
met
and
the
development
proceeds
according
to
the
developed
plan
and
update
the
plan
if
needed
As
cost
schedule
and
quality
are
the
major
driving
force
most
of
the
activity
of
this
phase
revolves
around
monitoring
factor
that
affect
these
Monitoring
potential
risk
for
the
project
which
might
prevent
the
project
from
meeting
it
objective
is
another
important
activity
during
this
phase
And
if
the
information
obtained
by
monitoring
suggests
that
objective
may
not
be
met
necessary
action
are
taken
in
this
phase
by
exerting
suitable
control
on
the
development
activity
Monitoring
a
development
process
requires
proper
information
about
the
project
Such
information
is
typically
obtained
by
the
management
process
from
the
development
process
Consequently
the
implementation
of
a
development
process
model
should
ensure
that
each
step
in
the
development
process
produce
information
that
the
management
process
need
for
that
step
That
is
the
development
process
provides
the
information
the
management
process
need
However
interpretation
of
the
information
is
part
of
monitoring
and
control
Whereas
monitoring
and
control
last
the
entire
duration
of
the
project
the
last
phase
of
the
management
process—termination
analysis—is
performed
when
the
development
process
is
over
The
basic
reason
for
performing
termi-
nation
analysis
is
to
provide
information
about
the
development
process
and
learn
from
the
project
in
order
to
improve
the
process
This
phase
is
also
of-
ten
called
postmortem
analysis
In
iterative
development
this
analysis
can
be
done
after
each
iteration
to
provide
feedback
to
improve
the
execution
of
fur-
ther
iteration
We
will
not
discus
it
further
in
the
book
for
an
example
of
a
postmortem
report
the
reader
is
referred
to
57
The
temporal
relationship
between
the
management
process
and
the
devel-
opment
process
is
shown
in
Figure
This
is
an
idealized
relationship
show-
ing
that
planning
is
done
before
development
begin
and
termination
analysis
is
done
after
development
is
over
As
the
figure
show
during
the
development
from
the
various
phase
of
the
development
process
quantitative
information
flow
to
the
monitoring
and
control
phase
of
the
management
process
which
us
the
information
to
exert
control
on
the
development
process
We
will
in
a
later
chapter
discus
in
detail
the
project
planning
phase
As
a
plan
also
includes
planning
for
monitoring
we
will
not
discus
the
monitoring
separately
but
discus
it
a
part
of
the
planning
activity
Figure
2.12
Temporal
relationship
between
development
and
management
pro-
ce
The
quality
and
productivity
achieved
in
a
software
project
depends
on
the
process
used
for
executing
the
project
Due
to
this
process
form
the
heart
of
software
engineering
A
process
is
the
set
of
activity
that
are
performed
in
some
order
so
that
the
desired
result
will
be
achieved
A
process
model
is
a
general
process
specification
which
ha
been
found
to
be
best
suited
for
some
situation
A
software
process
consists
of
many
different
component
process
most
im-
portant
being
the
development
process
and
the
project
management
process
Development
process
focus
on
how
the
software
is
to
be
engineered
There
are
many
different
process
model
each
being
well
suited
for
some
type
of
problem
The
waterfall
model
is
conceptually
the
simplest
model
of
software
de-
velopment
where
the
requirement
design
coding
and
testing
phase
are
performed
in
linear
progression
It
ha
been
very
widely
used
and
is
suit-
able
for
well-understood
problem
In
the
prototyping
model
a
prototype
is
built
before
building
the
final
system
which
is
used
to
further
develop
the
requirement
leading
to
more
stable
requirement
This
is
useful
for
project
where
requirement
are
not
clear
In
the
iterative
development
model
software
is
developed
in
iteration
each
iteration
resulting
in
a
working
software
system
This
model
doe
not
require
all
requirement
to
be
known
at
the
start
allows
feedback
from
earlier
iteration
for
next
one
and
reduces
risk
a
it
delivers
value
a
the
project
proceeds
In
RUP
a
project
is
executed
in
a
sequence
of
four
phases—inception
elab-
oration
construction
and
transition
each
ending
in
a
defined
milestone
A
phase
may
itself
be
done
iteratively
The
subprocesses
of
requirement
de-
sign
coding
testing
etc
are
considered
a
active
throughout
the
project
though
their
intensity
varies
from
phase
to
phase
RUP
is
a
flexible
frame-
work
which
can
allow
a
project
to
follow
a
traditional
waterfall
if
it
want
to
or
allow
prototyping
if
it
so
wish
In
the
timeboxing
model
the
different
iteration
are
of
equal
time
duration
and
are
divided
into
equal
length
stage
There
is
a
committed
team
for
each
stage
of
an
iteration
The
different
iteration
are
then
executed
in
a
pipelined
manner
with
each
dedicated
team
working
on
it
stage
but
for
different
iteration
As
multiple
iteration
are
concurrently
active
this
model
reduces
the
average
completion
time
of
each
iteration
and
hence
is
useful
in
situation
where
short
cycle
time
is
highly
desirable
Agile
approach
to
development
are
based
on
some
key
principle
like
developing
software
in
small
iteration
working
system
a
the
measure
of
progress
and
allowing
change
at
any
time
In
extreme
programming
XP
approach
a
project
start
with
short
user
story
detail
of
which
are
obtained
in
the
iteration
in
which
they
are
implemented
In
an
iteration
development
is
done
by
programmer-pairs
following
the
practice
of
test-
driven
development
frequent
integration
and
having
simple
design
which
are
refactored
when
needed
The
project
management
process
focus
on
planning
and
controlling
the
de-
velopment
process
and
consists
of
three
major
phases—planning
monitoring
and
control
and
termination
analysis
Much
of
project
management
revolves
around
the
project
plan
which
is
produced
during
the
planning
phase
What
is
the
relationship
between
a
process
model
process
specification
and
process
for
a
project
What
are
the
key
output
during
an
iteration
in
a
project
following
an
iterative
development
model
Which
of
the
development
process
model
discussed
in
this
chapter
would
you
employ
for
the
following
project
A
data
entry
system
for
office
staff
who
have
never
used
computer
before
The
user
interface
and
user-friendliness
are
extremely
important
A
spreadsheet
system
that
ha
some
basic
feature
and
many
other
desirable
feature
that
use
these
basic
feature
A
web-based
system
for
a
new
business
where
requirement
are
changing
fast
and
where
an
in-house
development
team
is
available
for
all
aspect
of
the
project
A
Web-site
for
an
on-line
store
which
ha
a
long
list
of
desired
feature
it
want
to
add
and
it
want
a
new
release
with
new
feature
to
be
done
very
frequently
A
project
us
the
timeboxing
process
model
with
three
stage
in
each
time
box
a
discussed
in
the
chapter
but
with
unequal
length
Suppose
the
requirement
specification
stage
take
the
build
stage
take
and
deployment
take
Design
the
process
for
this
project
that
maximizes
resource
utilization
Assume
that
each
resource
can
do
any
task
Hint
Exploit
the
fact
that
the
sum
of
duration
of
the
first
and
the
third
stage
is
equal
to
the
duration
of
the
second
stage
What
effect
is
the
project
monitoring
activity
likely
to
have
on
the
development
process
IEEE
defines
a
requirement
a
1
A
condition
of
capability
needed
by
a
user
to
solve
a
problem
or
achieve
an
objective
2
A
condition
or
a
capability
that
must
be
met
or
possessed
by
a
system
...
to
satisfy
a
contract
standard
specification
or
other
formally
imposed
document
53
Note
that
in
software
requirement
we
are
dealing
with
the
requirement
of
the
proposed
system
that
is
the
capability
that
the
system
which
is
yet
to
be
developed
should
have
As
we
have
seen
all
development
model
require
requirement
to
be
speci-
fied
Approaches
like
agile
require
only
high-level
requirement
to
be
specified
in
written
form—detailed
requirement
are
elicited
through
interaction
with
the
customer
in
the
iteration
the
requirement
is
to
be
implemented
and
di-
rectly
reflected
in
software
Other
approach
prefer
that
the
requirement
are
specified
precisely
In
such
situation
the
goal
of
the
requirement
activity
is
to
produce
the
Software
Requirements
Specification
SRS
that
describes
what
the
proposed
software
should
do
without
describing
how
the
software
will
do
The
role
of
the
SRS
in
a
project
and
the
value
a
good
SRS
brings
to
it
The
different
activity
in
the
process
for
producing
the
desired
SRS
The
desired
characteristic
of
an
SRS
the
structure
of
an
SRS
document
and
it
key
component
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
The
use
case
approach
for
analyzing
and
specifying
functional
requirement
and
how
use
case
can
be
developed
Some
other
approach
for
analyzing
requirement
like
the
data
flow
diagram
The
origin
of
most
software
system
is
in
the
need
of
some
client
The
software
system
itself
is
created
by
some
developer
Finally
the
completed
system
will
be
used
by
the
end
user
Thus
there
are
three
major
party
interested
in
a
new
system
the
client
the
developer
and
the
user
Somehow
the
requirement
for
the
system
that
will
satisfy
the
need
of
the
client
and
the
concern
of
the
user
have
to
be
communicated
to
the
developer
The
problem
is
that
the
client
usually
doe
not
understand
software
or
the
software
development
process
and
the
developer
often
doe
not
understand
the
client
’
s
problem
and
application
area
This
cause
a
communication
gap
between
the
party
involved
in
the
de-
velopment
project
A
basic
purpose
of
the
SRS
is
to
bridge
this
communication
gap
so
they
have
a
shared
vision
of
the
software
being
built
Hence
one
of
the
main
advantage
of
a
good
SRS
is
An
SRS
establishes
the
basis
for
agreement
between
the
client
and
the
sup-
plier
on
what
the
software
product
will
do
This
basis
for
agreement
is
frequently
formalized
into
a
legal
contract
between
the
client
or
the
customer
and
the
developer
the
supplier
So
through
SRS
the
client
clearly
describes
what
it
expects
from
the
supplier
and
the
developer
clearly
understands
what
capability
to
build
in
the
software
A
related
but
important
advantage
is
An
SRS
provides
a
reference
for
validation
of
the
final
product
That
is
the
SRS
help
the
client
determine
if
the
software
meet
the
require-
ments
Without
a
proper
SRS
there
is
no
way
a
client
can
determine
if
the
software
being
delivered
is
what
wa
ordered
and
there
is
no
way
the
developer
can
convince
the
client
that
all
the
requirement
have
been
fulfilled
Providing
the
basis
of
agreement
and
validation
should
be
strong
enough
reason
for
both
the
client
and
the
developer
to
do
a
thorough
and
rigorous
job
of
requirement
understanding
and
specification
but
there
are
other
very
practical
and
pressing
reason
for
having
a
good
SRS
Studies
have
shown
that
many
error
are
made
during
the
requirement
phase
And
an
error
in
the
SRS
will
manifest
itself
a
an
error
in
the
final
system
implementing
the
SRS
Clearly
if
we
want
a
high-quality
end
product
that
ha
few
error
we
must
begin
with
a
high-quality
SRS
In
other
word
we
can
conclude
that
A
high-quality
SRS
is
a
prerequisite
to
high-quality
software
Finally
the
quality
of
SRS
ha
an
impact
on
cost
and
schedule
of
the
project
We
know
that
error
can
exist
in
the
SRS
It
is
also
known
that
the
cost
of
fixing
an
error
increase
almost
exponentially
a
time
progress
10
12
Hence
by
improving
the
quality
of
requirement
we
can
have
a
huge
saving
in
the
future
by
having
fewer
expensive
defect
removal
In
other
word
The
requirement
process
is
the
sequence
of
activity
that
need
to
be
performed
in
the
requirement
phase
and
that
culminate
in
producing
a
high-quality
doc-
ument
containing
the
SRS
The
requirement
process
typically
consists
of
three
basic
task
problem
or
requirement
analysis
requirement
specification
and
requirement
validation
Problem
analysis
often
start
with
a
high-level
problem
statement.
Dur-
ing
analysis
the
problem
domain
and
the
environment
are
modeled
in
an
effort
to
understand
the
system
behavior
constraint
on
the
system
it
input
and
output
etc
The
basic
purpose
of
this
activity
is
to
obtain
a
thorough
under-
standing
of
what
the
software
need
to
provide
Frequently
during
analysis
the
analyst
will
have
a
series
of
meeting
with
the
client
and
end
user
In
the
early
meeting
the
client
and
end
user
will
explain
to
the
analyst
about
their
work
their
environment
and
their
need
a
they
perceive
them
Any
document
de-
scribing
the
work
or
the
organization
may
be
given
along
with
output
of
the
existing
method
of
performing
the
task
In
these
early
meeting
the
analyst
is
basically
the
listener
absorbing
the
information
provided
Once
the
analyst
understands
the
system
to
some
extent
he
us
the
next
few
meeting
to
seek
clarification
of
the
part
he
doe
not
understand
He
may
document
the
infor-
mation
or
build
some
model
and
he
may
do
some
brainstorming
or
thinking
about
what
the
system
should
do
In
the
final
few
meeting
the
analyst
essen-
tially
explains
to
the
client
what
he
understands
the
system
should
do
and
us
the
meeting
a
a
mean
of
verifying
if
what
he
proposes
the
system
should
do
is
indeed
consistent
with
the
objective
of
the
client
The
understanding
obtained
by
problem
analysis
form
the
basis
of
require-
ments
specification
in
which
the
focus
is
on
clearly
specifying
the
requirement
in
a
document
Issues
such
a
representation
specification
language
and
tool
are
addressed
during
this
activity
As
analysis
produce
large
amount
of
in-
formation
and
knowledge
with
possible
redundancy
properly
organizing
and
describing
the
requirement
is
an
important
goal
of
this
activity
Requirements
validation
focus
on
ensuring
that
what
have
been
specified
in
the
SRS
are
indeed
all
the
requirement
of
the
software
and
making
sure
that
the
SRS
is
of
good
quality
The
requirement
process
terminates
with
the
production
of
the
validated
SRS
We
will
discus
this
more
later
in
the
chapter
It
should
be
pointed
out
that
the
requirement
process
is
not
a
linear
se-
quence
of
these
three
activity
and
there
is
considerable
overlap
and
feedback
between
these
activity
The
overall
requirement
process
is
shown
in
Figure
As
shown
in
the
figure
from
the
specification
activity
we
may
go
back
to
the
analysis
activity
This
happens
a
frequently
some
part
of
the
problem
are
analyzed
and
then
specified
before
other
part
are
analyzed
and
speci-
fied
Furthermore
the
process
of
specification
frequently
show
shortcoming
in
the
knowledge
of
the
problem
thereby
necessitating
further
analysis
Once
the
specification
is
done
it
go
through
the
validation
activity
This
activity
may
reveal
problem
in
the
specification
itself
which
requires
going
back
to
the
specification
step
or
may
reveal
shortcoming
in
the
understanding
of
the
problem
which
requires
going
back
to
the
analysis
activity
The
final
output
is
the
SRS
document
As
analysis
precedes
specification
the
first
question
that
arises
is
If
formal
modeling
is
done
during
analysis
why
are
the
output
of
modeling
not
treated
a
an
SRS
The
main
reason
is
that
modeling
generally
focus
on
the
problem
structure
not
it
external
behavior
Consequently
thing
like
user
interface
are
rarely
modeled
whereas
they
fre-
quently
form
a
major
component
of
the
SRS
Similarly
for
ease
of
modeling
frequently
minor
issue
like
erroneous
situation
e.g.
error
in
output
are
rarely
modeled
properly
whereas
in
an
SRS
behavior
under
such
situation
also
ha
to
be
specified
Similarly
performance
constraint
design
constraint
standard
compliance
recovery
etc.
are
not
included
in
the
model
but
must
be
specified
clearly
in
the
SRS
because
the
designer
must
know
about
these
to
properly
design
the
system
It
should
therefore
be
clear
that
the
output
of
a
model
can
not
form
a
desirable
SRS
The
transition
from
analysis
to
specification
should
also
not
be
expected
to
be
straightforward
even
if
some
formal
modeling
is
used
during
analysis
A
good
SRS
need
to
specify
many
thing
some
of
which
are
not
satisfactorily
handled
during
analysis
Essentially
what
pass
from
requirement
analysis
activity
to
the
specification
activity
is
the
knowledge
acquired
about
the
sys-
tem
The
modeling
is
essentially
a
tool
to
help
obtain
a
thorough
and
com-
plete
knowledge
about
the
proposed
system
The
SRS
is
written
based
on
the
knowledge
acquired
during
analysis
As
converting
knowledge
into
a
structured
document
is
not
straightforward
specification
itself
is
a
major
task
which
is
relatively
independent
To
properly
satisfy
the
basic
goal
an
SRS
should
have
certain
property
and
should
contain
different
type
of
requirement
Some
of
the
desirable
charac-
teristics
of
an
SRS
are
53
An
SRS
is
correct
if
every
requirement
included
in
the
SRS
represents
something
required
in
the
final
system
It
is
complete
if
everything
the
software
is
supposed
to
do
and
the
response
of
the
software
to
all
class
of
input
data
are
specified
in
the
SRS
It
is
unambiguous
if
and
only
if
every
requirement
stated
ha
one
and
only
one
interpretation
Requirements
are
often
written
in
natural
language
which
is
inherently
ambiguous
If
the
requirement
are
specified
in
a
natural
language
the
SRS
writer
ha
to
be
especially
careful
to
ensure
that
there
are
no
ambiguity
An
SRS
is
verifiable
if
and
only
if
every
stated
requirement
is
verifiable
A
requirement
is
verifiable
if
there
exists
some
cost-effective
process
that
can
check
whether
the
final
software
meet
that
requirement
It
is
consistent
if
there
is
no
requirement
that
conflict
with
another
Terminology
can
cause
inconsistency
for
example
different
requirement
may
use
different
term
to
refer
to
the
same
object
There
may
be
logical
or
temporal
conflict
between
requirement
that
cause
inconsistency
This
occurs
if
the
SRS
contains
two
or
more
requirement
whose
logical
or
temporal
characteristic
can
not
be
satisfied
together
by
any
software
system
For
example
suppose
a
requirement
state
that
an
event
e
is
to
occur
before
another
event
f
But
then
another
set
of
requirement
state
directly
or
indirectly
by
transitivity
that
event
f
should
occur
before
event
e.
Inconsistencies
in
an
SRS
can
reflect
some
major
problem
Generally
all
the
requirement
for
software
are
not
of
equal
importance
Some
are
critical
others
are
important
but
not
critical
and
there
are
some
which
are
desirable
but
not
very
important
Similarly
some
requirement
are
core
requirement
which
are
not
likely
to
change
a
time
pass
while
others
are
more
dependent
on
time
Some
provide
more
value
to
the
user
than
oth-
er
An
SRS
is
ranked
for
importance
and/or
stability
if
for
each
requirement
the
importance
and
the
stability
of
the
requirement
are
indicated
Stability
of
a
requirement
reflects
the
chance
of
it
changing
in
the
future
It
can
be
reflected
in
term
of
the
expected
change
volume
This
understanding
of
value
each
requirement
provides
is
essential
for
iterative
development—selection
of
requirement
for
an
iteration
is
based
on
this
evaluation
Of
all
these
characteristic
completeness
is
perhaps
the
most
important
and
also
the
most
difficult
property
to
establish
One
of
the
most
common
defect
in
requirement
specification
is
incompleteness
Missing
requirement
necessitate
addition
and
modification
to
the
requirement
later
in
the
development
cycle
which
are
often
expensive
to
incorporate
Incompleteness
is
also
a
major
source
of
disagreement
between
the
client
and
the
supplier
Some
however
believe
that
completeness
in
all
detail
may
not
be
desirable
The
pursuit
of
completeness
can
lead
to
specifying
detail
and
assumption
that
may
be
commonly
understood
For
example
specifying
in
detail
what
a
common
operation
like
add
a
record
mean
And
specifying
these
detail
can
result
in
a
large
requirement
document
which
ha
it
own
problem
including
making
validation
harder
On
the
other
hand
if
too
few
detail
are
given
the
chance
of
developer
’
s
understanding
being
different
from
others
’
increase
which
can
lead
to
defect
in
the
software
For
completeness
a
reasonable
goal
is
to
have
sufficient
detail
for
the
project
at
hand
For
example
if
the
waterfall
model
is
to
be
followed
in
the
project
it
is
better
to
have
detailed
specification
so
the
need
for
change
is
minimized
On
the
other
hand
for
iterative
development
a
feedback
is
possible
and
opportunity
for
change
is
also
there
the
specification
can
be
le
detailed
And
if
an
agile
approach
is
being
followed
then
completeness
should
be
sought
only
for
top-level
requirement
a
detail
may
not
be
required
in
written
form
and
are
elicited
when
the
requirement
is
being
implemented
Together
the
performance
and
interface
requirement
and
design
constraint
can
be
called
nonfunctional
requirement
Completeness
of
specification
is
difficult
to
achieve
and
even
more
difficult
to
verify
Having
guideline
about
what
different
thing
an
SRS
should
specify
will
help
in
completely
specifying
the
requirement
The
basic
issue
an
SRS
must
address
are
Functional
requirement
specify
the
expected
behavior
of
the
system—which
output
should
be
produced
from
the
given
input
They
describe
the
relation-
ship
between
the
input
and
output
of
the
system
For
each
functional
require-
ment
a
detailed
description
of
all
the
data
input
and
their
source
the
unit
of
measure
and
the
range
of
valid
input
must
be
specified
All
the
operation
to
be
performed
on
the
input
data
to
obtain
the
output
should
be
specified
This
includes
specifying
the
validity
check
on
the
input
and
output
data
parameter
affected
by
the
operation
and
equation
or
other
logical
operation
that
must
be
used
to
transform
the
input
into
corresponding
output
For
example
if
there
is
a
formula
for
computing
the
output
it
should
be
specified
An
important
part
of
the
specification
is
the
system
behavior
in
abnormal
situation
like
invalid
input
which
can
occur
in
many
way
or
error
during
computation
The
functional
requirement
must
clearly
state
what
the
system
should
do
if
such
situation
occur
Specifically
it
should
specify
the
behavior
of
the
system
for
invalid
input
and
invalid
output
Furthermore
behavior
for
situation
where
the
input
is
valid
but
the
normal
operation
can
not
be
performed
should
also
be
specified
An
example
of
this
situation
is
a
reservation
system
where
a
reservation
can
not
be
made
even
for
a
valid
request
if
there
is
no
availability
In
short
the
system
behavior
for
all
foreseen
input
and
all
foreseen
system
state
should
be
specified
The
performance
requirement
part
of
an
SRS
specifies
the
performance
constraint
on
the
software
system
All
the
requirement
relating
to
the
per-
formance
characteristic
of
the
system
must
be
clearly
specified
There
are
two
type
of
performance
requirement
static
and
dynamic
Static
requirement
are
those
that
do
not
impose
constraint
on
the
execution
characteristic
of
the
system
These
include
requirement
like
the
number
of
terminal
to
be
supported
the
number
of
simultaneous
user
to
be
supported
and
the
number
of
file
that
the
system
ha
to
process
and
their
size
These
are
also
called
capacity
requirement
of
the
system
Dynamic
requirement
specify
constraint
on
the
execution
behavior
of
the
system
These
typically
include
response
time
and
throughput
constraint
on
the
system
Response
time
is
the
expected
time
for
the
completion
of
an
op-
eration
under
specified
circumstance
Throughput
is
the
expected
number
of
operation
that
can
be
performed
in
a
unit
time
For
example
the
SRS
may
specify
the
number
of
transaction
that
must
be
processed
per
unit
time
or
what
the
response
time
for
a
particular
command
should
be
Acceptable
range
of
the
different
performance
parameter
should
be
specified
a
well
a
accept-
able
performance
for
both
normal
and
peak
workload
condition
All
of
these
requirement
should
be
stated
in
measurable
term
Require-
ments
such
a
response
time
should
be
good
or
the
system
must
be
able
to
process
all
the
transaction
quickly
are
not
desirable
because
they
are
impre-
cise
and
not
verifiable
Instead
statement
like
the
response
time
of
command
x
should
be
le
than
one
second
90
%
of
the
time
or
a
transaction
should
be
processed
in
le
than
one
second
98
%
of
the
time
should
be
used
to
declare
performance
specification
There
are
a
number
of
factor
in
the
client
’
s
environment
that
may
restrict
the
choice
of
a
designer
leading
to
design
constraint
Such
factor
include
standard
that
must
be
followed
resource
limit
operating
environment
relia-
bility
and
security
requirement
and
policy
that
may
have
an
impact
on
the
design
of
the
system
An
SRS
should
identify
and
specify
all
such
constraint
Some
example
of
these
are
Standards
Compliance
This
specifies
the
requirement
for
the
standard
the
system
must
follow
The
standard
may
include
the
report
format
and
accounting
procedure
There
may
be
audit
requirement
which
may
require
logging
of
operation
Hardware
Limitations
The
software
may
have
to
operate
on
some
ex-
isting
or
predetermined
hardware
thus
imposing
restriction
on
the
design
Hardware
limitation
can
include
the
type
of
machine
to
be
used
operating
system
available
on
the
system
language
supported
and
limit
on
primary
and
secondary
storage
Reliability
and
Fault
Tolerance
Fault
tolerance
requirement
can
place
a
major
constraint
on
how
the
system
is
to
be
designed
a
they
make
the
system
more
complex
and
expensive
Recovery
requirement
are
often
an
integral
part
here
detailing
what
the
system
should
do
if
some
failure
occurs
to
ensure
certain
property
Security
Security
requirement
are
becoming
increasingly
important
These
requirement
place
restriction
on
the
use
of
certain
command
control
access
to
data
provide
different
kind
of
access
requirement
for
different
people
re-
quire
the
use
of
password
and
cryptography
technique
and
maintain
a
log
of
activity
in
the
system
They
may
also
require
proper
assessment
of
security
threat
proper
programming
technique
and
use
of
tool
to
detect
flaw
like
buffer
overflow
In
the
external
interface
specification
part
all
the
interaction
of
the
soft-
ware
with
people
hardware
and
other
software
should
be
clearly
specified
For
the
user
interface
the
characteristic
of
each
user
interface
of
the
software
prod-
uct
should
be
specified
User
interface
is
becoming
increasingly
important
and
must
be
given
proper
attention
A
preliminary
user
manual
should
be
created
with
all
user
command
screen
format
an
explanation
of
how
the
system
will
appear
to
the
user
and
feedback
and
error
message
Like
other
specification
these
requirement
should
be
precise
and
verifiable
So
a
statement
like
the
system
should
be
user
friendly
should
be
avoided
and
statement
like
com-
mands
should
be
no
longer
than
six
character
or
command
name
should
reflect
the
function
they
perform
used
For
hardware
interface
requirement
the
SRS
should
specify
the
logical
characteristic
of
each
interface
between
the
software
product
and
the
hard-
ware
component
If
the
software
is
to
execute
on
existing
hardware
or
on
pre-
determined
hardware
all
the
characteristic
of
the
hardware
including
memory
restriction
should
be
specified
In
addition
the
current
use
and
load
charac-
teristics
of
the
hardware
should
be
given
The
interface
requirement
should
specify
the
interface
with
other
software
the
system
will
use
or
that
will
use
the
system
This
includes
the
interface
with
the
operating
system
and
other
application
The
message
content
and
format
of
each
interface
should
be
specified
Requirements
have
to
be
specified
using
some
specification
language
Though
formal
notation
exist
for
specifying
specific
property
of
the
system
natural
language
are
now
most
often
used
for
specifying
requirement
When
formal
language
are
employed
they
are
often
used
to
specify
particular
property
or
for
specific
part
of
the
system
a
part
of
the
overall
SRS
All
the
requirement
for
a
system
stated
using
a
formal
notation
or
natural
language
have
to
be
included
in
a
document
that
is
clear
and
concise
For
this
it
is
necessary
to
properly
organize
the
requirement
document
Here
we
discus
the
organization
based
on
the
IEEE
guide
to
software
requirement
specification
53
The
IEEE
standard
recognize
the
fact
that
different
project
may
require
their
requirement
to
be
organized
differently
that
is
there
is
no
one
method
that
is
suitable
for
all
project
It
provides
different
way
of
structuring
the
SRS
The
first
two
section
of
the
SRS
are
the
same
in
all
of
them
The
general
structure
of
an
SRS
is
given
in
Figure
The
introduction
section
contains
the
purpose
scope
overview
etc.
of
the
requirement
document
The
key
aspect
here
is
to
clarify
the
motivation
and
business
objective
that
are
driving
this
project
and
the
scope
of
the
project
The
next
section
give
an
overall
perspective
of
the
system—how
it
fit
into
the
larger
system
and
an
overview
of
all
the
requirement
of
this
system
Detailed
requirement
are
not
mentioned
Product
perspective
is
essentially
the
relationship
of
the
product
to
other
product
defining
if
the
product
is
independent
or
is
a
part
of
a
larger
product
and
what
the
principal
interface
of
the
product
are
A
general
abstract
description
of
the
function
to
be
performed
by
the
product
is
given
Schematic
diagram
showing
a
general
view
of
different
function
and
their
relationship
with
each
other
can
often
be
useful
Similarly
typical
characteristic
of
the
eventual
end
user
and
general
constraint
are
also
specified
If
agile
method
are
being
used
this
may
be
sufficient
for
the
initial
require-
ments
phase
a
these
approach
prefer
to
do
the
detailed
requirement
when
the
requirement
is
to
be
implemented
The
detailed
requirement
section
describes
the
detail
of
the
requirement
that
a
developer
need
to
know
for
designing
and
developing
the
system
This
is
typically
the
largest
and
most
important
part
of
the
document
For
this
section
different
organization
have
been
suggested
in
the
standard
These
re-
quirements
can
be
organized
by
the
mode
of
operation
user
class
object
feature
stimulus
or
functional
hierarchy
53
One
method
to
organize
the
spe-
cific
requirement
is
to
first
specify
the
external
interface
followed
by
func-
tional
requirement
performance
requirement
design
constraint
and
system
attribute
This
structure
is
shown
in
Figure
53
The
external
interface
requirement
section
specifies
all
the
interface
of
the
software
to
people
other
software
hardware
and
other
system
User
interface
are
clearly
a
very
important
component
they
specify
each
human
interface
the
system
plan
to
have
including
screen
format
content
of
menu
and
command
structure
In
hardware
interface
the
logical
characteristic
of
each
interface
between
the
software
and
hardware
on
which
the
software
can
run
are
specified
Essentially
any
assumption
the
software
is
making
about
the
hardware
are
listed
here
In
software
interface
all
other
software
that
is
needed
for
this
software
to
run
is
specified
along
with
the
interface
Communication
interface
need
to
be
specified
if
the
software
communicates
with
other
entity
in
other
machine
In
the
functional
requirement
section
the
functional
capability
of
the
system
are
described
In
this
organization
the
functional
capability
for
all
the
mode
of
operation
of
the
software
are
given
For
each
functional
requirement
the
required
input
desired
output
and
processing
requirement
will
have
to
be
specified
For
the
input
the
source
of
the
input
the
unit
of
measure
valid
range
accuracy
etc.
have
to
be
specified
For
specifying
the
processing
all
operation
that
need
to
be
performed
on
the
input
data
and
any
intermediate
data
produced
should
be
specified
This
includes
validity
check
on
input
sequence
of
operation
response
to
abnormal
situation
and
method
that
must
be
used
in
processing
to
transform
the
input
into
corresponding
output
The
performance
section
should
specify
both
static
and
dynamic
perfor-
mance
requirement
All
factor
that
constrain
the
system
design
are
described
in
the
performance
constraint
section
The
attribute
section
specifies
some
of
the
overall
attribute
that
the
system
should
have
Any
requirement
not
covered
under
these
is
listed
under
other
requirement
Design
constraint
specify
all
the
constraint
imposed
on
design
e.g.
security
fault
tolerance
and
standard
When
use
case
are
employed
then
the
functional
requirement
section
of
the
SRS
is
replaced
by
use
case
description
And
the
product
perspective
part
of
the
SRS
may
provide
an
overview
or
summary
of
the
use
case
Functional
requirement
often
form
the
core
of
a
requirement
document
The
traditional
approach
for
specifying
functionality
is
to
specify
each
function
that
the
system
should
provide
Use
case
specify
the
functionality
of
a
system
by
specifying
the
behavior
of
the
system
captured
a
interaction
of
the
user
with
the
system
Use
case
can
be
used
to
describe
the
business
process
of
the
larger
business
or
organization
that
deploys
the
software
or
it
could
just
describe
the
behavior
of
the
software
system
We
will
focus
on
describing
the
behavior
of
software
system
that
are
to
be
built
Though
use
case
are
primarily
for
specifying
behavior
they
can
also
be
used
effectively
for
analysis
Later
when
we
discus
how
to
develop
use
case
we
will
discus
how
they
can
help
in
eliciting
requirement
also
Use
case
drew
attention
after
they
were
used
a
part
of
the
object-oriented
modeling
approach
proposed
by
Jacobson
56
Due
to
this
connection
with
an
object-oriented
approach
use
case
are
sometimes
viewed
a
part
of
an
object-oriented
approach
to
software
development
However
they
are
a
general
method
for
describing
the
interaction
of
a
system
even
non-IT
system
The
discussion
of
use
case
here
is
based
on
the
concept
and
process
discussed
in
24
A
software
system
in
our
case
whose
requirement
are
being
uncovered
may
be
used
by
many
user
or
by
other
system
In
use
case
terminology
an
actor
is
a
person
or
a
system
which
us
the
system
for
achieving
some
goal
Note
that
a
an
actor
interacts
for
achieving
some
goal
it
is
a
logical
entity
that
represents
a
group
of
user
people
or
system
who
behave
in
a
similar
manner
Different
actor
represent
group
with
different
goal
So
it
is
better
to
have
a
receiver
and
a
sender
actor
rather
than
having
a
generic
user
actor
for
a
system
in
which
some
message
are
sent
by
user
and
received
by
some
other
user
A
primary
actor
is
the
main
actor
that
initiate
a
use
case
UC
for
achieving
a
goal
and
whose
goal
satisfaction
is
the
main
objective
of
the
use
case
The
primary
actor
is
a
logical
concept
and
though
we
assume
that
the
primary
actor
executes
the
use
case
some
agent
may
actually
execute
it
on
behalf
of
the
primary
actor
For
example
a
VP
may
be
the
primary
actor
for
get
sale
growth
report
by
region
use
case
though
it
may
actually
be
executed
by
an
assistant
We
consider
the
primary
actor
a
the
person
who
actually
us
the
outcome
of
the
use
case
and
who
is
the
main
consumer
of
the
goal
Time-driven
trigger
is
another
example
of
how
a
use
case
may
be
executed
on
behalf
of
the
primary
actor
in
this
situation
the
report
is
generated
automatically
at
some
time
Note
however
that
although
the
goal
of
the
primary
actor
is
the
driving
force
behind
a
use
case
the
use
case
must
also
fulfill
goal
that
other
stake-
holder
might
have
for
this
use
case
That
is
the
main
goal
of
a
use
case
is
to
describe
behavior
of
the
system
that
result
in
satisfaction
of
the
goal
of
all
the
stakeholder
although
the
use
case
may
be
driven
by
the
goal
of
the
primary
actor
For
example
a
use
case
Withdraw
money
from
the
ATM
ha
a
customer
a
it
primary
actor
and
will
normally
describe
the
entire
interac-
tion
of
the
customer
with
the
ATM
However
the
bank
is
also
a
stakeholder
of
the
ATM
system
and
it
goal
may
include
that
all
step
are
logged
money
is
given
only
if
there
are
sufficient
fund
in
the
account
and
no
more
than
some
amount
is
given
at
a
time
etc
Satisfaction
of
these
goal
should
also
be
shown
by
the
use
case
Withdraw
money
from
the
ATM
For
describing
interaction
use
case
use
scenario
A
scenario
describes
a
set
of
action
that
are
performed
to
achieve
a
goal
under
some
specified
con-
ditions
The
set
of
action
is
generally
specified
a
a
sequence
a
that
is
the
most
convenient
way
to
express
it
in
text
though
in
actual
execution
the
ac-
tions
specified
may
be
executed
in
parallel
or
in
some
different
order
Each
step
in
a
scenario
is
a
logically
complete
action
performed
either
by
the
actor
or
the
system
Generally
a
step
is
some
action
by
the
actor
e.g.
enter
informa-
tion
some
logical
step
that
the
system
performs
to
progress
toward
achieving
it
goal
e.g.
validate
information
deliver
information
or
an
internal
state
change
by
the
system
to
satisfy
some
goal
e.g.
log
the
transaction
update
the
record
A
use
case
always
ha
a
main
success
scenario
which
describes
the
interac-
tion
if
nothing
fails
and
all
step
in
the
scenario
succeed
There
may
be
many
success
scenario
Though
the
use
case
aim
to
achieve
it
goal
different
situ-
ations
can
arise
while
the
system
and
the
actor
are
interacting
which
may
not
permit
the
system
to
achieve
the
goal
fully
For
these
situation
a
use
case
ha
extension
scenario
which
describe
the
system
behavior
if
some
of
the
step
in
the
main
scenario
do
not
complete
successfully
Sometimes
they
are
also
called
exception
scenario
A
use
case
is
a
collection
of
all
the
success
and
extension
scenario
related
to
the
goal
The
terminology
of
use
case
is
summarized
in
Table
To
achieve
the
desired
goal
a
system
can
divide
it
into
subgoals
Some
of
these
subgoals
may
be
achieved
by
the
system
itself
but
they
may
also
be
treated
a
separate
use
case
executed
by
supporting
actor
which
may
be
another
system
For
example
suppose
for
verifying
a
user
in
Withdraw
money
from
the
ATM
an
authentication
service
is
used
The
interaction
with
this
service
can
be
treated
a
a
separate
use
case
A
scenario
in
a
use
case
may
therefore
employ
another
use
case
for
performing
some
of
the
task
In
other
word
use
case
permit
a
hierarchic
organization
It
should
be
evident
that
the
basic
system
model
that
use
case
assume
is
that
a
system
primarily
responds
to
request
from
actor
who
use
the
system
By
describing
the
interaction
between
actor
and
the
system
the
system
be-
havior
can
be
specified
and
through
the
behavior
it
functionality
is
specified
A
key
advantage
of
this
approach
is
that
use
case
focus
on
external
behavior
thereby
cleanly
avoiding
doing
internal
design
during
requirement
something
that
is
desired
but
not
easy
to
do
with
many
modeling
approach
Use
case
are
naturally
textual
description
and
represent
the
behavioral
requirement
of
the
system
This
behavior
specification
can
capture
most
of
the
functional
requirement
of
the
system
Therefore
use
case
do
not
form
the
complete
SRS
but
can
form
a
part
of
it
The
complete
SRS
a
we
have
seen
will
need
to
capture
other
requirement
like
performance
and
design
constraint
Though
the
detailed
use
case
are
textual
diagram
can
be
used
to
sup-
plement
the
textual
description
For
example
the
use
case
diagram
of
UML
provides
an
overview
of
the
use
case
and
actor
in
the
system
and
their
depen-
dency
A
UML
use
case
diagram
generally
show
each
use
case
in
the
system
a
an
ellipse
show
the
primary
actor
for
the
use
case
a
a
stick
figure
connected
to
the
use
case
with
a
line
and
show
dependency
between
use
case
by
arc
between
use
case
Some
other
relationship
between
use
case
can
also
be
rep-
resented
However
a
use
case
are
basically
textual
in
nature
diagram
play
a
limited
role
in
either
developing
or
specifying
use
case
We
will
not
discus
use
case
diagram
further
Let
u
illustrate
these
concept
with
a
few
use
case
which
we
will
also
use
to
explain
other
concept
related
to
use
case
Let
u
consider
that
a
small
on-line
auction
system
is
to
be
built
for
a
university
community
called
the
University
Auction
System
UAS
through
which
different
member
of
the
university
can
sell
and
buy
good
We
will
assume
that
there
is
a
separate
financial
subsystem
through
which
the
payment
are
made
and
that
each
buyer
and
seller
ha
an
account
in
it
In
this
system
though
we
have
the
same
people
who
might
be
buying
and
selling
we
have
buyer
and
seller
a
separate
logical
actor
a
both
have
different
goal
to
achieve
Besides
these
the
auction
system
itself
is
a
stake-
holder
and
an
actor
The
financial
system
is
another
Let
u
first
consider
the
main
use
case
of
this
system—
put
an
item
for
auction
make
a
bid
and
complete
an
auction.
These
use
case
are
given
in
Figure
The
use
case
are
self-explanatory
This
is
the
great
value
of
use
cases—they
are
natural
and
story-like
which
make
them
easy
to
understand
by
both
an
an-
alyst
and
a
layman
This
help
considerably
in
minimizing
the
communication
gap
between
the
developer
and
other
stakeholder
Some
point
about
the
use
case
are
worth
discussing
The
use
case
are
generally
numbered
for
reference
purpose
The
name
of
the
use
case
specifies
the
goal
of
the
primary
actor
hence
there
is
no
separate
line
specifying
the
goal
The
primary
actor
can
be
a
person
or
a
system—for
UC2
they
are
person
but
for
UC3
it
is
a
system
The
primary
actor
can
also
be
another
software
which
might
request
a
service
The
precondition
of
a
use
case
specifies
what
the
system
will
ensure
before
allowing
the
use
case
to
be
initiated
Common
precondition
are
user
is
logged
in
input
data
exists
in
file
or
other
data
structure
etc
For
an
operation
like
delete
it
may
be
that
item
exists
or
for
a
tracking
use
case
it
may
be
that
the
tracking
number
is
valid
It
is
worth
noting
that
the
use
case
description
list
contains
some
action
that
are
not
necessarily
tied
to
the
goal
of
the
primary
actor
For
example
the
last
step
in
UC
This
action
is
clearly
not
needed
by
the
current
bidder
for
his
goal
However
a
the
system
and
other
bidder
are
also
stakeholder
for
this
use
case
the
use
case
ha
to
ensure
that
their
goal
are
also
satisfied
Similar
is
the
case
with
the
last
item
of
UC1
The
exception
situation
are
also
fairly
clear
We
have
listed
only
the
most
obvious
one
There
can
be
many
more
depending
on
the
goal
of
the
organiza-
tion
For
example
there
could
be
one
user
doe
not
complete
the
transaction
which
is
a
failure
condition
that
can
occur
anywhere
What
should
be
done
in
UC1
Put
an
item
for
auction
Primary
Actor
Seller
Precondition
Seller
ha
logged
in
Main
Success
Scenario
Seller
post
an
item
it
category
description
picture
etc
for
auction
System
show
past
price
of
similar
item
to
seller
Seller
specifies
the
starting
bid
price
and
a
date
when
auction
will
close
System
show
the
rating
of
the
seller
the
starting
bid
the
current
bid
and
the
highest
bid
asks
buyer
to
make
a
bid
System
accepts
the
bid
Blocks
fund
in
bidder
account
System
update
the
max
bid
price
informs
other
user
and
update
the
record
for
the
item
The
bidder
doe
not
have
enough
fund
in
his
account
System
cancel
the
bid
asks
the
user
to
get
more
fund
Precondition
The
last
date
for
bidding
ha
been
reached
Select
highest
bidder
send
email
to
selected
bidder
and
seller
informing
final
bid
price
send
email
to
other
bidder
also
Transfer
from
seller
’
s
acct
commission
amt
to
organization
’
s
acct
this
case
ha
to
then
be
specified
e.g.
all
the
record
are
cleaned
A
use
case
can
employ
other
use
case
to
perform
some
of
it
work
For
example
in
UC
block
the
necessary
fund
or
debit
bidder
’
s
account
and
credit
seller
’
s
are
action
that
need
to
be
performed
for
the
use
case
to
succeed
However
they
are
not
performed
in
this
use
case
but
are
treated
a
use
case
themselves
whose
behavior
ha
to
be
described
elsewhere
If
these
use
case
are
also
part
of
the
system
being
built
then
there
must
be
description
of
these
in
the
requirement
document
If
they
belong
to
some
other
system
then
proper
specification
about
them
will
have
to
be
obtained
The
financial
action
may
easily
be
outside
the
scope
of
the
auction
system
so
will
not
be
described
in
the
SRS
However
action
like
search
and
browse
are
most
likely
part
of
this
system
and
will
have
to
be
described
in
the
SRS
This
allows
use
case
to
be
hierarchically
organized
and
refinement
approach
can
be
used
to
define
a
higher-level
use
case
in
term
of
lower
service
and
then
defining
the
lower
service
However
these
lower-level
use
case
are
proper
use
case
with
a
primary
actor
main
scenario
etc
The
primary
actor
will
often
be
the
primary
actor
of
the
higher-level
use
case
For
example
the
primary
actor
for
the
use
case
find
an
item
is
the
buyer
It
also
implies
that
while
listing
the
scenario
new
use
case
and
new
actor
might
emerge
In
the
requirement
document
all
the
use
case
that
are
mentioned
in
this
one
will
need
to
be
specified
if
they
are
a
part
of
the
system
being
built
Besides
specifying
the
primary
actor
it
goal
and
the
success
and
exceptional
scenario
a
use
case
can
also
specify
a
scope
If
the
system
being
built
ha
many
subsystem
a
is
often
the
case
sometimes
system
use
case
may
actually
be
capturing
the
behavior
of
some
subsystem
In
such
a
situation
it
is
better
to
specify
the
scope
of
that
use
case
a
the
subsystem
For
example
a
use
case
for
a
system
may
be
log
in
Even
though
this
is
a
part
of
the
system
the
interaction
of
the
user
with
the
system
described
in
this
use
case
is
limited
to
the
interaction
with
the
login
and
authentication
subsystem
If
login
and
authentication
ha
been
identified
a
a
subsystem
or
a
component
then
it
is
better
to
specify
it
a
the
scope
Generally
a
business
use
case
ha
the
enterprise
or
the
organization
a
the
scope
a
system
use
case
ha
the
system
being
built
a
the
scope
and
a
component
use
case
is
where
the
scope
is
a
subsystem
UCs
where
the
scope
is
the
enterprise
can
often
run
over
a
long
period
of
time
e.g.
process
an
application
of
a
prospective
candidate
These
use
case
may
require
many
different
system
to
perform
different
task
before
the
UC
can
be
completed
Example
for
processing
an
application
the
HR
department
ha
to
do
some
thing
the
travel
department
ha
to
arrange
the
travel
and
lodging
and
the
technical
department
ha
to
conduct
the
interview
The
system
and
subsystem
use
case
are
generally
of
the
type
that
can
be
completed
in
one
relatively
short
sitting
All
the
three
use
case
above
are
system
use
case
As
mentioned
before
we
will
focus
on
describing
the
behavior
of
the
software
system
we
are
interested
in
building
However
the
enterprise-level
UCs
provide
the
context
in
which
the
system
operate
Hence
sometimes
it
may
be
useful
to
describe
some
of
the
key
business
process
a
summary-level
use
case
to
provide
the
context
for
the
system
being
designed
and
built
For
example
let
u
describe
the
overall
use
case
of
performing
an
auction
A
possible
use
case
is
given
in
Figure
This
use
case
is
not
a
one-sitting
use
case
and
is
really
a
business
process
which
provides
the
context
for
the
earlier
use
case
Though
this
use
case
is
also
largely
done
by
the
system
and
is
probably
part
of
the
system
being
built
frequently
such
use
case
may
not
be
completely
part
of
the
software
system
and
may
involve
manual
step
a
well
For
example
in
the
auction
an
item
use
case
if
the
delivery
of
the
item
being
auctioned
wa
to
be
ensured
by
the
auctioning
site
then
that
will
be
a
step
in
this
use
case
and
it
will
be
a
manual
step
Use
case
may
also
specify
post
condition
for
the
main
success
scenario
or
some
minimal
guarantee
they
provide
in
all
condition
For
example
in
some
use
case
atomicity
may
be
a
minimal
guarantee
That
is
no
matter
what
exception
occur
either
the
entire
transaction
will
be
completed
and
the
goal
achieved
or
the
system
state
will
be
a
if
nothing
wa
done
With
atomicity
there
will
be
no
partial
result
and
any
partial
change
will
be
rolled
back
UCs
not
only
document
requirement
a
their
form
is
like
storytelling
and
us
text
both
of
which
are
easy
and
natural
with
different
stakeholder
they
also
are
a
good
medium
for
discussion
and
brainstorming
Hence
UCs
can
also
be
used
for
requirement
elicitation
and
problem
analysis
While
developing
use
case
informal
or
formal
model
may
also
be
built
though
they
are
not
required
UCs
can
be
evolved
in
a
stepwise
refinement
manner
with
each
step
adding
more
detail
This
approach
allows
UCs
to
be
presented
at
different
level
of
ab-
straction
Though
any
number
of
level
of
abstraction
are
possible
four
natural
level
emerge
Actors
and
goal
The
actor-goal
list
enumerates
the
use
case
and
specifies
the
actor
for
each
goal
The
name
of
the
use
case
is
generally
the
goal
This
table
may
be
extended
by
giving
a
brief
description
of
each
of
the
use
case
At
this
level
the
use
case
together
specify
the
scope
of
the
system
and
give
an
overall
view
of
what
it
doe
Completeness
of
functionality
can
be
assessed
fairly
well
by
reviewing
these
Main
success
scenario
For
each
of
the
use
case
the
main
success
scenar-
io
are
provided
at
this
level
With
the
main
scenario
the
system
behavior
for
each
use
case
is
specified
This
description
can
be
reviewed
to
ensure
that
interest
of
all
the
stakeholder
are
met
and
that
the
use
case
is
delivering
the
desired
behavior
Failure
condition
Once
the
success
scenario
is
listed
all
the
possible
failure
condition
can
be
identified
At
this
level
for
each
step
in
the
main
success
scenario
the
different
way
in
which
a
step
can
fail
form
the
failure
condition
Before
deciding
what
should
be
done
in
these
failure
condition
which
is
done
at
the
next
level
it
is
better
to
enumerate
the
failure
condi-
tions
and
review
for
completeness
Failure
handling
This
is
perhaps
the
most
tricky
and
difficult
part
of
writing
a
use
case
Often
the
focus
is
so
much
on
the
main
functionality
that
people
do
not
pay
attention
to
how
failure
should
be
handled
Determining
what
should
be
the
behavior
under
different
failure
condition
will
often
identify
new
business
rule
or
new
actor
The
different
level
can
be
used
for
different
purpose
For
discussion
on
overall
functionality
or
capability
of
the
system
actor
and
goal-level
descrip-
tion
is
very
useful
Failure
condition
on
the
other
hand
are
very
useful
for
understanding
and
extracting
detailed
requirement
and
business
rule
under
special
case
These
four
level
can
also
guide
the
analysis
activity
A
step-by-step
ap-
proach
for
analysis
when
employing
use
case
is
Step
1
Identify
the
actor
and
their
goal
and
get
an
agreement
with
the
concerned
stakeholder
a
to
the
goal
The
actor-goal
list
will
clearly
define
the
scope
of
the
system
and
will
provide
an
overall
view
of
what
the
system
capability
are
Step
2
Understand
and
specify
the
main
success
scenario
for
each
UC
giving
more
detail
about
the
main
function
of
the
system
Interaction
and
dis-
cussion
are
the
primary
mean
to
uncover
these
scenario
though
model
may
be
built
if
required
During
this
step
the
analyst
may
uncover
that
to
complete
some
use
case
some
other
use
case
are
needed
which
have
not
been
identified
In
this
case
the
list
of
use
case
will
be
expanded
Step
3
When
the
main
success
scenario
for
a
use
case
is
agreed
upon
and
the
main
step
in
it
execution
are
specified
then
the
failure
condition
can
be
examined
Enumerating
failure
condition
is
an
excellent
method
of
uncovering
special
situation
that
can
occur
and
which
must
be
handled
by
the
system
Step
4
Finally
specify
what
should
be
done
for
these
failure
condition
As
detail
of
handling
failure
scenario
can
require
a
lot
of
effort
and
discussion
it
is
better
to
first
enumerate
the
different
failure
condition
and
then
get
the
detail
of
these
scenario
Very
often
when
deciding
the
failure
scenario
many
new
business
rule
of
how
to
deal
with
these
scenario
are
uncovered
Though
we
have
explained
the
basic
step
in
developing
use
case
at
any
step
an
analyst
may
have
to
go
back
to
earlier
step
a
during
some
detailed
analysis
new
actor
may
emerge
or
new
goal
and
new
use
case
may
be
un-
covered
That
is
using
use
case
for
analysis
is
also
an
interactive
task
What
should
be
the
level
of
detail
in
a
use
case
There
is
no
one
answer
to
a
question
like
this
the
actual
answer
always
depends
on
the
project
and
the
situation
So
it
is
with
use
case
Generally
it
is
good
to
have
sufficient
detail
which
are
not
overwhelming
but
are
sufficient
to
build
the
system
and
meet
it
quality
goal
For
example
if
there
is
a
small
collocated
team
building
the
system
it
is
quite
likely
that
use
case
which
list
the
main
exception
condition
and
give
a
few
key
step
for
the
scenario
will
suffice
On
the
other
hand
for
a
project
whose
development
is
to
be
subcontracted
to
some
other
organization
it
is
better
to
have
more
detailed
use
case
For
writing
use
case
general
technical
writing
rule
apply
Use
simple
gram-
mar
clearly
specify
who
is
performing
the
step
and
keep
the
overall
scenario
a
simple
a
possible
Also
when
writing
step
for
simplicity
it
is
better
to
combine
some
step
into
one
logical
step
if
it
make
sense
For
example
step
user
enters
his
name
user
enters
his
SSN
and
user
enters
his
address
can
be
easily
combined
into
one
step
user
enters
personal
information
The
basic
aim
of
problem
analysis
is
to
obtain
a
clear
understanding
of
the
need
of
the
client
and
the
user
what
exactly
is
desired
from
the
software
and
what
the
constraint
on
the
solution
are
Frequently
the
client
and
the
user
do
not
understand
or
know
all
their
need
because
the
potential
of
the
new
system
is
often
not
fully
appreciated
The
analyst
have
to
ensure
that
the
real
need
of
the
client
and
the
user
are
uncovered
even
if
they
don
’
t
know
them
clearly
That
is
the
analyst
are
not
just
collecting
and
organizing
information
about
the
client
’
s
organization
and
it
process
but
they
also
act
a
consultant
who
play
an
active
role
of
helping
the
client
and
user
identify
their
need
The
basic
principle
used
in
analysis
is
the
same
a
in
any
complex
task
divide
and
conquer
That
is
partition
the
problem
into
subproblems
and
then
try
to
understand
each
subproblem
and
it
relationship
to
other
subproblems
in
an
effort
to
understand
the
total
problem
The
concept
of
state
and
projection
can
sometimes
also
be
used
effectively
in
the
partitioning
process
A
state
of
a
system
represents
some
condition
about
the
system
Frequently
when
using
state
a
system
is
first
viewed
a
operating
in
one
of
the
several
possible
state
and
then
a
detailed
analysis
is
performed
for
each
state
This
approach
is
sometimes
used
in
real-time
software
or
process-control
software
In
projection
a
system
is
defined
from
multiple
point
of
view
86
While
using
projection
different
viewpoint
of
the
system
are
defined
and
the
system
is
then
analyzed
from
these
different
perspective
The
different
projection
obtained
are
combined
to
form
the
analysis
for
the
complete
system
Analyzing
the
system
from
the
different
perspective
is
often
easier
a
it
limit
and
focus
the
scope
of
the
study
In
the
remainder
of
this
section
we
will
discus
two
other
method
for
prob-
lem
analysis
As
the
goal
of
analysis
is
to
understand
the
problem
domain
an
analyst
must
be
familiar
with
different
method
of
analysis
and
pick
the
approach
that
he
feel
is
best
suited
to
the
problem
at
hand
Data
flow
diagram
also
called
data
flow
graph
are
commonly
used
during
problem
analysis
Data
flow
diagram
DFDs
are
quite
general
and
are
not
limited
to
problem
analysis
for
software
requirement
specification
They
were
in
use
long
before
the
software
engineering
discipline
began
DFDs
are
very
useful
in
understanding
a
system
and
can
be
effectively
used
during
analysis
A
DFD
show
the
flow
of
data
through
a
system
It
view
a
system
a
a
function
that
transforms
the
input
into
desired
output
Any
complex
system
will
not
perform
this
transformation
in
a
single
step
and
data
will
typically
undergo
a
series
of
transformation
before
it
becomes
the
output
The
DFD
aim
to
capture
the
transformation
that
take
place
within
a
system
to
the
input
data
so
that
eventually
the
output
data
is
produced
The
agent
that
performs
the
transformation
of
data
from
one
state
to
another
is
called
a
process
or
a
bubble
Thus
a
DFD
show
the
movement
of
data
through
the
different
transformation
or
process
in
the
system
The
process
are
shown
by
named
circle
and
data
flow
are
represented
by
named
arrow
entering
or
leaving
the
bubble
A
rectangle
represents
a
source
or
sink
and
is
a
net
originator
or
consumer
of
data
A
source
or
a
sink
is
typically
outside
the
main
system
of
study
An
example
of
a
DFD
for
a
system
that
pay
worker
is
shown
in
Figure
In
this
DFD
there
is
one
basic
input
data
flow
the
weekly
timesheet
which
originates
from
the
source
worker
The
basic
output
is
the
paycheck
the
sink
for
which
is
also
the
worker
In
this
system
first
the
employee
’
s
record
is
re-
trieved
using
the
employee
ID
which
is
contained
in
the
timesheet
From
the
employee
record
the
rate
of
payment
and
overtime
are
obtained
These
rate
and
the
regular
and
overtime
hour
from
the
timesheet
are
used
to
compute
the
pay
After
the
total
pay
is
determined
tax
are
deducted
To
compute
the
tax
deduction
information
from
the
tax-rate
file
is
used
The
amount
of
tax
deducted
is
recorded
in
the
employee
and
company
record
Finally
the
pay-
check
is
issued
for
the
net
pay
The
amount
paid
is
also
recorded
in
company
record
Some
convention
used
in
drawing
this
DFD
should
be
explained
All
ex-
ternal
file
such
a
employee
record
company
record
and
tax
rate
are
shown
a
a
labeled
straight
line
The
need
for
multiple
data
flow
by
a
process
is
rep-
resented
by
a
between
the
data
flow
This
symbol
represents
the
AND
relationship
For
example
if
there
is
a
between
the
two
input
data
flow
A
and
B
for
a
process
it
mean
that
A
AND
B
are
needed
for
the
process
In
the
DFD
for
the
process
weekly
pay
the
data
flow
hour
and
pay
rate
both
are
needed
a
shown
in
the
DFD
Similarly
the
OR
relationship
is
represented
by
a
+
between
the
data
flow
This
DFD
is
an
abstract
description
of
the
system
for
handling
payment
It
doe
not
matter
if
the
system
is
automated
or
manual
This
diagram
could
very
well
be
for
a
manual
system
where
the
computation
are
all
done
with
calculator
and
the
record
are
physical
folder
and
ledger
The
detail
and
minor
data
path
are
not
represented
in
this
DFD
For
example
what
happens
if
there
are
error
in
the
weekly
timesheet
is
not
shown
in
this
DFD
This
is
done
to
avoid
getting
bogged
down
with
detail
while
constructing
a
DFD
for
the
overall
system
If
more
detail
are
desired
the
DFD
can
be
further
refined
It
should
be
pointed
out
that
a
DFD
is
not
a
flowchart
A
DFD
represents
the
flow
of
data
while
a
flowchart
show
the
flow
of
control
A
DFD
doe
not
represent
procedural
information
So
while
drawing
a
DFD
one
must
not
get
involved
in
procedural
detail
and
procedural
thinking
must
be
consciously
avoided
For
example
consideration
of
loop
and
decision
must
be
ignored
In
drawing
the
DFD
the
designer
ha
to
specify
the
major
transforms
in
the
path
of
the
data
flowing
from
the
input
to
output
How
those
transforms
are
performed
is
not
an
issue
while
drawing
the
data
flow
graph
Many
system
are
too
large
for
a
single
DFD
to
describe
the
data
processing
clearly
It
is
necessary
that
some
decomposition
and
abstraction
mechanism
be
used
for
such
system
DFDs
can
be
hierarchically
organized
which
help
in
progressively
partitioning
and
analyzing
large
system
Such
DFDs
together
are
called
a
leveled
DFD
set
28
A
leveled
DFD
set
ha
a
starting
DFD
which
is
a
very
abstract
represen-
tation
of
the
system
identifying
the
major
input
and
output
and
the
major
process
in
the
system
Often
before
the
initial
DFD
a
context
diagram
may
be
drawn
in
which
the
entire
system
is
shown
a
a
single
process
with
all
it
input
output
sink
and
source
Then
each
process
is
refined
and
a
DFD
is
drawn
for
the
process
In
other
word
a
bubble
in
a
DFD
is
expanded
into
a
DFD
during
refinement
For
the
hierarchy
to
be
consistent
it
is
important
that
the
net
input
and
output
of
a
DFD
for
a
process
are
the
same
a
the
input
and
output
of
the
process
in
the
higher-level
DFD
This
refinement
stop
if
each
bubble
is
considered
to
be
atomic
in
that
each
bubble
can
be
easily
specified
or
understood
It
should
be
pointed
out
that
during
refine-
ment
though
the
net
input
and
output
are
preserved
a
refinement
of
the
data
might
also
occur
That
is
a
unit
of
data
may
be
broken
into
it
component
for
processing
when
the
detailed
DFD
for
a
process
is
being
drawn
So
a
the
process
are
decomposed
data
decomposition
also
occurs
In
a
DFD
data
flow
are
identified
by
unique
name
These
name
are
chosen
so
that
they
convey
some
meaning
about
what
the
data
is
However
for
specifying
the
precise
structure
of
data
flow
a
data
dictionary
is
often
used
The
associated
data
dictionary
state
precisely
the
structure
of
each
data
flow
in
the
DFD
To
define
the
data
structure
a
regular
expression
type
notation
is
used
While
specifying
the
structure
of
a
data
item
sequence
or
composition
is
represented
by
+
selection
by
vertical
bar
mean
one
OR
the
other
and
repetition
by
Entity
relationship
diagram
ERDs
have
been
used
for
year
for
modeling
the
data
aspect
of
a
system
An
ERD
can
be
used
to
model
the
data
in
the
system
and
how
the
data
item
relate
to
each
other
but
doe
not
cover
how
the
data
is
to
be
processed
or
how
the
data
is
actually
manipulated
and
changed
in
the
system
It
is
used
often
by
database
designer
to
represent
the
structure
of
the
database
and
is
a
useful
tool
for
analyzing
software
system
which
employ
database
ER
model
form
the
logical
database
design
and
can
be
easily
converted
into
initial
table
structure
for
a
relational
database
ER
diagram
have
two
main
concept
and
notation
to
representing
them
These
are
entity
and
relationship
Entities
are
the
main
information
holder
or
concept
in
a
system
Entities
can
be
viewed
a
type
which
describe
all
element
of
some
type
which
have
common
property
Entities
are
represented
a
box
in
an
ER
diagram
with
a
box
representing
all
instance
of
the
concept
or
type
the
entity
is
representing
An
entity
is
essentially
equivalent
to
a
table
in
a
database
or
a
sheet
in
a
spreadsheet
with
each
row
representing
an
instance
of
this
entity
Entities
may
have
attribute
which
are
property
of
the
concept
being
represented
Attributes
can
be
viewed
a
the
column
of
the
database
table
and
are
represented
a
ellipsis
attached
to
the
entity
To
avoid
cluttering
attribute
are
sometimes
not
shown
in
an
ER
diagram
If
all
identity
are
identified
and
represented
we
will
have
a
set
of
labeled
box
in
the
diagram
each
box
representing
some
entity
Entities
of
course
do
not
exist
in
isolation
They
have
relationship
between
them
and
that
is
the
reason
why
they
exist
together
in
the
same
system
Relationships
between
two
entity
are
represented
by
a
line
connecting
the
box
representing
the
entity
Having
a
line
between
two
box
mean
that
each
element
of
one
entity
is
related
to
element
of
the
other
entity
and
vice
versa
This
relationship
can
also
be
given
a
name
by
labeling
the
line
In
some
notation
the
name
of
the
relationship
is
mentioned
inside
a
diamond
Some
example
of
relationship
are
studies-in
between
student
and
college
works-
for
between
employee
and
company
and
owner
between
people
and
car
Note
that
the
relationship
need
not
be
between
two
distinct
entity
There
can
be
a
relationship
between
element
of
the
same
entity
For
example
for
an
entity
type
Employee
there
can
be
a
relationship
Supervisor
which
is
between
element
of
the
entity
Employee
An
ER
diagram
specifies
some
property
of
the
relationship
also
In
par-
ticular
it
can
specify
if
the
relationship
is
optional
or
necessary
and
with
how
many
element
an
element
of
an
entity
is
related
to
This
lead
to
many
form
of
relationship
The
common
one
are
one-to-one
that
one
element
of
an
entity
is
related
to
exactly
one
element
of
the
other
entity
one-to-many
or
many-to-one
that
one
element
is
related
to
many
element
of
the
other
entity
and
many-to-many
that
one
element
of
entity
A
is
related
to
many
element
of
entity
B
and
one
element
of
entity
B
is
related
to
many
element
of
entity
A
There
are
various
notation
to
express
the
nature
of
relationship
a
common
one
is
to
put
0
1
or
M
on
the
two
side
of
the
relationship
line
to
rep-
resent
the
cardinality
of
the
relationship
Thus
for
a
one-to-many
relationship
a
1
will
be
put
on
one
end
and
N
will
be
put
on
the
other
end
of
the
line
Relationships
reflect
some
property
of
the
problem
domain
For
example
a
course
ha
many
student
and
a
student
is
taking
many
course
leading
to
many-to-many
relationship
between
course
and
student
But
a
student
study
in
exactly
one
college
though
the
college
ha
many
student
leading
to
many-
to-one
relationship
between
student
and
college
A
department
in
a
college
ha
exactly
one
head
and
one
person
can
be
Head
of
only
one
department
leading
to
one-to-one
relationship
Let
u
draw
the
ER
diagram
for
the
university
auction
system
some
use
case
of
which
were
discussed
earlier
From
the
use
case
described
we
can
easily
identify
some
entities—users
category
item
and
bid
The
relationship
between
them
are
also
clear
A
user
can
sell
many
item
but
each
item
ha
only
one
seller
so
there
is
a
one-to-many
relationship
Sell
between
the
user
and
item
Similarly
there
is
a
one-to-many
relationship
between
item
and
bid
between
user
and
bid
and
between
category
and
item
The
ER
diagram
of
this
is
shown
in
Figure
From
the
ER
diagram
it
is
easy
to
determine
the
initial
logical
structure
of
the
table
Each
entity
represents
a
table
and
relationship
determine
what
field
a
table
must
have
to
support
the
relationship
in
addition
to
having
field
for
each
of
the
attribute
For
example
from
the
ER
diagram
for
the
auction
system
we
can
say
that
there
will
be
four
table
for
user
category
item
and
bid
As
user
is
related
to
item
by
one-to-many
the
item
table
should
have
a
user-ID
field
to
uniquely
identify
the
user
who
is
selling
the
item
Similarly
the
bid
table
must
have
a
user-ID
to
identify
the
user
who
placed
the
bid
and
an
item-ID
to
identify
the
item
for
which
the
bid
ha
been
made
As
we
can
see
an
ER
diagram
is
complementary
to
method
like
use
case
Whereas
use
case
focus
on
the
nature
of
interaction
and
functionality
ER
diagram
focus
on
the
structure
of
the
entity
being
used
in
the
use
case
Due
to
their
complementary
nature
both
use
case
and
ER
diagram
can
be
used
while
analyzing
the
requirement
of
a
system
and
both
may
be
contained
in
an
SRS
The
development
of
software
start
with
a
requirement
document
which
is
also
used
to
determine
eventually
whether
or
not
the
delivered
software
system
is
acceptable
It
is
therefore
important
that
the
requirement
specification
con-
tains
no
error
and
specifies
the
client
’
s
requirement
correctly
Furthermore
a
the
longer
an
error
remains
undetected
the
greater
the
cost
of
correcting
it
it
is
extremely
desirable
to
detect
error
in
the
requirement
before
the
design
and
development
of
the
software
begin
Due
to
the
nature
of
the
requirement
specification
phase
there
is
a
lot
of
room
for
misunderstanding
and
committing
error
and
it
is
quite
possible
that
the
requirement
specification
doe
not
accurately
represent
the
client
’
s
need
The
basic
objective
of
the
requirement
validation
activity
is
to
ensure
that
the
SRS
reflects
the
actual
requirement
accurately
and
clearly
A
related
objective
is
to
check
that
the
SRS
document
is
itself
of
good
quality
Before
we
discus
validation
let
u
consider
the
type
of
error
that
typically
occur
in
an
SRS
Many
different
type
of
error
are
possible
but
the
most
com-
mon
error
that
occur
can
be
classified
in
four
type
omission
inconsistency
incorrect
fact
and
ambiguity
Omission
is
a
common
error
in
requirement
In
this
type
of
error
some
user
requirement
is
simply
not
included
in
the
SRS
the
omitted
requirement
may
be
related
to
the
behavior
of
the
system
it
perfor-
mance
constraint
or
any
other
factor
Omission
directly
affect
the
external
completeness
of
the
SRS
Another
common
form
of
error
in
requirement
is
inconsistency
Inconsistency
can
be
due
to
contradiction
within
the
require-
ments
themselves
or
to
incompatibility
of
the
stated
requirement
with
the
actual
requirement
of
the
client
or
with
the
environment
in
which
the
system
will
operate
The
third
common
requirement
error
is
incorrect
fact
Errors
of
this
type
occur
when
some
fact
recorded
in
the
SRS
is
not
correct
The
fourth
common
error
type
is
ambiguity
Errors
of
this
type
occur
when
there
are
some
requirement
that
have
multiple
meaning
that
is
their
interpretation
is
not
unique
Some
project
have
collected
data
about
requirement
error
In
27
the
effectiveness
of
different
method
and
tool
in
detecting
requirement
error
in
specification
for
a
data
processing
application
is
reported
On
average
a
total
of
more
than
250
error
were
detected
and
the
percentage
of
different
type
of
error
wa
In
5
the
error
detected
in
the
requirement
specification
of
the
A-
which
deal
with
a
real-time
flight
control
software
were
reported
A
total
of
about
80
error
were
detected
out
of
which
about
23
%
were
clerical
in
nature
Of
the
remaining
the
distribution
with
error
type
wa
Though
the
distribution
of
error
is
different
in
these
two
case
reflecting
the
difference
in
application
domain
and
the
error
detection
method
used
they
do
suggest
that
the
major
problem
besides
clerical
error
are
omission
incorrect
fact
inconsistency
and
ambiguity
If
we
take
the
average
of
the
two
data
table
it
show
that
all
four
class
of
error
are
very
significant
and
a
good
fraction
of
error
belong
to
each
of
these
type
This
implies
that
besides
improving
the
quality
of
the
SRS
itself
e.g.
no
clerical
error
the
validation
should
focus
on
uncovering
these
type
of
error
As
requirement
are
generally
textual
document
that
can
not
be
executed
inspection
and
review
are
eminently
suitable
for
requirement
validation
Consequently
inspection
of
the
SRS
frequently
called
requirement
review
are
the
most
common
method
of
validation
Because
requirement
specifica-
tion
formally
specifies
something
that
originally
existed
informally
in
people
’
s
mind
requirement
validation
must
involve
the
client
and
the
user
Due
to
this
the
requirement
review
team
generally
consists
of
client
a
well
a
user
representative
Requirements
review
is
a
review
by
a
group
of
people
to
find
error
and
point
out
other
matter
of
concern
in
the
requirement
specification
of
a
system
The
review
group
should
include
the
author
of
the
requirement
document
someone
who
understands
the
need
of
the
client
a
person
of
the
design
team
and
the
person
s
responsible
for
maintaining
the
requirement
document
It
is
also
good
practice
to
include
some
people
not
directly
involved
with
product
development
like
a
software
quality
engineer
Although
the
primary
goal
of
the
review
process
is
to
reveal
any
error
in
the
requirement
such
a
those
discussed
earlier
the
review
process
is
also
used
to
consider
factor
affecting
quality
such
a
testability
and
readability
During
the
review
one
of
the
job
of
the
reviewer
is
to
uncover
the
requirement
that
are
too
subjective
and
too
difficult
to
define
criterion
for
testing
that
requirement
During
the
review
the
review
team
must
go
through
each
requirement
and
if
any
error
are
there
then
they
discus
and
agree
on
the
nature
of
the
error
A
detailed
inspection
process
may
be
used
we
will
discus
one
such
process
later
in
Chapter
Requirements
review
are
probably
the
most
effective
mean
for
detecting
requirement
error
The
data
in
5
about
the
A-33
%
of
the
total
requirement
error
detected
were
detected
by
review
process
and
about
45
%
of
the
requirement
error
were
detected
during
the
design
phase
when
the
requirement
document
is
used
a
a
reference
for
design
This
clearly
suggests
that
if
requirement
are
reviewed
then
not
only
a
substantial
fraction
of
the
error
are
detected
by
them
but
a
vast
majority
of
the
remaining
error
are
detected
soon
afterward
in
the
design
activity
Though
requirement
review
remain
the
most
commonly
used
and
viable
mean
for
requirement
validation
other
possibility
arise
if
some
special-
purpose
tool
for
modeling
and
analysis
are
used
For
example
if
the
require-
ments
are
written
in
a
formal
specification
language
or
a
language
specifically
designed
for
machine
processing
then
it
is
possible
to
have
tool
to
verify
some
property
of
requirement
These
tool
will
focus
on
check
for
internal
con-
sistency
and
completeness
which
sometimes
lead
to
checking
of
external
com-
pleteness
However
these
tool
can
not
directly
check
for
external
completeness
after
all
how
will
a
tool
know
that
some
requirement
ha
been
completely
omitted
For
this
reason
requirement
review
are
needed
even
if
the
re-
quirements
are
specified
through
a
tool
or
are
in
a
formal
notation
The
main
goal
of
the
requirement
process
is
to
produce
the
software
require-
ments
specification
SRS
which
accurately
capture
the
client
’
s
requirement
and
which
form
the
basis
of
software
development
and
validation
There
are
three
basic
activity
in
the
requirement
process—problem
anal-
ysis
specification
and
validation
The
goal
of
analysis
is
to
understand
the
different
aspect
of
the
problem
it
context
and
how
it
fit
within
the
client
’
s
organization
In
requirement
specification
the
understood
problem
is
speci-
fied
or
written
producing
the
SRS
Requirements
validation
is
done
to
ensure
that
the
requirement
specified
in
the
SRS
are
indeed
what
are
desired
The
key
desirable
characteristic
of
an
SRS
are
correctness
completeness
consistency
unambiguousness
verifiability
and
ranked
for
importance
A
good
SRS
should
specify
all
the
function
the
software
need
to
support
performance
requirement
of
the
system
the
design
constraint
that
exist
and
all
the
external
interface
Use
case
are
a
popular
approach
for
specifying
functional
requirement
Each
use
case
specifies
the
interaction
of
the
system
with
the
primary
actor
who
initiate
the
use
case
for
achieving
some
goal
A
use
case
ha
a
precondition
a
normal
scenario
a
well
a
many
excep-
tional
scenario
thereby
providing
the
complete
behavior
of
the
system
For
developing
use
case
first
the
actor
and
goal
should
be
identified
then
the
main
success
scenario
then
the
failure
condition
and
finally
the
failure
handling
With
data
flow
diagram
a
system
is
analyzed
from
the
point
of
view
of
how
data
flow
through
the
system
A
DFD
consists
of
process
and
data
flow
through
the
process
Omission
incorrect
fact
inconsistency
and
ambiguity
are
the
most
common
error
in
an
SRS
For
validation
the
most
commonly
used
method
is
doing
a
structured
group
review
of
the
requirement
The
basic
goal
of
the
requirement
activity
is
to
get
an
SRS
that
ha
some
desirable
property
What
is
the
role
of
modeling
in
developing
such
an
SRS
What
are
the
main
component
of
an
SRS
And
what
are
the
main
criterion
for
evaluating
the
quality
of
an
SRS
Take
an
on-line
social
networking
site
of
your
choice
List
the
major
use
case
for
this
system
along
with
the
goal
precondition
and
exception
scenario
Do
the
same
exercise
for
a
conference
management
site
which
allows
author
to
submit
paper
program
chair
to
assign
reviewer
and
do
the
final
paper
selection
based
on
review
and
reviewer
to
enter
the
review
Who
do
you
think
should
be
included
in
the
requirement
review
team
Planning
is
the
most
important
project
management
activity
It
ha
two
ba-
sic
objectives—establish
reasonable
cost
schedule
and
quality
goal
for
the
project
and
to
draw
out
a
plan
to
deliver
the
project
goal
A
project
succeeds
if
it
meet
it
cost
schedule
and
quality
goal
Without
the
project
goal
be-
ing
defined
it
is
not
possible
to
even
declare
if
a
project
ha
succeeded
And
without
detailed
planning
no
real
monitoring
or
controlling
of
the
project
is
possible
Often
project
are
rushed
toward
implementation
with
not
enough
effort
spent
on
planning
No
amount
of
technical
effort
later
can
compensate
for
lack
of
careful
planning
Lack
of
proper
planning
is
a
sure
ticket
to
fail-
ure
for
a
large
software
project
For
this
reason
we
treat
project
planning
a
an
independent
chapter
Note
that
we
also
cover
the
monitoring
phase
of
the
project
management
process
a
part
of
planning
a
how
the
project
is
to
be
monitored
is
also
a
part
of
the
planning
phase
The
input
to
the
planning
activity
are
the
requirement
specification
and
maybe
the
architecture
description
A
very
detailed
requirement
document
is
not
essential
for
planning
but
for
a
good
plan
all
the
important
requirement
must
be
known
and
it
is
highly
desirable
that
key
architecture
decision
have
been
taken
There
are
generally
two
main
output
of
the
planning
activity
the
overall
project
management
plan
document
that
establishes
the
project
goal
on
the
cost
schedule
and
quality
front
and
defines
the
plan
for
managing
risk
monitoring
the
project
etc
and
the
detailed
plan
often
referred
to
a
the
detailed
project
schedule
specifying
the
task
that
need
to
be
performed
to
meet
the
goal
the
resource
who
will
perform
them
and
their
schedule
The
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
overall
plan
guide
the
development
of
the
detailed
plan
which
then
becomes
the
main
guiding
document
during
project
execution
for
project
monitoring
How
to
estimate
effort
and
schedule
for
the
project
to
establish
project
goal
and
milestone
and
determine
the
team
size
needed
for
executing
the
project
How
to
establish
quality
goal
for
the
project
and
prepare
a
quality
plan
How
to
identify
high-priority
risk
that
can
threaten
the
success
of
the
project
and
plan
for
their
mitigation
How
to
plan
for
monitoring
a
project
using
measurement
to
check
if
a
project
is
progressing
a
per
the
plan
How
to
develop
a
detailed
task
schedule
from
the
overall
estimate
and
other
planning
task
done
such
that
if
followed
the
overall
goal
of
the
project
will
be
met
For
a
software
development
project
overall
effort
and
schedule
estimate
are
essential
prerequisite
for
planning
the
project
These
estimate
are
needed
before
development
is
initiated
a
they
establish
the
cost
and
schedule
goal
of
the
project
Without
these
even
simple
question
like
is
the
project
late
are
there
cost
overrun
and
when
is
the
project
likely
to
complete
can
not
be
answered
A
more
practical
use
of
these
estimate
is
in
bidding
for
software
project
where
cost
and
schedule
estimate
must
be
given
to
a
potential
client
for
the
development
contract
As
the
bulk
of
the
cost
of
software
development
is
due
to
the
human
effort
cost
can
easily
be
determined
from
effort
by
using
a
suitable
person-month
cost
value
Effort
and
schedule
estimate
are
also
required
for
determining
the
staffing
level
for
a
project
during
different
phase
for
the
detailed
plan
and
for
project
monitoring
The
accuracy
with
which
effort
can
be
estimated
clearly
depends
on
the
level
of
information
available
about
the
project
The
more
detailed
the
infor-
mation
the
more
accurate
the
estimation
can
be
Of
course
even
with
all
the
information
available
the
accuracy
of
the
estimate
will
depend
on
the
effec-
tiveness
and
accuracy
of
the
estimation
procedure
or
model
employed
and
the
process
If
from
the
requirement
specification
the
estimation
approach
can
produce
estimate
that
are
within
20
%
of
the
actual
effort
about
two-thirds
of
the
time
then
the
approach
can
be
considered
good
Here
we
discus
two
commonly
used
approach
Although
the
effort
for
a
project
is
a
function
of
many
parameter
it
is
generally
agreed
that
the
primary
factor
that
control
the
effort
is
the
size
of
the
project
That
is
the
larger
the
project
the
greater
is
the
effort
requirement
The
top-
down
approach
utilizes
this
and
considers
effort
a
a
function
of
project
size
Note
that
to
use
this
approach
we
need
to
first
determine
the
nature
of
the
function
and
then
to
apply
the
function
we
need
to
estimate
the
size
of
the
project
for
which
effort
is
to
be
estimated
If
past
productivity
on
similar
project
is
known
then
it
can
be
used
a
the
estimation
function
to
determine
effort
from
the
size
If
productivity
is
P
KLOC/PM
then
the
effort
estimate
for
the
project
will
be
SIZE/P
person-
month
Note
that
a
productivity
itself
depends
on
the
size
of
the
project
larger
project
often
have
lower
productivity
this
approach
can
work
only
if
the
size
and
type
of
the
project
are
similar
to
the
set
of
project
from
which
the
productivity
P
wa
obtained
and
that
in
the
new
project
a
similar
productivity
can
be
obtained
by
following
a
process
similar
to
what
wa
used
in
earlier
project
A
more
general
function
for
determining
effort
from
size
that
is
commonly
used
is
of
the
form
where
a
and
b
are
constant
2
and
project
size
is
generally
in
KLOC
size
could
also
be
in
another
size
measure
called
function
point
which
can
be
de-
termined
from
requirement
Values
for
these
constant
for
an
organization
are
determined
through
regression
analysis
which
is
applied
to
data
about
the
project
that
have
been
performed
in
the
past
For
example
Watson
and
Felix
81
analyzed
the
data
of
more
than
60
project
done
at
IBM
Federal
Systems
Division
ranging
from
4000
to
467,000
line
of
delivered
source
code
and
found
that
if
the
SIZE
estimate
is
in
thousand
of
delivered
line
of
code
KLOC
the
total
effort
E
in
person-months
PM
can
be
given
by
the
equa-
tion
E
=
5.2
SIZE
.91
In
the
COnstructive
COst
MOdel
COCOMO
12
13
for
the
initial
estimate
also
called
nominal
estimate
the
equation
for
an
or-
ganic
project
is
E
=
3.9
SIZE
.91
Though
size
is
the
primary
factor
affecting
cost
other
factor
also
have
some
effect
In
the
COCOMO
model
after
determining
the
initial
estimate
some
other
factor
are
incorporated
for
obtaining
the
final
estimate
To
do
this
COCOMO
us
a
set
of
1
Examples
of
the
attribute
are
required
software
reliability
product
complexity
analyst
capability
application
experience
use
of
modern
tool
and
required
development
schedule
Each
cost
driver
ha
a
rating
scale
and
for
each
rating
a
multiplying
factor
is
provided
For
example
for
the
reliability
the
rating
scale
is
very
low
low
nominal
high
and
very
high
the
multiplying
factor
for
these
rating
are
.75
.88
1.00
1.15
and
1.40
respectively
So
if
the
reliability
requirement
for
the
project
is
judged
to
be
low
then
the
multiplying
factor
is
.75
while
if
it
is
judged
to
be
very
high
the
factor
is
1.40
The
attribute
and
their
multiplying
factor
for
different
rating
are
shown
in
Table
12
13
The
multiplying
factor
for
all
1
EAF
The
final
effort
estimate
E
is
obtained
by
multiplying
the
initial
estimate
by
the
EAF
In
other
word
adjustment
is
made
to
the
size-based
estimate
using
the
rating
for
these
1
As
an
example
consider
a
system
being
built
for
supporting
auction
in
a
university
some
of
the
use
case
of
this
were
discussed
in
the
previous
chapter
From
the
use
case
and
other
requirement
it
is
decided
that
the
system
will
comprise
a
few
different
module
The
module
and
their
expected
size
are
The
total
size
of
this
software
is
estimated
to
be
If
we
want
to
use
COCOMO
for
estimation
we
should
estimate
the
value
of
the
different
cost
driver
Suppose
we
expect
that
the
complexity
of
the
system
is
high
the
programmer
capability
is
low
and
the
application
experience
of
the
team
is
low
All
other
factor
have
a
nominal
rating
From
these
the
effort
adjustment
factor
EAF
is
The
initial
effort
estimate
for
the
project
is
obtained
from
the
relevant
equa-
tions
We
have
From
the
overall
estimate
estimate
of
the
effort
required
for
the
different
phase
in
the
project
can
also
be
determined
This
is
generally
done
by
using
an
effort
distribution
among
phase
The
percentage
of
total
effort
spent
in
a
phase
varies
with
the
type
and
size
of
the
project
and
can
be
obtained
from
data
of
similar
project
in
the
past
A
general
distribution
of
effort
among
different
phase
wa
discussed
in
the
previous
chapter
The
effort
distribution
suggested
by
COCOMO
for
one
type
of
software
system
is
given
in
Table
It
should
be
noted
that
to
use
the
top-down
approach
for
estimation
even
if
we
have
a
suitable
function
we
need
to
have
an
estimate
of
the
project
size
In
other
word
we
have
replaced
the
problem
of
effort
estimation
by
size
estimation
One
may
then
ask
why
not
directly
do
effort
estimation
rather
than
size
estimation
The
answer
is
that
size
estimation
is
often
easier
than
direct
effort
estimation
This
is
mainly
due
to
the
fact
that
the
system
size
can
be
estimated
from
the
size
of
it
component
which
is
often
easier
to
do
by
adding
the
size
estimate
of
all
the
component
Similar
property
doe
not
hold
for
effort
estimation
a
effort
for
developing
a
system
is
not
the
sum
of
effort
for
developing
the
component
a
additional
effort
is
needed
for
integration
and
other
such
activity
when
building
a
system
from
developed
component
Clearly
for
top-down
estimation
to
work
well
it
is
important
that
good
estimate
for
the
size
of
the
software
be
obtained
There
is
no
known
simple
method
for
estimating
the
size
accurately
When
estimating
software
size
the
best
way
may
be
to
get
a
much
detail
a
possible
about
the
software
to
be
developed
and
to
be
aware
of
our
bias
when
estimating
the
size
of
the
vari-
ous
component
By
obtaining
detail
and
using
them
for
size
estimation
the
estimate
are
likely
to
be
closer
to
the
actual
size
of
the
final
software
A
somewhat
different
approach
for
effort
estimation
is
the
bottom-up
approach
In
this
approach
the
project
is
first
divided
into
task
and
then
estimate
for
the
different
task
of
the
project
are
obtained
From
the
estimate
of
the
different
task
the
overall
estimate
is
determined
That
is
the
overall
estimate
of
the
project
is
derived
from
the
estimate
of
it
part
This
type
of
approach
is
also
called
activity-based
estimation
Essentially
in
this
approach
the
size
and
complexity
of
the
project
is
captured
in
the
set
of
task
the
project
ha
to
perform
The
bottom-up
approach
lends
itself
to
direct
estimation
of
effort
once
the
project
is
partitioned
into
smaller
task
it
is
possible
to
directly
estimate
the
effort
required
for
them
especially
if
task
are
relatively
small
One
difficulty
in
this
approach
is
that
to
get
the
overall
estimate
all
the
task
have
to
be
enumerated
A
risk
of
bottom-up
method
is
that
one
may
omit
some
activity
Also
directly
estimating
the
effort
for
some
overhead
task
such
a
project
management
that
span
the
project
can
be
difficult
If
architecture
of
the
system
to
be
built
ha
been
developed
and
if
past
information
about
how
effort
is
distributed
over
different
phase
is
known
then
the
bottom-up
approach
need
not
completely
list
all
the
task
and
a
le
tedious
approach
is
possible
Here
we
describe
one
such
approach
used
in
a
commercial
organization
58
In
this
approach
the
major
program
or
unit
or
module
in
the
software
being
built
are
first
determined
Each
program
unit
is
then
classified
a
simple
medium
or
complex
based
on
certain
criterion
For
each
classification
unit
an
average
effort
for
coding
and
unit
testing
is
decided
This
average
coding
effort
can
be
based
on
past
data
from
a
similar
project
from
some
guideline
or
on
experience
of
people
Once
the
number
of
unit
in
the
three
category
of
complexity
is
known
and
the
estimated
coding
effort
for
each
program
is
selected
the
total
coding
effort
for
the
project
is
known
From
the
total
coding
effort
the
effort
required
for
the
other
phase
and
activity
in
the
project
is
determined
a
a
percentage
of
coding
effort
For
this
from
information
about
the
past
performance
of
the
process
the
likely
distribution
of
effort
in
different
phase
of
this
project
is
determined
This
distribution
is
then
used
to
determine
the
effort
for
other
phase
and
activity
from
the
effort
estimate
of
coding
From
these
estimate
the
total
effort
for
the
project
is
obtained
This
approach
lends
itself
to
a
judicious
mixture
of
experience
and
data
If
suitable
past
data
are
not
available
for
example
if
launching
a
new
type
of
project
one
can
estimate
the
coding
effort
using
experience
once
the
nature
of
the
different
type
of
unit
is
specified
With
this
estimate
we
can
obtain
the
estimate
for
other
activity
by
working
with
some
reasonable
or
standard
effort
distribution
This
strategy
can
easily
account
for
activity
that
are
sometimes
difficult
to
enumerate
early
but
do
consume
effort
by
budgeting
effort
for
an
other
or
miscellaneous
category
The
procedure
for
estimation
can
be
summarized
a
the
following
sequence
of
step
Identify
module
in
the
system
and
classify
them
a
simple
medium
or
complex
Determine
the
average
coding
effort
for
simple/medium/complex
module
Get
the
total
coding
effort
using
the
coding
effort
of
different
type
of
module
and
the
count
for
them
Using
the
effort
distribution
for
similar
project
estimate
the
effort
for
other
task
and
the
total
effort
Refine
the
estimate
based
on
project-specific
factor
This
procedure
us
a
judicious
mixture
of
past
data
in
the
form
of
dis-
tribution
of
effort
and
experience
of
the
programmer
This
approach
is
also
simple
and
similar
to
how
many
of
u
plan
any
project
For
this
reason
for
small
project
many
people
find
this
approach
natural
and
comfortable
Note
that
this
method
of
classifying
program
into
a
few
category
and
us-
ing
an
average
coding
effort
for
each
category
is
used
only
for
effort
estimation
In
detailed
scheduling
when
a
project
manager
assigns
each
unit
to
a
member
of
the
team
for
coding
and
budget
time
for
the
activity
characteristic
of
the
unit
are
taken
into
account
to
give
more
or
le
time
than
the
average
After
establishing
a
goal
on
the
effort
front
we
need
to
establish
the
goal
for
delivery
schedule
With
the
effort
estimate
in
person-months
it
may
be
tempting
to
pick
any
project
duration
based
on
convenience
and
then
fix
a
suitable
team
size
to
ensure
that
the
total
effort
match
the
estimate
However
a
is
well
known
now
person
and
month
are
not
fully
interchangeable
in
a
software
project
Person
and
month
can
be
interchanged
arbitrarily
only
if
all
the
task
in
the
project
can
be
done
in
parallel
and
no
communication
is
needed
between
people
performing
the
task
This
is
not
true
for
software
projects—there
are
dependency
between
task
e.g.
testing
can
only
be
done
after
coding
is
done
and
a
person
performing
some
task
in
a
project
need
to
communicate
with
others
performing
other
task
As
Brooks
ha
pointed
out
16
...
man
and
month
are
interchangeable
only
for
activity
that
require
no
communication
among
men
like
sowing
wheat
or
reaping
cotton
This
is
not
even
approximately
true
of
software
However
for
a
project
with
some
estimated
effort
multiple
schedule
or
project
duration
are
indeed
possible
For
example
for
a
project
whose
effort
estimate
is
5-months
a
total
schedule
of
A
schedule
of
a
is
a
schedule
of
approximately
But
a
schedule
of
5
Similarly
no
one
would
execute
the
project
in
2
In
other
word
once
the
effort
is
fixed
there
is
some
flexibility
in
setting
the
schedule
by
appropriately
staffing
the
project
but
this
flexibility
is
not
unlimited
Empirical
data
also
suggests
that
no
simple
equation
between
effort
and
schedule
fit
well
72
The
objective
is
to
fix
a
reasonable
schedule
that
can
be
achieved
if
suit-
able
number
of
resource
are
assigned
One
method
to
determine
the
overall
schedule
is
to
determine
it
a
a
function
of
effort
Such
function
can
be
de-
termined
from
data
from
completed
project
using
statistical
technique
like
fitting
a
regression
curve
through
the
scatter
plot
obtained
by
plotting
the
ef-
fort
and
schedule
of
past
project
This
curve
is
generally
nonlinear
because
the
schedule
doe
not
grow
linearly
with
effort
Many
model
follow
this
approach
2
12
The
IBM
Federal
Systems
Division
found
that
the
total
duration
M
in
calendar
month
can
be
estimated
by
M
=
4.1E.36
In
COCOMO
the
equation
for
schedule
for
an
organic
type
of
software
is
M
=
2.5E.38
As
schedule
is
not
a
function
solely
of
effort
the
schedule
determined
in
this
manner
is
essentially
a
guideline
Another
method
for
checking
a
schedule
for
medium-sized
project
is
the
rule
of
thumb
called
the
square
root
check
58
This
check
suggests
that
the
proposed
schedule
can
be
around
the
square
root
of
the
total
effort
in
person-
month
This
schedule
can
be
met
if
suitable
resource
are
assigned
to
the
project
For
example
if
the
effort
estimate
is
50
person-months
a
schedule
of
about
From
this
macro
estimate
of
schedule
we
can
determine
the
schedule
for
the
major
milestone
in
the
project
To
determine
the
milestone
we
must
first
understand
the
manpower
ramp-up
that
usually
take
place
in
a
project
The
number
of
people
that
can
be
gainfully
utilized
in
a
software
project
tends
to
follow
the
Rayleigh
curve
71
72
That
is
in
the
beginning
and
the
end
few
people
are
needed
on
the
project
the
peak
team
size
PTS
is
needed
somewhere
near
the
middle
of
the
project
and
again
fewer
people
are
needed
after
that
This
occurs
because
only
a
few
people
are
needed
and
can
be
used
in
the
initial
phase
of
requirement
analysis
and
design
The
human
resource
requirement
peak
during
coding
and
unit
testing
and
during
system
testing
and
integration
again
fewer
people
are
required
Often
the
staffing
level
is
not
changed
continuously
in
a
project
and
ap-
proximations
of
the
Rayleigh
curve
are
used
assigning
a
few
people
at
the
start
having
the
peak
team
during
the
coding
phase
and
then
leaving
a
few
people
for
integration
and
system
testing
If
we
consider
design
and
analysis
build
and
test
a
three
major
phase
the
manpower
ramp-up
in
project
typically
resembles
the
function
shown
in
Figure
58
For
ease
of
scheduling
particularly
for
smaller
project
often
the
required
people
are
assigned
together
around
the
start
of
the
project
This
approach
can
lead
to
some
people
being
unoccupied
at
the
start
and
toward
the
end
This
slack
time
is
often
used
for
supporting
project
activity
like
training
and
documentation
Given
the
effort
estimate
for
a
phase
we
can
determine
the
duration
of
the
phase
if
we
know
the
manpower
ramp-up
For
these
three
major
phase
the
percentage
of
the
schedule
consumed
in
the
build
phase
is
smaller
than
the
percentage
of
the
effort
consumed
because
this
phase
involves
more
people
Similarly
the
percentage
of
the
schedule
consumed
in
the
design
and
testing
phase
exceeds
their
effort
percentage
The
exact
schedule
depends
on
the
planned
manpower
ramp-up
and
how
many
resource
can
be
used
effectively
in
a
phase
on
that
project
Generally
speaking
design
requires
about
a
quarter
of
the
schedule
build
consumes
about
half
and
integration
and
system
test-
ing
consume
the
remaining
quarter
COCOMO
give
19
%
for
design
62
%
for
programming
and
18
%
for
integration
Having
set
the
goal
for
effort
and
schedule
the
goal
for
the
third
key
dimension
of
a
project—quality—needs
to
be
defined
However
unlike
schedule
and
effort
quantified
quality
goal
setting
for
a
project
and
then
planning
to
meet
it
is
much
harder
For
effort
and
schedule
goal
we
can
easily
check
if
a
detailed
plan
meet
these
goal
e.g.
by
seeing
if
the
last
task
end
before
the
target
date
and
if
the
sum
total
of
effort
of
all
task
is
le
than
the
overall
effort
goal
For
quality
even
if
we
set
the
goal
in
term
of
expected
delivered
defect
density
it
is
not
easy
to
plan
for
achieving
this
goal
or
for
checking
if
a
plan
can
meet
these
goal
Hence
often
quality
goal
are
specified
in
term
of
acceptance
criteria—
the
delivered
software
should
finally
work
for
all
the
situation
and
test
case
in
the
acceptance
criterion
Further
there
may
even
be
an
acceptance
criterion
on
the
number
of
defect
that
can
be
found
during
the
acceptance
testing
For
example
no
more
than
n
defect
are
uncovered
by
acceptance
testing
The
quality
plan
is
the
set
of
quality-related
activity
that
a
project
plan
to
do
to
achieve
the
quality
goal
To
plan
for
quality
let
u
first
understand
the
defect
injection
and
removal
cycle
a
it
is
defect
that
determine
the
quality
of
the
final
delivered
software
Software
development
is
a
highly
people-oriented
activity
and
hence
it
is
error-prone
In
a
software
project
we
start
with
no
defect
there
is
no
soft-
ware
to
contain
defect
Defects
are
injected
into
the
software
being
built
during
the
different
phase
in
the
project
That
is
during
the
transformation
from
user
need
to
software
to
satisfy
those
need
defect
are
injected
in
the
transformation
activity
undertaken
These
injection
stage
are
primarily
the
requirement
specification
the
high-level
design
the
detailed
design
and
cod-
ing
To
ensure
that
high-quality
software
is
delivered
these
defect
are
removed
through
the
quality
control
QC
activity
The
QC
activity
for
defect
re-
moval
include
requirement
review
design
review
code
review
unit
testing
integration
testing
system
testing
acceptance
testing
etc
Figure
show
the
process
of
defect
injection
and
removal
As
the
final
goal
is
to
deliver
software
with
low
defect
density
ensuring
quality
revolves
around
two
main
theme
reduce
the
defect
being
injected
and
increase
the
defect
being
removed
The
first
is
often
done
through
standard
methodology
following
of
good
process
etc.
which
help
reduce
the
chance
of
error
by
the
project
personnel
There
are
specific
technique
for
defect
prevention
also
The
quality
plan
therefore
focus
mostly
on
planning
suitable
quality
control
task
for
removing
defect
Reviews
and
testing
are
two
most
common
QC
activity
utilized
in
a
project
Whereas
review
are
structured
human-oriented
process
testing
is
the
process
of
executing
software
or
part
of
it
in
an
attempt
to
identify
de-
fects
The
most
common
approach
for
quality
planning
in
a
project
is
to
specify
the
QC
activity
to
be
performed
in
the
project
and
have
suitable
guideline
for
performing
each
of
the
QC
task
such
that
the
chance
of
meeting
the
qual-
ity
goal
are
high
During
project
execution
these
activity
are
carried
out
in
accordance
with
the
defined
procedure
When
this
approach
is
used
for
ensuring
quality
making
quantitative
claim
can
be
quite
hard
For
quantitative
assessment
of
the
quality
process
metrics-based
analysis
is
necessary
That
however
is
an
advanced
topic
beyond
the
scope
of
this
book
and
indeed
many
organization
Hence
for
ensuring
quality
the
reliance
is
primarily
on
applying
suitable
QC
technique
at
the
right
place
in
the
process
and
using
experience
to
ensure
that
sufficient
QC
task
are
done
in
the
project
Hence
the
quality
plan
for
the
project
is
largely
a
specification
of
which
QC
task
is
to
be
done
and
when
and
what
process
and
guideline
are
to
be
used
for
performing
the
QC
task
The
choice
depends
on
the
nature
and
goal
and
constraint
of
the
project
Typically
the
QC
task
will
be
schedulable
task
in
the
detailed
schedule
of
the
project
For
example
it
will
specify
what
document
will
be
inspected
what
part
of
the
code
will
be
inspected
and
what
level
of
testing
will
be
performed
The
plan
can
be
considerably
enhanced
if
some
expectation
of
defect
level
that
are
expected
to
be
found
for
the
different
quality
control
task
are
also
mentioned—these
can
then
aid
the
monitoring
of
quality
a
the
project
proceeds
A
software
project
is
a
complex
undertaking
Unforeseen
event
may
have
an
adverse
impact
on
a
project
’
s
ability
to
meet
the
cost
schedule
or
quality
goal
Risk
management
is
an
attempt
to
minimize
the
chance
of
failure
caused
by
unplanned
event
The
aim
of
risk
management
is
not
to
avoid
getting
into
project
that
have
risk
but
to
minimize
the
impact
of
risk
in
the
project
that
are
undertaken
A
risk
is
a
probabilistic
event—it
may
or
may
not
occur
For
this
reason
we
frequently
have
an
optimistic
tendency
to
simply
not
see
risk
or
to
hope
that
they
will
not
occur
Social
and
organizational
factor
also
may
stigmatize
risk
and
discourage
clear
identification
of
them
19
This
kind
of
attitude
get
the
project
in
trouble
if
the
risk
event
materialize
something
that
is
likely
to
happen
in
a
large
project
Not
surprisingly
then
risk
management
is
considered
first
among
the
best
practice
for
managing
large
software
project
17
It
first
came
to
the
forefront
with
Boehm
’
s
tutorial
on
risk
management
11
Since
then
several
book
have
targeted
risk
management
for
software
18
45
Risk
is
defined
a
an
exposure
to
the
chance
of
injury
or
loss
That
is
risk
implies
that
there
is
a
possibility
that
something
negative
may
happen
In
the
context
of
software
project
negative
implies
that
there
is
an
adverse
effect
on
cost
quality
or
schedule
Risk
management
is
the
area
that
try
to
ensure
that
the
impact
of
risk
on
cost
quality
and
schedule
is
minimal
Risk
management
can
be
considered
a
dealing
with
the
possibility
and
actual
occurrence
of
those
event
that
are
not
regular
or
commonly
expected
that
is
they
are
probabilistic
The
commonly
expected
event
such
a
people
going
on
leave
or
some
requirement
changing
are
handled
by
normal
project
management
So
in
a
sense
risk
management
begin
where
normal
project
management
end
It
deal
with
event
that
are
infrequent
somewhat
out
of
the
control
of
the
project
management
and
which
can
have
a
major
impact
on
the
project
Most
project
have
risk
The
idea
of
risk
management
is
to
minimize
the
possibility
of
risk
materializing
if
possible
or
to
minimize
the
effect
if
risk
actually
materialize
For
example
when
constructing
a
building
there
is
a
risk
that
the
building
may
later
collapse
due
to
an
earthquake
That
is
the
possibil-
ity
of
an
earthquake
is
a
risk
If
the
building
is
a
large
residential
complex
then
the
potential
cost
in
case
the
earthquake
risk
materializes
can
be
enormous
This
risk
can
be
reduced
by
shifting
to
a
zone
that
is
not
earthquake-prone
Alternatively
if
this
is
not
acceptable
then
the
effect
of
this
risk
materializing
are
minimized
by
suitably
constructing
the
building
the
approach
taken
in
Japan
and
California
At
the
same
time
if
a
small
dumping
ground
is
to
be
constructed
no
such
approach
might
be
followed
a
the
financial
and
other
impact
of
an
actual
earthquake
on
such
a
building
is
so
low
that
it
doe
not
warrant
special
measure
It
should
be
clear
that
risk
management
ha
to
deal
with
identifying
the
undesirable
event
that
can
occur
the
probability
of
their
occurring
and
the
loss
if
an
undesirable
event
doe
occur
Once
this
is
known
strategy
can
be
formulated
for
either
reducing
the
probability
of
the
risk
materializing
or
re-
ducing
the
effect
of
risk
materializing
So
the
risk
management
revolves
around
risk
assessment
and
risk
control
The
goal
of
risk
assessment
is
to
prioritize
the
risk
so
that
attention
and
resource
can
be
focused
on
the
more
risky
item
Risk
identification
is
the
first
step
in
risk
assessment
which
identifies
all
the
different
risk
for
a
particular
project
These
risk
are
project-dependent
and
identifying
them
is
an
exercise
in
envisioning
what
can
go
wrong
Methods
that
can
aid
risk
identification
include
checklist
of
possible
risk
survey
meeting
and
brainstorming
and
review
of
plan
process
and
work
product
45
Checklists
of
frequently
occurring
risk
are
probably
the
most
common
tool
for
risk
identification—most
organization
prepare
a
list
of
commonly
occurring
risk
for
project
prepared
from
a
survey
of
previous
project
Such
a
list
can
form
the
starting
point
for
identifying
risk
for
the
current
project
Based
on
survey
of
experienced
project
manager
Boehm
11
ha
produced
a
list
of
the
top
10
risk
item
likely
to
compromise
the
success
of
a
software
project
Figure
show
some
of
these
risk
along
with
the
technique
preferred
by
management
for
managing
these
risk
Top
risk
in
a
commercial
software
organization
can
be
found
in
58
The
top-ranked
risk
item
is
personnel
shortfall
This
involves
just
having
fewer
people
than
necessary
or
not
having
people
with
specific
skill
that
a
project
might
require
Some
of
the
way
to
manage
this
risk
are
to
get
the
top
talent
possible
and
to
match
the
need
of
the
project
with
the
skill
of
the
available
personnel
Adequate
training
along
with
having
some
key
personnel
for
critical
area
of
the
project
will
also
reduce
this
risk
The
second
item
unrealistic
schedule
and
budget
happens
very
frequently
due
to
business
and
other
reason
It
is
very
common
that
high-level
manage-
ment
imposes
a
schedule
for
a
software
project
that
is
not
based
on
the
char-
acteristics
of
the
project
and
is
unrealistic
Underestimation
may
also
happen
due
to
inexperience
or
optimism
The
next
few
item
are
related
to
requirement
Projects
run
the
risk
of
developing
the
wrong
software
if
the
requirement
analysis
is
not
done
properly
and
if
development
begin
too
early
Similarly
often
improper
user
interface
may
be
developed
This
requires
extensive
rework
of
the
user
interface
later
or
the
software
benefit
are
not
obtained
because
user
are
reluctant
to
use
it
Gold
plating
refers
to
adding
feature
in
the
software
that
are
only
marginally
useful
This
add
unnecessary
risk
to
the
project
because
gold
plating
consumes
resource
and
time
with
little
return
Risk
identification
merely
identifies
the
undesirable
event
that
might
take
place
during
the
project
i.e.
enumerates
the
unforeseen
event
that
might
occur
It
doe
not
specify
the
probability
of
these
risk
materializing
nor
the
impact
on
the
project
if
the
risk
indeed
materialize
Hence
the
next
task
are
risk
analysis
and
prioritization
In
risk
analysis
the
probability
of
occurrence
of
a
risk
ha
to
be
estimated
along
with
the
loss
that
will
occur
if
the
risk
doe
materialize
This
is
often
done
through
discussion
using
experience
and
understanding
of
the
situation
though
structured
approach
also
exist
Once
the
probability
of
risk
materializing
and
loss
due
to
materializa-
tion
of
different
risk
have
been
analyzed
they
can
be
prioritized
One
approach
for
prioritization
is
through
the
concept
of
risk
exposure
RE
11
which
is
sometimes
called
risk
impact
RE
is
defined
by
the
relationship
where
Prob
UO
is
the
probability
of
the
risk
materializing
i.e.
undesirable
outcome
and
Loss
UO
is
the
total
loss
incurred
due
to
the
unsatisfactory
outcome
The
loss
is
not
only
the
direct
financial
loss
that
might
be
incurred
but
also
any
loss
in
term
of
credibility
future
business
and
loss
of
property
or
life
The
RE
is
the
expected
value
of
the
loss
due
to
a
particular
risk
For
risk
prioritization
using
RE
is
the
higher
the
RE
the
higher
the
priority
of
the
risk
item
The
main
objective
of
risk
management
is
to
identify
the
top
few
risk
item
and
then
focus
on
them
Once
a
project
manager
ha
identified
and
prioritized
the
risk
the
top
risk
can
be
easily
identified
The
question
then
becomes
what
to
do
about
them
Knowing
the
risk
is
of
value
only
if
you
can
prepare
a
plan
so
that
their
consequence
are
minimal—that
is
the
basic
goal
of
risk
management
One
obvious
strategy
is
risk
avoidance
which
entail
taking
action
that
will
avoid
the
risk
altogether
like
the
earlier
example
of
shifting
the
building
site
to
a
zone
that
is
not
earthquake-prone
For
some
risk
avoidance
might
be
possible
For
most
risk
the
strategy
is
to
perform
the
action
that
will
either
reduce
the
probability
of
the
risk
materializing
or
reduce
the
loss
due
to
the
risk
materializing
These
are
called
risk
mitigation
step
To
decide
what
mitigation
step
to
take
a
list
of
commonly
used
risk
mitigation
step
for
various
risk
is
very
useful
here
For
the
risk
mentioned
in
Figure
suitable
risk
mitigation
step
are
also
given
Note
that
unlike
risk
assessment
which
is
largely
an
analytical
exercise
risk
mitigation
comprises
active
measure
that
have
to
be
performed
to
minimize
the
impact
of
risk
In
other
word
selecting
a
risk
mitigation
step
is
not
just
an
intellectual
exercise
The
risk
mitigation
step
must
be
executed
and
monitored
To
ensure
that
the
needed
action
are
executed
properly
they
must
be
incorporated
into
the
detailed
project
schedule
Risk
prioritization
and
consequent
planning
are
based
on
the
risk
perception
at
the
time
the
risk
analysis
is
performed
Because
risk
are
probabilistic
event
that
frequently
depend
on
external
factor
the
threat
due
to
risk
may
change
with
time
a
factor
change
Clearly
then
the
risk
perception
may
also
change
with
time
Furthermore
the
risk
mitigation
step
undertaken
may
affect
the
risk
perception
This
dynamism
implies
that
risk
in
a
project
should
not
be
treated
a
static
and
must
be
monitored
and
reevaluated
periodically
Hence
in
addition
to
monitoring
the
progress
of
the
planned
risk
mitigation
step
a
project
must
periodically
revisit
the
risk
perception
and
modify
the
risk
mitigation
plan
if
needed
Risk
monitoring
is
the
activity
of
monitoring
the
status
of
various
risk
and
their
control
activity
One
simple
approach
for
risk
monitoring
is
to
analyze
the
risk
afresh
at
each
major
milestone
and
change
the
plan
a
needed
Though
the
concept
of
risk
exposure
is
rich
a
simple
practical
way
of
doing
risk
planning
is
to
simply
categorize
risk
and
the
impact
in
a
few
level
and
then
use
it
for
prioritization
This
approach
is
used
in
many
organization
Here
we
discus
a
simple
approach
used
in
an
organization
58
In
this
approach
the
probability
of
a
risk
occurring
is
categorized
a
low
medium
or
high
The
risk
impact
can
also
be
classified
a
low
medium
and
high
With
these
rating
the
following
simple
method
for
risk
prioritization
can
be
specified
For
each
risk
rate
the
probability
of
it
happening
a
low
medium
or
high
For
each
risk
ass
it
impact
on
the
project
a
low
medium
or
high
Rank
the
risk
based
on
the
probability
and
effect
on
the
project
for
example
a
high-probability
high-impact
item
will
have
higher
rank
than
a
risk
item
with
a
medium
probability
and
high
impact
In
case
of
conflict
use
judgment
Select
the
top
few
risk
item
for
mitigation
and
tracking
An
example
of
this
approach
is
given
in
Table
which
show
the
various
rating
and
the
risk
mitigation
step
58
As
we
can
see
the
risk
management
plan
which
is
essentially
this
table
can
be
very
brief
and
focused
For
moni-
toring
the
risk
one
way
is
to
redo
risk
management
planning
at
milestone
giving
more
attention
to
the
risk
listed
in
the
project
plan
During
risk
mon-
itoring
at
milestone
reprioritization
may
occur
and
mitigation
plan
for
the
remainder
of
the
project
may
change
depending
on
the
current
situation
and
the
impact
of
mitigation
step
taken
earlier
A
project
management
plan
is
merely
a
document
that
can
be
used
to
guide
the
execution
of
a
project
Even
a
good
plan
is
useless
unless
it
is
properly
executed
And
execution
can
not
be
properly
driven
by
the
plan
unless
it
is
monitored
carefully
and
the
actual
performance
is
tracked
against
the
plan
Monitoring
requires
measurement
to
be
made
to
ass
the
situation
of
a
project
If
measurement
are
to
be
taken
during
project
execution
we
must
plan
carefully
regarding
what
to
measure
when
to
measure
and
how
to
measure
Hence
measurement
planning
is
a
key
element
in
project
planning
In
addition
how
the
measurement
data
will
be
analyzed
and
reported
must
also
be
planned
in
advance
to
avoid
the
situation
of
collecting
data
but
not
knowing
what
to
do
with
it
Without
careful
planning
for
data
collection
and
it
analysis
neither
is
likely
to
happen
In
this
section
we
discus
the
issue
of
measurement
and
project
tracking
The
basic
purpose
of
measurement
in
a
project
is
to
provide
data
to
project
management
about
the
project
’
s
current
state
such
that
they
can
effectively
monitor
and
control
the
project
and
ensure
that
the
project
goal
are
met
As
project
goal
are
established
in
term
of
software
to
be
delivered
cost
schedule
and
quality
for
monitoring
the
state
of
a
project
size
effort
schedule
and
defect
are
the
basic
measurement
that
are
needed
43
75
Schedule
is
one
of
the
most
important
metric
because
most
project
are
driven
by
schedule
and
deadline
Only
by
monitoring
the
actual
schedule
can
we
properly
ass
if
the
project
is
on
time
or
if
there
is
a
delay
It
is
however
easy
to
measure
because
calendar
time
is
usually
used
in
all
plan
Effort
is
the
main
resource
consumed
in
a
software
project
Consequently
tracking
of
effort
is
a
key
activity
during
monitoring
it
is
essential
for
evaluating
whether
the
project
is
executing
within
budget
For
effort
data
some
type
of
timesheet
system
is
needed
where
each
person
working
on
the
project
enters
the
amount
of
time
spent
on
the
project
For
better
monitoring
the
effort
spent
on
various
task
should
be
logged
separately
Generally
effort
is
recorded
through
some
on-line
system
like
the
weekly
activity
report
system
in
57
which
allows
a
person
to
record
the
amount
of
time
spent
against
a
particular
activity
in
a
project
At
any
point
total
effort
on
an
activity
can
be
aggregated
Because
defect
have
a
direct
relationship
to
software
quality
tracking
of
defect
is
critical
for
ensuring
quality
A
large
software
project
may
include
thousand
of
defect
that
are
found
by
different
people
at
different
stage
Just
to
keep
track
of
the
defect
found
and
their
status
defect
must
be
logged
and
their
closure
tracked
If
defect
found
are
being
logged
monitoring
can
focus
on
how
many
defect
have
been
found
so
far
what
percentage
of
defect
are
still
open
and
other
issue
Defect
tracking
is
considered
one
of
the
best
practice
for
managing
a
project
17
Size
is
another
fundamental
metric
because
it
represents
progress
toward
delivering
the
desired
functionality
and
many
data
for
example
delivered
de-
fect
density
are
normalized
with
respect
to
size
The
size
of
delivered
software
can
be
measured
in
term
of
LOC
which
can
be
determined
through
the
use
of
regular
editor
and
line
counter
or
function
point
At
a
more
gross
level
just
the
number
of
module
or
number
of
feature
might
suffice
For
effective
monitoring
a
project
must
plan
for
collecting
these
measure-
ments
Most
often
organization
provide
tool
and
policy
support
for
recording
this
basic
data
which
is
then
available
to
project
manager
for
tracking
The
main
goal
of
project
manager
for
monitoring
a
project
is
to
get
visibility
into
the
project
execution
so
that
they
can
determine
whether
any
action
need
to
be
taken
to
ensure
that
the
project
goal
are
met
As
project
goal
are
in
term
of
effort
schedule
and
quality
the
focus
of
monitoring
is
on
these
aspect
Different
level
of
monitoring
might
be
done
for
a
project
The
three
main
level
of
monitoring
are
activity
level
status
reporting
and
milestone
analysis
Measurements
taken
on
the
project
are
employed
for
monitoring
Activity-level
monitoring
ensures
that
each
activity
in
the
detailed
schedule
ha
been
done
properly
and
within
time
This
type
of
monitoring
may
be
done
daily
in
project
team
meeting
or
by
the
project
manager
checking
the
status
of
all
the
task
scheduled
to
be
completed
on
that
day
A
completed
task
is
often
marked
a
100
%
complete
in
detailed
schedule—this
is
used
by
tool
like
the
Microsoft
Project
to
track
the
percentage
completion
of
the
overall
project
or
a
higher-level
task
This
monitoring
is
to
ensure
that
the
project
continues
to
proceed
a
per
the
planned
schedule
Status
report
are
often
prepared
weekly
to
take
stock
of
what
ha
happened
and
what
need
to
be
done
Status
report
typically
contain
a
summary
of
the
activity
successfully
completed
since
the
last
status
report
any
activity
that
have
been
delayed
any
issue
in
the
project
that
need
attention
and
if
everything
is
in
place
for
the
next
week
Again
the
purpose
of
this
is
to
ensure
that
the
project
is
proceeding
a
per
the
planned
schedule
The
milestone
analysis
is
done
at
each
milestone
or
every
few
week
if
milestone
are
too
far
apart
and
is
more
elaborate
Analysis
of
actual
versus
estimated
for
effort
and
schedule
is
often
included
in
the
milestone
analysis
If
the
deviation
is
significant
it
may
imply
that
the
project
may
run
into
trouble
and
might
not
meet
it
objective
This
situation
call
for
project
manager
to
understand
the
reason
for
the
variation
and
to
apply
corrective
and
preventive
action
if
necessary
Defects
found
by
different
quality
control
task
and
the
number
of
defect
fixed
may
also
be
reported
This
report
monitor
the
progress
of
the
project
with
respect
to
all
the
goal
The
activity
discussed
so
far
result
in
a
project
management
plan
document
that
establishes
the
project
goal
for
effort
schedule
and
quality
and
defines
the
approach
for
risk
management
ensuring
quality
and
project
monitoring
Now
this
overall
plan
ha
to
be
translated
into
a
detailed
action
plan
which
can
then
be
followed
in
the
project
and
which
if
followed
will
lead
to
a
successful
project
That
is
we
need
to
develop
a
detailed
plan
or
schedule
of
what
to
do
when
such
that
following
this
plan
will
lead
to
delivering
the
software
with
expected
quality
and
within
cost
and
schedule
Whereas
the
overall
planning
document
is
typically
prepared
at
the
start
of
the
project
and
is
relatively
static
the
detailed
plan
is
a
dynamic
document
that
reflects
the
current
plan
of
the
project
The
detailed
plan
is
what
assigns
work
item
to
individual
member
of
the
team
For
the
detailed
schedule
the
major
phase
identified
during
effort
and
schedule
estimation
are
broken
into
small
schedulable
activity
in
a
hierarchi-
cal
manner
For
example
the
detailed
design
phase
can
be
broken
into
task
for
developing
the
detailed
design
for
each
module
review
of
each
detailed
design
fixing
of
defect
found
and
so
on
For
each
detailed
task
the
project
manager
estimate
the
time
required
to
complete
it
and
assigns
a
suitable
resource
so
that
the
overall
schedule
is
met
and
the
overall
effort
also
match
In
addition
to
the
engineering
task
that
are
the
outcome
of
the
development
process
the
QC
task
identified
in
the
quality
plan
the
monitoring
activity
defined
in
the
monitoring
plan
and
the
risk
mitigation
activity
should
also
be
scheduled
At
each
level
of
refinement
the
project
manager
determines
the
effort
for
the
overall
task
from
the
detailed
schedule
and
check
it
against
the
effort
estimate
If
this
detailed
schedule
is
not
consistent
with
the
overall
schedule
and
effort
estimate
the
detailed
schedule
must
be
changed
If
it
is
found
that
the
best
detailed
schedule
can
not
match
the
milestone
effort
and
schedule
then
the
earlier
estimate
must
be
revised
Thus
scheduling
is
an
iterative
process
Generally
the
project
manager
refines
the
task
to
a
level
so
that
the
lowest-level
activity
can
be
scheduled
to
occupy
no
more
than
a
few
day
from
a
single
resource
Activities
related
to
task
such
a
project
management
co-
ordination
database
management
and
configuration
management
may
also
be
listed
in
the
schedule
even
though
these
activity
have
le
direct
effect
on
de-
termining
the
schedule
because
they
are
ongoing
task
rather
than
schedulable
activity
Nevertheless
they
consume
resource
and
hence
are
often
included
in
the
project
schedule
Rarely
will
a
project
manager
complete
the
detailed
schedule
of
the
entire
project
all
at
once
Once
the
overall
schedule
is
fixed
detailing
for
a
phase
may
only
be
done
at
the
start
of
that
phase
For
detailed
scheduling
tool
like
Microsoft
Project
or
a
spreadsheet
can
be
very
useful
For
each
lowest-level
activity
the
project
manager
specifies
the
effort
duration
start
date
end
date
and
resource
Dependencies
between
activity
due
either
to
an
inherent
dependency
for
example
you
can
conduct
a
unit
test
plan
for
a
program
only
after
it
ha
been
coded
or
to
a
resource-
related
dependency
the
same
resource
is
assigned
two
task
may
also
be
specified
From
these
tool
the
overall
effort
and
schedule
of
higher-level
task
can
be
determined
A
detailed
project
schedule
is
never
static
Changes
may
be
needed
because
the
actual
progress
in
the
project
may
be
different
from
what
wa
planned
because
newer
task
are
added
in
response
to
change
request
or
because
of
other
unforeseen
situation
Changes
are
done
a
and
when
the
need
arises
The
final
schedule
frequently
maintained
using
some
suitable
tool
is
often
the
most
live
project
plan
document
During
the
project
if
plan
must
be
changed
and
additional
activity
must
be
done
after
the
decision
is
made
the
change
must
be
reflected
in
the
detailed
schedule
a
this
reflects
the
task
actually
planned
to
be
performed
Hence
the
detailed
schedule
becomes
the
main
document
that
track
the
activity
and
schedule
It
should
be
noted
that
only
the
number
of
resource
is
decided
during
the
overall
project
planning
However
detailed
scheduling
can
be
done
effec-
tively
only
after
actual
assignment
of
people
ha
been
done
a
task
assignment
need
information
about
the
capability
of
the
team
member
In
our
discus-
sion
above
we
have
implicitly
assumed
that
the
project
’
s
team
is
led
by
a
project
manager
who
doe
the
planning
and
task
assignment
This
form
of
hi-
erarchical
team
organization
is
fairly
common
and
wa
earlier
called
the
Chief
Programmer
Team
As
an
example
consider
the
example
of
a
project
from
58
The
overall
effort
estimate
for
this
project
is
50-days
or
about
2-months
this
estimation
wa
done
using
the
bottom-up
approach
discussed
earlier
The
customer
gave
approximately
5
..
Because
this
is
more
than
the
square
root
of
effort
in
person-months
this
schedule
wa
accepted
Hence
these
define
the
effort
and
schedule
goal
of
the
project
The
milestone
are
determined
by
using
the
effort
estimate
for
the
phase
and
an
estimate
of
the
number
of
resource
available
Table
show
the
high-
level
schedule
of
the
project
This
project
us
the
RUP
process
in
which
initial
requirement
and
design
is
done
in
two
iteration
and
the
development
is
done
in
three
iteration
The
overall
project
duration
with
these
milestone
is
140
day
This
high-level
schedule
is
an
outcome
of
the
overall
project
planning
and
is
not
suitable
for
assigning
resource
and
detailed
planning
For
detailed
scheduling
these
task
are
broken
into
schedulable
activity
In
this
way
the
schedule
also
becomes
a
checklist
of
task
for
the
project
As
mentioned
above
this
exploding
of
top-level
activity
is
not
done
fully
at
the
start
but
rather
take
place
many
time
during
the
project
Table
show
part
of
the
detailed
schedule
of
the
construction-iteration
For
each
activity
the
table
specifies
the
activity
by
a
short
name
the
module
to
which
the
activity
is
contributing
and
the
effort
the
duration
may
also
be
specified
For
each
task
how
much
is
completed
is
given
in
the
%
Complete
column
This
information
is
used
for
activity
tracking
The
detailed
schedule
also
specifies
the
resource
to
which
the
task
is
assigned
specified
by
initial
of
the
person
Sometimes
the
predecessor
of
the
activity
the
activity
upon
which
the
task
depends
are
also
specified
This
information
help
in
determining
the
critical
path
and
the
critical
resource
This
project
finally
had
a
total
of
about
32
Project
planning
serf
two
purpose
to
set
the
overall
goal
or
expectation
for
the
project
and
the
overall
approach
for
managing
the
project
and
to
schedule
task
to
be
done
such
that
the
goal
are
met
In
overall
planning
effort
and
schedule
estimation
are
done
to
establish
the
cost
and
schedule
goal
for
the
project
a
well
a
key
milestone
Project
quality
planning
and
risk
management
planning
establish
approach
for
achieving
quality
goal
and
ensuring
that
the
project
doe
not
fail
in
the
face
of
risk
In
the
detailed
schedule
task
for
achieving
each
milestone
are
identified
and
scheduled
to
be
executed
by
specific
team
member
such
that
the
overall
schedule
and
effort
for
different
milestone
is
met
In
a
top-down
approach
for
estimating
effort
for
the
project
the
effort
is
estimated
from
the
size
estimate
which
may
be
refined
using
other
char-
acteristics
of
the
project
Effort
for
different
phase
is
determined
from
the
overall
effort
using
effort
distribution
data
COCOMO
is
one
model
that
us
this
approach
In
a
bottom-up
approach
to
estimation
the
module
to
be
built
are
identified
first
and
then
using
an
average
effort
for
coding
such
module
the
overall
coding
effort
is
determined
From
coding
effort
effort
for
other
phase
and
overall
project
are
determined
using
the
effort
distribution
data
Some
flexibility
exists
in
setting
the
schedule
goal
for
a
project
An
overall
schedule
can
be
determined
a
a
function
of
effort
and
then
adjusted
to
meet
the
project
need
and
constraint
Once
a
reasonable
schedule
is
chosen
the
staffing
requirement
of
the
project
are
determined
using
the
project
’
s
effort
Schedule
for
milestone
are
determined
from
effort
estimate
and
the
effective
team
size
in
different
phase
With
estimation
and
scheduling
the
effort
and
schedule
goal
for
the
project
are
established
Setting
measurable
quality
goal
for
a
project
is
harder
The
quality
goal
can
be
set
in
term
of
performance
during
acceptance
testing
A
quality
plan
consists
of
quality
control
task
that
should
be
performed
in
the
project
to
achieve
the
quality
goal
A
project
can
have
risk
and
to
meet
project
goal
under
the
presence
of
risk
requires
proper
risk
management
Risks
are
those
event
which
may
or
may
not
occur
but
if
they
do
occur
they
can
have
a
negative
impact
on
the
project
Risks
can
be
prioritized
based
on
the
expected
loss
from
a
risk
which
is
a
combination
of
the
probability
of
the
risk
and
the
total
possible
loss
due
to
risk
materializing
For
high-priority
risk
activity
are
planned
during
normal
project
execution
such
that
if
the
risk
do
materialize
their
effect
is
minimal
The
progress
of
the
project
need
to
be
monitored
using
suitable
measure-
ments
so
corrective
action
can
be
taken
when
needed
The
measurement
commonly
used
for
monitoring
are
actual
schedule
effort
consumed
defect
found
and
the
size
of
the
product
Status
report
and
milestone
report
on
actual
vs.
estimated
on
effort
and
schedule
activity
that
got
missed
defect
found
so
far
and
risk
can
suffice
to
effectively
monitor
the
performance
of
a
project
with
respect
to
it
plan
The
overall
project
management
plan
document
containing
the
effort
sched-
ule
and
quality
goal
and
plan
for
quality
monitoring
and
risk
manage-
ment
set
the
overall
context
For
execution
the
overall
schedule
is
broken
into
a
detailed
schedule
of
task
to
be
done
so
a
to
meet
the
goal
and
constraint
These
task
are
assigned
to
specific
team
member
with
identified
start
and
end
date
The
sched-
ule
ha
to
be
such
that
it
is
consistent
with
the
overall
schedule
and
effort
estimate
Tasks
planned
for
quality
monitoring
and
risk
management
are
also
scheduled
in
the
detailed
schedule
This
detailed
schedule
lay
out
the
path
the
project
should
follow
in
order
to
achieve
the
project
objective
It
is
the
most
live
project
planning
document
and
any
change
in
the
project
plan
must
be
reflected
suitably
in
the
detailed
schedule
What
is
the
role
of
effort
estimation
in
a
project
and
why
is
it
important
to
do
this
estimation
early
If
an
architecture
of
the
proposed
system
ha
been
designed
specifying
the
major
component
in
the
system
and
you
have
source
code
of
similar
component
avail-
able
in
your
organization
’
s
repository
which
method
will
you
use
for
estimation
Suppose
an
organization
plan
to
use
COCOMO
for
effort
estimation
but
it
want
to
use
only
three
cost
drivers—product
complexity
programmer
capability
and
development
schedule
In
this
situation
from
the
initial
estimate
by
how
much
can
the
final
estimate
vary
Why
are
all
combination
of
people
and
month
that
are
consistent
with
the
effort
estimate
not
feasible
A
customer
asks
you
to
complete
a
project
whose
effort
estimate
is
E
in
time
T
How
will
you
decide
whether
to
accept
this
schedule
or
not
For
a
group
student
project
being
done
in
a
semester
course
list
the
major
risk
that
a
typical
project
will
face
and
risk
mitigation
strategy
for
the
high-priority
risk
Suppose
you
make
a
detailed
schedule
for
your
project
whose
effort
and
schedule
estimate
for
various
milestone
have
been
done
in
the
project
plan
How
will
you
check
if
the
detailed
schedule
is
consistent
with
the
overall
plan
What
will
you
do
if
it
is
not
Any
complex
system
is
composed
of
subsystem
that
interact
under
the
control
of
system
design
such
that
the
system
provides
the
expected
behavior
When
designing
such
a
system
therefore
the
logical
approach
is
to
identify
the
sub-
system
that
should
compose
the
system
the
interface
of
these
subsystem
and
the
rule
for
interaction
between
the
subsystem
This
is
what
software
architecture
aim
to
do
Software
architecture
is
a
relatively
recent
area
As
the
software
system
increasingly
become
distributed
and
more
complex
architecture
becomes
an
important
step
in
building
the
system
Due
to
a
wide
range
of
option
now
available
for
how
a
system
may
be
configured
and
connected
carefully
de-
signing
the
architecture
becomes
very
important
It
is
during
the
architecture
design
where
choice
like
using
some
type
of
middleware
or
some
type
of
back-
end
database
or
some
type
of
server
or
some
type
of
security
component
are
made
Architecture
is
also
the
earliest
place
when
property
like
reliability
and
performance
can
be
evaluated
for
the
system
a
capability
that
is
increasingly
becoming
important
The
key
role
an
architecture
description
play
in
a
software
project
The
multiple
architectural
view
that
can
be
used
to
specify
different
struc-
tural
aspect
of
the
system
being
built
The
component
and
connector
architecture
of
a
system
and
how
it
can
be
expressed
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
Different
style
that
have
been
proposed
for
component
and
connector
view
that
can
be
used
to
design
the
architecture
of
the
proposed
system
What
is
architecture
Generally
speaking
architecture
of
a
system
provides
a
very
high
level
view
of
the
part
of
the
system
and
how
they
are
related
to
form
the
whole
system
That
is
architecture
partition
the
system
in
logical
part
such
that
each
part
can
be
comprehended
independently
and
then
describes
the
system
in
term
of
these
part
and
the
relationship
between
these
part
Any
complex
system
can
be
partitioned
in
many
different
way
each
pro-
viding
a
useful
view
and
each
having
different
type
of
logical
part
The
same
hold
true
for
a
software
system—there
is
no
unique
structure
of
the
system
that
can
be
described
by
it
architecture
there
are
many
possible
structure
Due
to
this
possibility
of
having
multiple
structure
one
of
the
most
widely
accepted
definition
of
software
architecture
is
that
the
software
architecture
of
a
system
is
the
structure
or
structure
of
the
system
which
comprise
software
el-
ements
the
externally
visible
property
of
those
element
and
the
relationship
among
them
6
This
definition
implies
that
for
element
in
an
architecture
we
are
only
interested
in
those
abstraction
that
specify
those
property
that
other
element
can
assume
to
exist
and
that
are
needed
to
specify
relationship
Details
on
how
these
property
are
supported
are
not
needed
for
architecture
This
is
an
important
capability
that
allows
architecture
description
to
repre-
sent
a
complex
system
in
a
succinct
form
that
is
easily
comprehended
An
architecture
description
of
a
system
will
therefore
describe
the
different
structure
of
the
system
The
next
natural
question
is
why
should
a
team
building
a
software
system
for
some
customer
be
interested
in
creating
and
documenting
the
structure
of
the
proposed
system
Some
of
the
important
us
that
software
architecture
description
play
are
6
23
54
Understanding
and
communication
An
architecture
description
is
primar-
ily
to
communicate
the
architecture
to
it
various
stakeholder
which
in-
clude
the
user
who
will
use
the
system
the
client
who
commissioned
the
system
the
builder
who
will
build
the
system
and
of
course
the
archi-
tects
Through
this
description
the
stakeholder
gain
an
understanding
of
some
macro
property
of
the
system
and
how
the
system
intends
to
ful-
fill
the
functional
and
quality
requirement
As
the
description
provides
a
common
language
between
stakeholder
it
also
becomes
the
vehicle
for
negotiation
and
agreement
among
the
stakeholder
who
may
have
conflict-
ing
goal
Reuse
The
software
engineering
world
ha
for
a
long
time
been
working
toward
a
discipline
where
software
can
be
assembled
from
part
that
are
developed
by
different
people
and
are
available
for
others
to
use
If
one
want
to
build
a
software
product
in
which
existing
component
may
be
reused
then
architecture
becomes
the
key
point
at
which
reuse
at
the
high-
est
level
is
decided
The
architecture
ha
to
be
chosen
in
a
manner
such
that
the
component
which
have
to
be
reused
can
fit
properly
and
together
with
other
component
that
may
be
developed
Architecture
also
facili-
tate
reuse
among
product
that
are
similar
and
building
product
family
such
that
the
common
part
of
these
different
but
similar
product
can
be
reused
Architecture
help
specify
what
is
fixed
and
what
is
variable
in
these
different
product
and
can
help
minimize
the
set
of
variable
element
such
that
different
product
can
share
software
part
to
the
maximum
Again
it
is
very
hard
to
achieve
this
type
of
reuse
at
a
detail
level
Construction
and
Evolution
As
architecture
partition
the
system
into
part
some
architecture-provided
partitioning
can
naturally
be
used
for
constructing
the
system
which
also
requires
that
the
system
be
broken
into
part
such
that
different
team
or
individual
can
separately
work
on
different
part
A
suitable
partitioning
in
the
architecture
can
provide
the
project
with
the
part
that
need
to
be
built
to
build
the
system
As
almost
by
definition
the
part
specified
in
an
architecture
are
relatively
indepen-
dent
the
dependence
between
part
coming
through
their
relationship
they
can
be
built
independently
Analysis
It
is
highly
desirable
if
some
important
property
about
the
be-
havior
of
the
system
can
be
determined
before
the
system
is
actually
built
This
will
allow
the
designer
to
consider
alternative
and
select
the
one
that
will
best
suit
the
need
Many
engineering
discipline
use
model
to
analyze
design
of
a
product
for
it
cost
reliability
performance
etc
Archi-
tecture
open
such
possibility
for
software
also
It
is
possible
though
the
method
are
not
fully
developed
or
standardized
yet
to
analyze
or
predict
the
property
of
the
system
being
built
from
it
architecture
For
exam-
ple
the
reliability
or
the
performance
of
the
system
can
be
analyzed
Such
an
analysis
can
help
determine
whether
the
system
will
meet
the
quality
and
performance
requirement
and
if
not
what
need
to
be
done
to
meet
the
requirement
For
example
while
building
a
website
for
shopping
it
is
possible
to
analyze
the
response
time
or
throughput
for
a
proposed
archi-
tecture
given
some
assumption
about
the
request
load
and
hardware
It
can
then
be
decided
whether
the
performance
is
satisfactory
or
not
and
if
not
what
new
capability
should
be
added
for
example
a
different
archi-
tecture
or
a
faster
server
for
the
back
end
to
improve
it
to
a
satisfactory
level
Not
all
of
these
us
may
be
significant
in
a
project
and
which
of
these
us
is
pertinent
to
a
project
depends
on
the
nature
of
the
project
In
some
project
communication
may
be
very
important
but
a
detailed
performance
analysis
may
be
unnecessary
because
the
system
is
too
small
or
is
meant
for
only
a
few
user
In
some
other
system
performance
analysis
may
be
the
primary
use
of
architecture
There
is
a
general
view
emerging
that
there
is
no
unique
architecture
of
a
system
The
definition
that
we
have
adopted
given
above
also
express
this
sentiment
Consequently
there
is
no
one
architecture
drawing
of
the
system
The
situation
is
similar
to
that
of
civil
construction
a
discipline
that
is
the
original
user
of
the
concept
of
architecture
and
from
where
the
concept
of
software
architecture
ha
been
borrowed
For
a
building
if
you
want
to
see
the
floor
plan
you
are
shown
one
set
of
drawing
If
you
are
an
electrical
engineer
and
want
to
see
how
the
electricity
distribution
ha
been
planned
you
will
be
shown
another
set
of
drawing
And
if
you
are
interested
in
safety
and
firefighting
another
set
of
drawing
is
used
These
drawing
are
not
independent
of
each
other—they
are
all
about
the
same
building
However
each
drawing
provides
a
different
view
of
the
building
a
view
that
focus
on
explaining
one
aspect
of
the
building
and
try
to
a
good
job
at
that
while
not
divulging
much
about
the
other
aspect
And
no
one
drawing
can
express
all
the
different
aspects—such
a
drawing
will
be
too
complex
to
be
of
any
use
Similar
is
the
situation
with
software
architecture
In
software
the
different
drawing
are
called
view
A
view
represents
the
system
a
composed
of
some
type
of
element
and
relationship
between
them
Which
element
are
used
by
a
view
depends
on
what
the
view
want
to
highlight
Different
view
expose
different
property
and
attribute
thereby
allowing
the
stakeholder
and
ana-
lysts
to
properly
evaluate
those
attribute
for
the
system
By
focusing
only
on
some
aspect
of
the
system
a
view
reduces
the
complexity
that
a
reader
ha
to
deal
with
at
a
time
thereby
aiding
system
understanding
and
analysis
A
view
describes
a
structure
of
the
system
We
will
use
these
two
concepts—
view
and
structures—interchangeably
We
will
also
use
the
term
architectural
view
to
refer
to
a
view
Many
type
of
view
have
been
proposed
Most
of
the
proposed
view
generally
belong
to
one
of
these
three
type
6
23
In
a
module
view
the
system
is
viewed
a
a
collection
of
code
unit
each
implementing
some
part
of
the
system
functionality
That
is
the
main
element
in
this
view
are
module
These
view
are
code-based
and
do
not
explicitly
rep-
resent
any
runtime
structure
of
the
system
Examples
of
module
are
package
a
class
a
procedure
a
method
a
collection
of
function
and
a
collection
of
class
The
relationship
between
these
module
are
also
code-based
and
de-
pend
on
how
code
of
a
module
interacts
with
another
module
Examples
of
relationship
in
this
view
are
is
a
part
of
i.e.
module
B
is
a
part
of
module
A
us
or
depends
on
a
module
A
us
service
of
module
B
to
perform
it
own
function
and
correctness
of
module
A
depends
on
correctness
of
module
B
and
generalization
or
specialization
a
module
B
is
a
generalization
of
a
module
A
In
a
component
and
connector
C
&
C
view
the
system
is
viewed
a
a
col-
lection
of
runtime
entity
called
component
That
is
a
component
is
a
unit
which
ha
an
identity
in
the
executing
system
Objects
not
class
a
collec-
tion
of
object
and
a
process
are
example
of
component
While
executing
component
need
to
interact
with
others
to
support
the
system
service
Con-
nectors
provide
mean
for
this
interaction
Examples
of
connector
are
pipe
and
socket
Shared
data
can
also
act
a
a
connector
If
the
component
use
some
middleware
to
communicate
and
coordinate
then
the
middleware
is
a
connector
Hence
the
primary
element
of
this
view
are
component
and
con-
nectors
An
allocation
view
focus
on
how
the
different
software
unit
are
allocated
to
resource
like
the
hardware
file
system
and
people
That
is
an
allocation
view
specifies
the
relationship
between
software
element
and
element
of
the
environment
in
which
the
software
system
is
executed
They
expose
structural
property
like
which
process
run
on
which
processor
and
how
the
system
file
are
organized
on
a
file
system
An
architecture
description
consists
of
view
of
different
type
with
each
view
exposing
some
structure
of
the
system
Module
view
show
how
the
soft-
ware
is
structured
a
a
set
of
implementation
unit
C
&
C
view
show
how
the
software
is
structured
a
interacting
runtime
element
and
allocation
view
show
how
software
relates
to
nonsoftware
structure
These
three
type
of
view
of
the
same
system
form
the
architecture
of
the
system
Note
that
the
different
view
are
not
unrelated
They
all
represent
the
same
system
Hence
there
are
relationship
between
element
in
one
view
and
ele-
ments
in
another
view
These
relationship
may
be
simple
or
may
be
complex
For
example
the
relationship
between
module
and
component
may
be
one
to
one
in
that
one
module
implement
one
component
On
the
other
hand
it
may
be
quite
complex
with
a
module
being
used
by
multiple
component
and
a
component
using
multiple
module
While
creating
the
different
view
the
designer
have
to
be
aware
of
this
relationship
The
next
question
is
what
are
the
standard
view
that
should
be
expressed
for
describing
the
architecture
of
a
system
To
answer
this
question
the
analogy
with
building
may
again
help
If
one
is
building
a
simple
small
house
then
perhaps
there
is
no
need
to
have
a
separate
view
describing
the
emergency
and
the
fire
system
Similarly
if
there
is
no
air
conditioning
in
the
building
there
need
not
be
any
view
for
that
On
the
other
hand
an
office
building
will
perhaps
require
both
of
these
view
in
addition
to
other
view
describing
plumbing
space
wiring
etc
However
despite
the
fact
that
there
are
multiple
drawing
showing
different
view
of
a
building
there
is
one
view
that
predominates
in
construction—that
of
physical
structure
This
view
form
the
basis
of
other
view
in
that
other
view
can
not
really
be
completed
unless
this
view
can
be
done
Other
view
may
or
may
not
be
needed
for
constructing
a
building
depending
on
the
nature
of
the
project
Hence
in
a
sense
the
view
giving
the
building
structure
may
be
considered
a
the
primary
view
in
that
it
is
almost
always
used
and
other
view
rely
on
this
view
substantially
The
view
also
capture
perhaps
the
most
important
property
to
be
analyzed
in
the
early
stage
namely
that
of
space
organization
The
situation
with
software
architecture
is
also
somewhat
similar
As
we
have
said
depending
on
what
property
are
of
interest
different
view
of
the
software
architecture
are
needed
However
of
these
view
the
C
&
C
view
ha
become
the
defacto
primary
view
one
which
is
almost
always
prepared
when
an
architecture
is
designed
some
definition
even
view
architecture
only
in
term
of
C
&
C
view
In
this
chapter
we
will
focus
primarily
on
the
C
&
C
view
A
note
about
relationship
between
architecture
and
design
is
in
order
As
partitioning
a
system
into
smaller
part
and
composing
the
system
from
these
part
is
also
a
goal
of
design
a
natural
question
is
what
is
the
difference
between
a
design
and
architecture
a
both
aim
to
achieve
similar
objective
and
seem
to
fundamentally
rely
on
the
divide
and
conquer
rule
First
it
should
be
clear
that
architecture
is
a
design
in
that
it
is
in
the
solution
domain
and
talk
about
the
structure
of
the
proposed
system
Furthermore
an
architecture
view
give
a
high-level
view
of
the
system
relying
on
abstraction
to
convey
the
meaning—
something
which
design
also
doe
So
architecture
is
design
We
can
view
architecture
a
a
very
high-level
design
focusing
only
on
main
component
and
the
architecture
activity
a
the
first
step
in
design
What
we
term
a
design
is
really
about
the
module
that
will
eventually
exist
a
code
That
is
they
are
a
more
concrete
representation
of
the
implementation
though
not
yet
an
implementation
Consequently
during
design
lower-level
issue
like
the
data
structure
file
and
source
of
data
have
to
be
addressed
while
such
issue
are
not
generally
significant
at
the
architecture
level
We
also
take
the
view
that
design
can
be
considered
a
providing
the
module
view
of
the
architecture
of
the
system
The
boundary
between
architecture
and
high-level
design
are
not
fully
clear
The
way
the
field
ha
evolved
we
can
say
that
the
line
between
architec-
ture
and
design
is
really
up
to
the
designer
or
the
architect
At
the
architecture
level
one
need
to
show
only
those
part
that
are
needed
to
perform
the
desired
evaluation
The
internal
structure
of
these
part
is
not
important
On
the
other
hand
during
design
designing
the
structure
of
the
part
that
can
lead
to
con-
structing
them
is
one
of
the
key
task
However
which
part
of
the
structure
should
be
examined
and
revealed
during
architecture
and
which
part
during
design
is
a
matter
of
choice
Generally
speaking
detail
that
are
not
needed
to
perform
the
type
of
analysis
we
wish
to
do
at
the
architecture
time
are
unnecessary
and
should
be
left
for
design
to
uncover
The
C
&
C
architecture
view
of
a
system
ha
two
main
elements—components
and
connector
Components
are
usually
computational
element
or
data
store
that
have
some
presence
during
the
system
execution
Connectors
define
the
mean
of
interaction
between
these
component
A
C
&
C
view
of
the
system
de-
fine
the
component
and
which
component
is
connected
to
which
and
through
what
connector
A
C
&
C
view
describes
a
runtime
structure
of
the
system—
what
component
exist
when
the
system
is
executing
and
how
they
interact
during
the
execution
The
C
&
C
structure
is
essentially
a
graph
with
compo-
nents
a
node
and
connector
a
edge
The
C
&
C
view
is
perhaps
the
most
common
view
of
architecture
and
most
box-and-line
drawing
representing
architecture
attempt
to
capture
this
view
Most
often
when
people
talk
about
the
architecture
they
refer
to
the
C
&
C
view
Most
architecture
description
language
also
focus
on
the
C
&
C
view
Components
are
generally
unit
of
computation
or
data
store
in
the
system
A
component
ha
a
name
which
is
generally
chosen
to
represent
the
role
of
the
component
or
the
function
it
performs
The
name
also
provides
a
unique
identity
to
the
component
which
is
necessary
for
referencing
detail
about
the
component
in
the
supporting
document
a
a
C
&
C
drawing
will
only
show
the
component
name
A
component
is
of
a
component
type
where
the
type
represents
a
generic
component
defining
the
general
computation
and
the
interface
a
component
of
that
type
must
have
Note
that
though
a
component
ha
a
type
in
the
C
&
C
architecture
view
we
have
component
i.e.
actual
instance
and
not
type
Examples
of
these
type
are
client
server
filter
etc
Different
domain
may
have
other
generic
type
like
controller
actuator
and
sensor
for
a
control
system
domain
In
a
diagram
representing
a
C
&
C
architecture
view
of
a
system
it
is
highly
desirable
to
have
a
different
representation
for
different
component
type
so
the
different
type
can
be
identified
visually
In
a
box-and-line
diagram
often
all
component
are
represented
a
rectangular
box
Such
an
approach
will
require
that
type
of
the
component
are
described
separately
and
the
reader
ha
to
read
the
description
to
figure
out
the
type
of
the
component
It
is
much
better
to
use
a
different
symbol/notation
for
each
different
component
type
Some
of
the
common
symbol
used
for
representing
commonly
found
component
type
are
shown
in
Figure
To
make
sure
that
the
meaning
of
the
different
symbol
are
clear
to
the
reader
it
is
desirable
to
have
a
key
of
the
different
symbol
to
describe
what
The
different
component
of
a
system
are
likely
to
interact
while
the
system
is
in
operation
to
provide
the
service
expected
of
the
system
After
all
component
exist
to
provide
part
of
the
service
and
feature
of
the
system
and
these
must
be
combined
to
deliver
the
overall
system
functionality
For
composing
a
system
from
it
component
information
about
the
interaction
between
component
is
necessary
Interaction
between
component
may
be
through
a
simple
mean
supported
by
the
underlying
process
execution
infrastructure
of
the
operating
system
For
example
a
component
may
interact
with
another
using
the
procedure
call
mechanism
a
connector
which
is
provided
by
the
runtime
environment
for
the
programming
language
However
the
interaction
may
involve
more
complex
mechanism
a
well
Examples
of
such
mechanism
are
remote
procedure
call
TCP/IP
port
and
a
protocol
like
HTTP
These
mechanism
require
a
fair
amount
of
underlying
runtime
infrastructure
a
well
a
special
programming
within
the
component
to
use
the
infrastructure
Consequently
it
is
extremely
important
to
identify
and
explicitly
represent
these
connector
Specification
of
connector
will
help
identify
the
suitable
infrastructure
needed
to
implement
an
architecture
a
well
a
clarify
the
programming
need
for
component
using
them
Note
that
connector
need
not
be
binary
and
a
connector
may
provide
an
n-way
communication
between
multiple
component
For
example
a
broadcast
bus
may
be
used
a
a
connector
which
allows
a
component
to
broadcast
it
message
to
all
the
other
component
Some
of
the
common
symbol
used
for
representing
commonly
found
connector
type
are
shown
in
Figure
A
connector
also
ha
a
name
that
should
describe
the
nature
of
interaction
the
connector
support
A
connector
also
ha
a
type
which
is
a
generic
de-
scription
of
the
interaction
specifying
property
like
whether
it
is
a
binary
or
n-way
type
of
interface
it
support
etc
If
a
protocol
is
used
by
a
connector
type
it
should
be
explicitly
stated
It
is
worth
pointing
out
that
the
implementation
of
a
connector
may
be
quite
complex
If
the
connector
is
provided
by
the
underlying
system
then
the
component
just
have
to
ensure
that
they
use
the
connector
a
per
their
specification
If
however
the
underlying
system
doe
not
provide
a
connector
used
in
an
architecture
then
a
mentioned
above
the
connector
will
have
to
be
implemented
a
part
of
the
project
to
build
the
system
That
is
during
the
development
not
only
will
the
component
need
to
be
developed
but
resource
will
have
to
be
assigned
to
also
develop
the
connector
This
situation
might
arise
for
a
specialized
system
that
requires
connector
that
are
specific
to
the
problem
domain
Generally
while
creating
an
architecture
it
is
wise
for
the
architect
to
use
the
connector
which
are
available
on
the
system
on
which
the
software
will
be
deployed
Suppose
we
have
to
design
and
build
a
simple
system
for
taking
an
on-line
survey
of
student
on
a
campus
There
is
a
set
of
multiple-choice
question
and
the
proposed
system
will
provide
the
survey
form
to
the
student
who
can
fill
and
submit
it
on-line
We
also
want
that
when
the
user
submits
the
form
he/she
is
also
shown
the
current
result
of
the
survey
that
is
what
percentage
of
student
so
far
have
filled
which
option
for
the
different
question
The
system
is
best
built
using
the
Web
this
is
the
likely
choice
of
any
developer
For
this
simple
system
a
traditional
3-tier
architecture
is
proposed
It
consists
of
a
client
which
will
display
the
form
that
the
student
can
complete
and
submit
and
will
also
display
the
result
The
second
component
is
the
server
which
process
the
data
submitted
by
the
student
and
save
it
on
the
database
which
is
the
third
component
The
server
also
query
the
database
to
get
the
outcome
of
the
survey
and
sends
the
result
in
proper
format
HTML
back
to
the
client
which
then
display
the
result
The
C
&
C
view
is
shown
in
Figure
Note
that
the
client
server
and
database
are
all
different
type
of
compo-
nents
and
hence
are
shown
using
different
symbol
Note
also
that
the
connec-
tor
between
the
component
are
also
of
different
type
The
diagram
make
the
different
type
clear
making
the
diagram
stand
alone
and
easy
to
comprehend
Note
that
at
the
architecture
level
a
host
of
detail
are
not
discussed
How
is
the
URL
of
the
survey
set
What
are
the
module
that
go
into
building
these
component
and
what
language
are
they
written
in
Questions
like
these
are
not
the
issue
at
this
level
Note
also
that
the
connector
between
the
client
and
the
server
explicitly
say
that
http
is
to
be
used
And
the
diagram
also
say
that
it
is
a
Web
client
This
implies
that
it
is
assumed
that
there
will
be
a
Web
browser
running
on
the
machine
from
which
the
student
will
take
the
survey
Having
the
http
a
the
connector
also
implies
that
there
is
a
proper
http
server
running
and
that
the
server
of
this
system
will
be
suitably
attached
to
it
to
allow
access
by
client
In
other
word
the
entire
infrastructure
of
browser
and
the
http
server
for
the
purpose
of
this
application
mainly
provides
the
connector
between
the
client
and
the
server
and
a
virtual
machine
to
run
the
client
of
the
application
There
are
some
implication
of
choice
of
this
connector
on
the
component
The
client
will
have
to
be
written
in
a
manner
that
it
can
send
the
request
using
http
this
will
imply
using
some
type
of
scripting
language
or
HTML
form
Similarly
it
also
implies
that
the
server
ha
to
take
it
request
from
the
http
server
in
the
format
specified
by
the
http
protocol
Furthermore
the
server
ha
to
send
it
result
back
to
the
client
in
the
HTML
format
These
are
all
constraint
on
implementing
this
architecture
Hence
when
discussing
it
and
finally
accepting
it
the
implication
for
the
infrastructure
a
well
a
the
implementation
should
be
fully
understood
and
action
should
be
taken
to
make
sure
that
these
assumption
are
valid
The
above
architecture
ha
no
security
and
a
student
can
take
the
survey
a
many
time
a
he
wish
Furthermore
even
a
nonstudent
can
take
the
survey
Now
suppose
the
Dean
of
Students
want
that
this
system
be
open
only
to
registered
student
and
that
each
student
is
allowed
to
take
the
survey
at
most
once
To
identify
the
student
it
wa
explained
that
each
student
ha
an
account
and
their
account
information
is
available
from
the
main
proxy
server
of
the
institute
Now
the
architecture
will
have
to
be
quite
different
The
proposed
architec-
ture
now
ha
a
separate
login
form
for
the
user
and
a
separate
server
component
which
doe
the
validation
For
validation
it
go
to
the
proxy
for
checking
if
the
login
and
password
provided
are
valid
If
so
the
server
return
a
cookie
to
the
client
which
store
it
a
per
the
cookie
protocol
When
the
student
completes
the
survey
form
the
cookie
information
validates
the
user
and
the
server
check
if
this
student
ha
already
completed
the
survey
The
architecture
for
this
system
is
shown
in
Figure
Note
that
even
though
we
are
saying
that
the
connection
between
the
client
and
the
server
is
that
of
http
it
is
somewhat
different
from
the
connection
in
the
earlier
architecture
In
the
first
architecture
plain
http
is
sufficient
In
this
one
a
cooky
are
also
needed
the
connector
is
really
http
+
cooky
So
if
the
user
disables
cooky
the
required
connector
is
not
available
and
this
architecture
will
not
work
Now
suppose
we
want
the
system
to
be
extended
in
a
different
way
It
wa
found
that
the
database
server
is
somewhat
unreliable
and
is
frequently
down
It
wa
also
felt
that
when
the
student
is
given
the
result
of
the
survey
when
he
submits
the
form
a
somewhat
outdated
result
is
acceptable
a
the
result
are
really
statistical
data
and
a
little
inaccuracy
will
not
matter
We
assume
that
the
survey
result
can
be
outdated
by
about
even
if
it
doe
not
include
data
of
it
is
OK
What
the
Dean
wanted
wa
to
make
the
system
more
reliable
and
provide
some
facility
for
completing
the
survey
even
when
the
database
is
down
To
make
the
system
more
reliable
the
following
strategy
wa
proposed
When
the
student
submits
the
survey
the
server
interacts
with
the
database
a
before
The
result
of
the
survey
however
are
also
stored
in
the
cache
by
the
server
If
the
database
is
down
or
unavailable
the
survey
data
is
stored
locally
in
a
cache
component
and
the
result
saved
in
the
cache
component
is
used
to
provide
the
result
to
the
student
This
can
be
done
for
up
to
after
which
the
survey
can
not
be
completed
So
now
we
have
another
component
in
the
server
called
the
cache
manager
And
there
is
a
connection
between
the
server
and
this
new
component
of
the
call/return
type
This
architecture
is
shown
in
Figure
It
should
be
clear
that
by
using
the
cache
the
availability
of
the
system
is
improved
The
cache
will
also
have
an
impact
on
performance
These
exten-
sion
show
how
architecture
affect
both
availability
and
performance
and
how
properly
selecting
or
tuning
the
architecture
can
help
meet
the
quality
goal
or
just
improve
the
quality
of
the
system
Of
course
detail-level
decision
like
how
a
particular
module
is
implemented
also
have
implication
on
performance
but
they
are
quite
distinct
and
orthogonal
to
the
architecture-level
decision
We
will
later
do
a
formal
evaluation
of
these
different
architecture
to
see
the
impact
of
architectural
decision
on
some
quality
attribute
It
should
be
clear
that
different
system
will
have
different
architecture
There
are
some
general
architecture
that
have
been
observed
in
many
system
and
that
seem
to
represent
general
structure
that
are
useful
for
architecture
of
a
class
of
problem
These
are
called
architectural
style
A
style
defines
a
family
of
architecture
that
satisfy
the
constraint
of
that
style
6
23
76
In
this
section
we
discus
some
common
style
for
the
C
&
C
view
which
can
be
useful
for
a
large
set
of
problem
23
76
These
style
can
provide
idea
for
creating
an
architecture
view
for
the
problem
at
hand
Styles
can
also
be
combined
to
form
richer
view
Pipe-and-filter
style
of
architecture
is
well
suited
for
system
that
primarily
do
data
transformation
whereby
some
input
data
is
received
and
the
goal
of
the
system
is
to
produce
some
output
data
by
suitably
transforming
the
input
data
A
system
using
pipe-and-filter
architecture
achieves
the
desired
transformation
by
applying
a
network
of
smaller
transformation
and
composing
them
in
a
manner
such
that
together
the
overall
desired
transformation
is
achieved
The
pipe-and-filter
style
ha
only
one
component
type
called
the
filter
It
also
ha
only
one
connector
type
called
the
pipe
A
filter
performs
a
data
transformation
and
sends
the
transformed
data
to
other
filter
for
further
processing
using
the
pipe
connector
In
other
word
a
filter
receives
the
data
it
need
from
some
defined
input
pipe
performs
the
data
transformation
and
then
sends
the
output
data
to
other
filter
on
the
defined
output
pipe
A
filter
may
have
more
than
one
input
and
more
than
one
output
Filters
can
be
independent
and
asynchronous
entity
and
a
they
are
concerned
only
with
the
data
arriving
on
the
pipe
a
filter
need
not
know
the
identity
of
the
filter
that
sent
the
input
data
or
the
identity
of
the
filter
that
will
consume
the
data
they
produce
The
pipe
connector
is
a
unidirectional
channel
which
conveys
stream
of
data
received
on
one
end
to
the
other
end
A
pipe
doe
not
change
the
data
in
any
manner
but
merely
transport
it
to
the
filter
on
the
receiver
end
in
the
order
in
which
the
data
element
are
received
As
filter
can
be
asynchronous
and
should
work
without
the
knowledge
of
the
identity
of
the
producer
or
the
consumer
buffering
and
synchronization
need
to
ensure
smooth
functioning
of
the
producer-consumer
relationship
embodied
in
connecting
two
filter
by
a
pipe
is
ensured
by
the
pipe
The
filter
merely
consume
and
produce
data
There
are
some
constraint
that
this
style
imposes
First
a
mentioned
above
the
filter
should
work
without
knowing
the
identity
of
the
consumer
or
the
producer
they
should
only
require
the
data
element
they
need
Second
a
pipe
which
is
a
two-way
connector
must
connect
an
output
port
of
a
filter
to
an
input
port
of
another
filter
A
pure
pipe-and-filter
structure
will
also
generally
have
a
constraint
that
a
filter
ha
independent
thread
of
control
which
process
the
data
a
it
come
Implementing
this
will
require
suitable
underlying
infrastructure
to
support
a
pipe
mechanism
which
buffer
the
data
and
doe
the
synchronization
needed
for
example
blocking
the
producer
when
the
buffer
is
full
and
blocking
the
consumer
filter
when
the
buffer
is
empty
For
using
this
pipe
the
filter
builder
must
be
fully
aware
of
the
property
of
the
pipe
particularly
with
regard
to
buffering
and
synchronization
input
and
output
mechanism
and
the
symbol
for
end
of
data
However
there
could
be
situation
in
which
the
constraint
that
a
filter
process
the
data
a
it
come
may
not
be
required
Without
this
constraint
pipe-and-filter
style
view
may
have
filter
that
produce
the
data
completely
before
passing
it
on
or
which
start
their
processing
only
after
complete
input
is
available
In
such
a
system
the
filter
can
not
operate
concurrently
and
the
system
is
like
a
batch-processing
system
However
it
can
considerably
simplify
the
pipe
and
easier
mechanism
can
be
used
for
supporting
them
Let
’
s
consider
an
example
of
a
system
needed
to
count
the
frequency
of
different
word
in
a
file
An
architecture
using
the
pipes-and-filter
style
for
a
system
to
achieve
this
is
given
in
Figure
This
architecture
proposes
that
the
input
data
be
first
split
into
a
sequence
of
word
by
a
component
Sequencer
This
sequence
of
word
is
then
sorted
by
the
component
Sorting
which
pass
the
output
of
sorted
word
to
another
filter
Counting
that
count
the
number
of
occurrence
of
the
different
word
This
structure
of
sorting
the
word
first
ha
been
chosen
a
it
will
make
the
task
of
determining
the
frequency
more
efficient
even
though
it
involves
a
sort
operation
It
should
be
clear
that
this
proposed
architecture
can
implement
the
desired
functionality
Later
in
the
chapter
we
will
further
discus
some
implementation
issue
related
to
this
architecture
As
can
be
seen
from
this
example
pipe-and-filter
architectural
style
is
well
suited
for
data
processing
and
transformation
Consequently
it
is
useful
in
text
processing
application
Signal
processing
application
also
find
it
useful
a
such
application
typically
perform
encoding
error
correction
and
other
transformation
on
the
data
The
pipe-and-filter
style
due
to
the
constraint
allows
a
system
’
s
overall
transformation
to
be
composed
of
smaller
transformation
Or
viewing
it
in
another
manner
it
allows
a
desired
transformation
to
be
factored
into
smaller
transformation
and
then
filter
built
for
the
smaller
transformation
That
is
it
allows
the
technique
of
functional
composition
and
decomposition
to
be
utilized
something
that
is
mathematically
appealing
In
this
style
there
are
two
type
of
components—data
repository
and
data
accessors
Components
of
data
repository
type
are
where
the
system
store
shared
data—these
could
be
file
system
or
database
These
component
pro-
vide
a
reliable
and
permanent
storage
take
care
of
any
synchronization
need
for
concurrent
access
and
provide
data
access
support
Components
of
data
accessors
type
access
data
from
the
repository
perform
computation
on
the
data
obtained
and
if
they
want
to
share
the
result
with
other
component
put
the
result
back
in
the
depository
In
other
word
the
accessors
are
computa-
tional
element
that
receive
their
data
from
the
repository
and
save
their
data
in
the
repository
a
well
These
component
do
not
directly
communicate
with
each
other—the
data
repository
component
are
the
mean
of
communication
and
data
transfer
between
them
There
are
two
variation
of
this
style
possible
In
the
blackboard
style
if
some
data
is
posted
on
the
data
repository
all
the
accessor
component
that
need
to
know
about
it
are
informed
In
other
word
the
shared
data
source
is
an
active
agent
a
well
which
either
informs
the
component
about
the
arrival
of
interesting
data
or
start
the
execution
of
the
component
that
need
to
act
upon
this
new
data
In
database
this
form
of
style
is
often
supported
through
trigger
The
other
is
the
repository
style
in
which
the
data
repository
is
just
a
passive
repository
which
provides
permanent
storage
and
related
control
for
data
accessing
The
component
access
the
repository
a
and
when
they
want
As
can
be
imagined
many
database
application
use
this
architectural
style
Databases
though
originally
more
like
repository
now
act
both
a
repository
a
well
a
blackboard
a
they
provide
trigger
and
can
act
a
efficient
data
storage
a
well
Many
Web
system
frequently
follow
this
style
at
the
back
end—in
response
to
user
request
different
script
data
accessors
access
and
update
some
shared
data
Many
programming
environment
are
also
organized
this
way
the
common
representation
of
the
program
artifact
is
stored
in
the
repository
and
the
different
tool
access
it
to
perform
the
desired
translation
or
to
obtain
the
desired
information
Some
year
back
there
wa
a
standard
defined
for
the
common
repository
to
facilitate
integration
of
tool
As
an
example
of
a
system
using
this
style
of
architecture
let
u
consider
a
student
registration
system
in
a
university
The
system
clearly
ha
a
central
repository
which
contains
information
about
course
student
prerequisite
etc
It
ha
an
Administrator
component
that
set
up
the
repository
right
to
different
people
etc
The
Registration
component
allows
student
to
reg-
ister
and
update
the
information
for
student
and
course
The
Approvals
component
is
for
granting
approval
for
those
course
that
require
instructor
’
s
consent
The
Reports
component
produce
the
report
regarding
the
student
registered
in
different
course
at
the
end
of
the
registration
The
component
Course
Feedback
is
used
for
taking
feedback
from
student
at
the
end
of
the
course
This
architecture
is
shown
in
Figure
Note
that
the
different
computation
component
do
not
need
to
communi-
cate
with
each
other
and
do
not
even
need
to
know
about
each
other
’
s
presence
For
example
if
later
it
is
decided
that
the
scheduling
of
course
can
be
auto-
mated
based
on
data
on
registration
and
other
information
about
classroom
etc
then
another
component
called
Scheduling
can
be
simply
added
No
ex-
isting
computation
component
need
to
change
or
be
informed
about
the
new
component
being
added
This
example
is
based
on
a
system
that
is
actually
used
in
the
author
’
s
university
There
is
really
only
one
connector
type
in
this
style—read/write
Note
how-
ever
that
this
general
connector
style
may
take
more
precise
form
in
particular
architecture
For
example
though
a
database
can
be
viewed
a
supporting
read
and
update
for
a
program
interacting
with
it
the
database
system
may
provide
transaction
service
a
well
Connectors
using
this
transaction
service
allow
complete
transaction
which
may
involve
multiple
read
and
writes
and
preserve
atomicity
to
be
performed
by
an
application
Note
also
that
a
in
many
other
case
the
connector
involve
a
consider-
able
amount
of
underlying
infrastructure
For
example
read
and
writes
to
a
file
system
involve
a
fair
amount
of
file
system
software
involving
issue
like
directory
buffering
locking
and
synchronization
Similarly
a
considerable
amount
of
software
go
into
database
to
support
the
type
of
connection
it
provides
for
query
update
and
transaction
We
will
see
another
use
of
this
style
later
when
we
discus
the
case
study
Another
very
common
style
used
to
build
system
today
is
the
client-server
style
Client-server
computing
is
one
of
the
basic
paradigm
of
distributed
com-
puting
and
this
architecture
style
is
built
upon
this
paradigm
In
this
style
there
are
two
component
types—clients
and
server
A
con-
straint
of
this
style
is
that
a
client
can
only
communicate
with
the
server
and
can
not
communicate
with
other
client
The
communication
between
a
client
component
and
a
server
component
is
initiated
by
the
client
when
the
client
sends
a
request
for
some
service
that
the
server
support
The
server
receives
the
request
at
it
defined
port
performs
the
service
and
then
return
the
result
of
the
computation
to
the
client
who
requested
the
service
There
is
one
connector
type
in
this
style—the
request/reply
type
A
con-
nector
connects
a
client
to
a
server
This
type
of
connector
is
asymmetric—the
client
end
of
the
connector
can
only
make
request
and
receive
the
reply
while
the
server
end
can
only
send
reply
in
response
to
the
request
it
get
through
this
connector
The
communication
is
frequently
synchronous—the
client
wait
for
the
server
to
return
the
result
before
proceeding
That
is
the
client
is
blocked
at
the
request
until
it
get
the
reply
A
general
form
of
this
style
is
an
n-tier
structure
In
this
style
a
client
sends
a
request
to
a
server
but
the
server
in
order
to
service
the
request
sends
some
request
to
another
server
That
is
the
server
also
act
a
a
client
for
the
next
tier
This
hierarchy
can
continue
for
some
level
providing
an
n-tier
system
A
common
example
of
this
is
the
3-tier
architecture
In
this
style
the
client
that
make
request
and
receive
the
final
result
reside
in
the
client
tier
The
middle
tier
called
the
business
tier
contains
the
component
that
process
the
data
submitted
by
the
client
and
applies
the
necessary
business
rule
The
third
tier
is
the
database
tier
in
which
the
data
resides
The
business
tier
interacts
with
the
database
tier
for
all
it
data
need
Most
often
in
a
client-server
architecture
the
client
and
the
server
com-
ponent
reside
on
different
machine
Even
if
they
reside
on
the
same
machine
they
are
designed
in
a
manner
such
that
they
can
exist
on
different
machine
Hence
the
connector
between
the
client
and
the
server
is
expected
to
support
the
request/result
type
of
connection
across
different
machine
Consequently
these
connector
are
internally
quite
complex
and
involve
a
fair
amount
of
net-
working
to
support
Many
of
the
client-server
system
today
use
TCP
port
for
their
connector
The
Web
us
the
HTTP
for
supporting
this
connector
Note
that
there
is
a
distinction
between
a
layered
architecture
and
a
tiered
architecture
The
tiered
style
is
a
component
and
connector
architecture
view
in
which
each
tier
is
a
component
and
these
component
communicate
with
the
adjacent
one
through
a
defined
protocol
A
layered
architecture
is
a
module
view
providing
how
module
are
organized
and
used
In
the
layered
organiza-
tion
module
are
organized
in
layer
with
module
in
a
layer
allowed
to
invoke
service
only
of
the
module
in
the
layer
below
Hence
layered
and
tiered
rep-
resent
two
different
view
We
can
have
an
n-tiered
architecture
in
which
some
tier
s
have
a
layered
architecture
For
example
in
a
client-server
architecture
the
server
might
have
a
layered
architecture
that
is
module
that
compose
the
server
are
organized
in
the
layered
style
Publish-Subscribe
Style
In
this
style
there
are
two
type
of
component
One
type
of
component
subscribes
to
a
set
of
defined
event
Other
type
of
compo-
nents
generate
or
publish
event
In
response
to
these
event
the
component
that
have
published
their
intent
to
process
the
event
are
invoked
This
type
of
style
is
most
natural
in
user
interface
framework
where
many
event
are
defined
like
mouse
click
and
component
are
assigned
to
these
event
When
that
event
occurs
the
associated
component
is
executed
As
is
the
case
with
most
connector
it
is
the
task
of
the
runtime
infrastructure
to
ensure
that
this
type
of
connector
i.e.
publish-subscribe
is
supported
This
style
can
be
seen
a
a
special
case
of
the
blackboard
style
except
that
the
repository
aspect
is
not
being
used
Peer-to-peer
style
or
object-oriented
style
If
we
take
a
client-server
style
and
generalize
each
component
to
be
a
client
a
well
a
a
server
then
we
have
this
style
In
this
style
component
are
peer
and
any
component
can
request
a
service
from
any
other
component
The
object-oriented
computation
model
represents
this
style
well
If
we
view
component
a
object
and
connector
a
method
invocation
then
we
have
this
style
This
model
is
the
one
that
is
primarily
supported
through
middleware
connector
like
CORBA
or
.NET
Communicating
process
style
Perhaps
the
oldest
model
of
distributed
com-
puting
is
that
of
communicating
process
This
style
try
to
capture
this
model
of
computing
The
component
in
this
model
are
process
or
thread
which
communicate
with
each
other
either
with
message
passing
or
through
shared
memory
This
style
is
used
in
some
form
in
many
complex
system
which
use
multiple
thread
or
process
So
far
we
have
focused
on
representing
view
through
diagram
While
de-
signing
diagram
are
indeed
a
good
way
to
explore
option
and
encourage
discussion
and
brainstorming
between
the
architect
But
when
the
designing
is
over
the
architecture
ha
to
be
properly
communicated
to
all
stakehold-
er
for
negotiation
and
agreement
This
requires
that
architecture
be
precisely
documented
with
enough
information
to
perform
the
type
of
analysis
the
dif-
ferent
stakeholder
wish
to
make
to
satisfy
themselves
that
their
concern
have
been
adequately
addressed
Without
a
properly
documented
description
of
the
architecture
it
is
not
possible
to
have
a
clear
common
understanding
Hence
properly
documenting
an
architecture
is
a
important
a
creating
one
In
this
section
we
discus
what
an
architecture
document
should
contain
Our
discus-
sion
is
based
on
the
recommendation
in
6
23
54
Just
like
different
project
require
different
view
different
project
will
need
different
level
of
detail
in
their
architecture
documentation
In
general
however
a
document
describing
the
architecture
should
contain
the
following
We
know
that
an
architecture
for
a
system
is
driven
by
the
system
objective
and
the
need
of
the
stakeholder
Hence
the
first
aspect
that
an
architecture
document
should
contain
is
identification
of
stakeholder
and
their
concern
This
portion
should
give
an
overview
of
the
system
the
different
stakeholder
and
the
system
property
for
which
the
architecture
will
be
evaluated
A
con-
text
diagram
that
establishes
the
scope
of
the
system
it
boundary
the
key
actor
that
interact
with
the
system
and
source
and
sink
of
data
can
also
be
very
useful
A
context
diagram
is
frequently
represented
by
showing
the
system
in
the
center
and
showing
it
connection
with
people
and
system
including
source
and
sink
of
data
With
the
context
defined
the
document
can
proceed
with
describing
the
different
structure
or
view
As
stated
before
multiple
view
of
different
type
may
be
needed
and
which
view
are
chosen
depends
on
the
need
of
the
project
and
it
stakeholder
The
description
of
view
in
the
architecture
documentation
will
almost
always
contain
a
pictorial
representation
of
the
view
which
is
often
the
primary
presentation
of
the
view
As
discussed
earlier
in
any
view
diagram
it
is
desirable
to
have
different
symbol
for
different
element
type
and
provide
a
key
for
the
different
type
such
that
the
type
of
the
different
component
represented
using
the
symbol
is
clear
to
a
reader
It
is
of
course
highly
desirable
to
keep
the
diagram
simple
and
uncluttered
If
necessary
to
keep
the
complexity
of
the
view
manageable
a
hierarchical
approach
can
be
followed
to
make
the
main
view
simple
and
provide
further
detail
a
structure
of
the
element
However
a
pictorial
representation
is
not
a
complete
description
of
the
view
It
give
an
intuitive
idea
of
the
design
but
is
not
sufficient
for
providing
the
detail
For
example
the
purpose
and
functionality
of
a
module
or
a
component
is
indicated
only
by
it
name
which
is
not
sufficient
Hence
supporting
doc-
umentation
is
needed
for
the
view
diagram
This
supporting
documentation
should
have
some
or
all
of
the
following
Element
Catalog
Provides
more
information
about
the
element
shown
in
the
primary
representation
Besides
describing
the
purpose
of
the
element
it
should
also
describe
the
element
’
interface
remember
that
all
element
have
interface
through
which
they
interact
with
other
element
All
the
different
interface
provided
by
the
element
should
be
specified
Interfaces
should
have
unique
identity
and
the
specification
should
give
both
syntactic
and
semantic
information
Syntactic
information
is
often
in
term
of
signa-
tures
which
describe
all
the
data
item
involved
in
the
interface
and
their
type
Semantic
information
must
describe
what
the
interface
doe
The
de-
scription
should
also
clearly
state
the
error
condition
that
the
interface
can
return
Architecture
Rationale
Though
a
view
specifies
the
element
and
the
rela-
tionship
between
them
it
doe
not
provide
any
insight
into
why
the
architect
chose
the
particular
structure
Architecture
rationale
give
the
reason
for
se-
lecting
the
different
element
and
composing
them
in
the
way
it
wa
done
This
section
may
also
provide
some
discussion
on
the
alternative
that
were
considered
and
why
they
were
rejected
This
discussion
besides
explaining
the
choice
is
also
useful
later
when
an
analyst
making
a
change
wonder
why
the
architecture
should
not
be
changed
in
some
manner
that
might
make
the
change
easy
Behavior
A
view
give
the
structural
information
It
doe
not
represent
the
actual
behavior
or
execution
Consequently
in
a
structure
all
possible
in-
teractions
during
an
execution
are
shown
Sometimes
it
is
necessary
to
get
some
idea
of
the
actual
behavior
of
the
system
in
some
scenario
Such
a
description
is
useful
for
arguing
about
property
like
deadlock
Behavior
de-
scription
can
be
provided
to
help
aid
understanding
of
the
system
execution
Often
diagram
like
collaboration
diagram
or
sequence
diagram
we
will
discus
these
further
in
Chapter
are
used
Other
Information
This
may
include
a
description
of
all
those
decision
that
have
not
been
taken
during
architecture
creation
but
have
been
deliberately
left
for
the
future
such
a
the
choice
of
a
server
or
protocol
If
this
is
done
then
it
must
be
specified
a
fixing
these
will
have
impact
on
the
architecture
We
know
that
the
different
view
are
related
In
what
we
have
discussed
so
far
the
view
have
been
described
independently
The
architecture
document
therefore
besides
describing
the
view
should
also
describe
the
relationship
between
the
different
view
This
is
the
primary
purpose
of
the
across
view
documentation
Essentially
this
documentation
describes
the
relationship
be-
tween
element
of
the
different
view
for
example
how
module
in
a
module
view
relate
to
component
in
a
component
view
or
how
component
in
a
C
&
C
view
relate
to
process
in
a
process
view
This
part
of
the
document
can
also
describe
the
rationale
of
the
overall
architecture
why
the
selected
view
were
chosen
and
any
other
information
that
cut
across
view
However
often
the
relationship
between
the
different
view
is
straightfor-
ward
or
very
strong
In
such
situation
the
different
structure
may
look
very
similar
and
describing
the
view
separately
can
lead
to
a
repetition
In
such
situation
for
practical
reason
it
is
better
to
combine
different
view
into
one
Besides
eliminating
the
duplication
this
approach
can
also
help
clearly
show
the
strong
relationship
between
the
two
view
and
in
the
process
also
reduce
the
across
view
documentation
Combined
view
are
also
useful
for
some
analysis
which
requires
multiple
view
for
example
performance
analy-
si
which
frequently
requires
both
the
C
&
C
view
a
well
a
the
allocation
view
So
sometimes
it
may
be
desirable
to
show
some
combined
view
Combining
of
view
however
should
be
done
only
if
the
relationship
be-
tween
the
view
is
strong
and
straightforward
Otherwise
putting
multiple
view
in
one
diagram
will
clutter
the
view
and
make
it
confusing
The
objec-
tive
of
reveal
multiple
view
in
one
is
not
merely
to
reduce
the
number
of
view
but
is
to
be
done
primarily
to
aid
understanding
and
showing
the
relationship
An
example
of
combining
is
when
there
are
multiple
module
in
the
module
view
that
form
the
different
layer
in
the
layer
view
In
such
a
situation
it
is
probably
more
natural
to
show
one
view
consisting
of
the
layer
and
overlay-
ing
the
module
structure
on
the
layer
that
is
showing
the
module
structure
within
the
layer
Many
layered
system
’
architecture
actually
use
this
ap-
proach
In
such
a
situation
it
is
best
to
show
them
together
creating
a
hybrid
style
in
which
both
a
module
view
and
a
C
&
C
view
are
captured
Overall
if
the
mapping
can
be
shown
easily
and
in
a
simple
manner
then
different
view
should
be
combined
for
the
sake
of
simplicity
and
compactness
If
however
the
relationship
between
the
different
view
is
complex
for
example
a
many-
to-many
relationship
between
element
of
the
different
view
then
it
is
best
to
keep
them
separate
and
specify
the
relationship
separately
The
general
structure
discussed
here
can
provide
a
guide
for
organizing
the
architecture
document
However
the
main
purpose
of
the
document
is
to
clearly
communicate
the
architecture
to
the
stakeholder
such
that
the
desired
analysis
can
be
done
And
if
some
of
these
section
are
redundant
for
that
purpose
they
may
not
be
included
Similarly
if
more
information
need
to
be
provided
then
it
should
be
done
Finally
a
word
on
the
language
chosen
for
describing
different
part
of
the
architecture
Here
the
choice
varies
from
the
formal
architecture
description
language
ADLs
to
informal
notation
Many
people
now
use
UML
to
repre-
sent
the
architecture
which
allows
various
possibility
to
show
the
primary
description
of
the
view
and
also
allows
annotation
capability
for
supporting
document
We
believe
that
any
method
can
be
used
a
long
a
the
objective
is
met
To
allow
flexibility
we
suggest
using
a
problem-specific
notation
but
following
the
guideline
for
good
view
representation
and
using
a
combination
of
header
definition
and
text
for
the
supporting
documentation
Architecture
of
a
software
system
impact
some
of
the
key
nonfunctional
qual-
ity
attribute
like
modifiability
performance
reliability
portability
etc
The
architecture
ha
a
much
more
significant
impact
on
some
of
these
property
than
the
design
and
coding
choice
That
is
even
though
choice
of
algorithm
data
structure
etc.
are
important
for
many
of
these
attribute
often
they
have
le
of
an
impact
than
the
architectural
choice
Clearly
then
evaluating
a
proposed
architecture
for
these
property
can
have
a
beneficial
impact
on
the
project—any
architectural
change
that
are
required
to
meet
the
desired
goal
for
these
attribute
can
be
done
during
the
architecture
design
itself
There
are
many
nonfunctional
quality
attribute
Not
all
of
them
are
af-
fected
by
architecture
significantly
Some
of
the
attribute
on
which
architecture
ha
a
significant
impact
are
performance
reliability
and
availability
security
some
aspect
of
it
modifiability
reusability
and
portability
Attributes
like
usability
are
only
mildly
affected
by
architecture
How
should
a
proposed
architecture
be
evaluated
for
these
attribute
For
some
attribute
like
performance
and
reliability
it
is
possible
to
build
formal
model
using
technique
like
queuing
network
and
use
them
for
assessing
the
value
of
the
attribute
However
these
model
require
information
beyond
the
architecture
description
generally
in
form
of
execution
time
and
reliability
of
each
component
Another
approach
is
procedural—a
sequence
of
step
is
followed
to
subjec-
tively
evaluate
the
impact
of
the
architecture
on
some
of
the
attribute
One
such
informal
analysis
approach
that
is
often
used
is
a
follows
First
identify
the
attribute
of
interest
for
which
an
architecture
should
be
evaluated
These
attribute
are
usually
determined
from
stakeholder
’
interests—the
attribute
the
different
stakeholder
are
most
interested
in
These
attribute
are
then
listed
in
a
table
Then
for
each
attribute
an
experience-based
subjective
or
quantitative
analysis
is
done
to
ass
the
level
supported
by
the
architecture
The
analysis
might
mention
the
level
for
each
attribute
e.g.
good
average
poor
or
might
simply
mention
whether
it
is
satisfactory
or
not
Based
on
the
outcome
of
this
analysis
the
architecture
is
either
accepted
or
rejected
If
rejected
it
may
be
enhanced
to
improve
the
performance
for
the
attribute
for
which
the
proposed
architecture
wa
unsatisfactory
Many
technique
have
been
proposed
for
evaluation
and
a
survey
of
them
is
given
in
29
Architecture
of
a
software
system
provides
a
very
high-level
view
of
the
system
in
term
of
part
of
the
system
and
how
they
are
related
to
form
the
whole
system
Depending
on
how
the
system
is
partitioned
we
get
a
different
architectural
view
of
the
system
Consequently
the
architecture
of
a
software
system
is
defined
a
the
structure
of
the
system
which
comprise
software
element
their
externally
visible
property
and
relationship
among
them
Architecture
facilitates
development
of
a
high-quality
system
It
also
allows
analysis
of
many
of
the
system
property
like
performance
that
depend
mostly
on
architecture
to
be
done
early
in
the
software
life
cycle
There
are
three
main
architectural
view
of
a
system—module
component
and
connector
and
allocation
In
a
module
view
the
system
is
viewed
a
a
structure
of
programming
module
like
package
class
function
etc
In
a
component
and
connector
C
&
C
view
the
system
is
a
collection
of
runtime
entity
called
component
which
interact
with
each
other
through
the
connector
An
allocation
view
describes
how
the
different
software
unit
are
allocated
to
hardware
resource
in
the
system
C
&
C
view
is
most
common
and
is
often
the
centerpiece
of
the
architecture
description
This
view
is
often
described
by
block
diagram
specifying
the
different
component
and
the
different
connector
between
the
component
There
are
some
common
style
for
a
C
&
C
view
which
have
been
found
useful
for
creating
this
architecture
view
for
a
system
These
include
pipe
and
filter
shared
data
client-server
publish-subscribe
peer
to
peer
and
communicat-
ing
process
style
Each
of
these
style
describes
the
type
of
component
and
connector
that
exist
and
the
constraint
on
how
they
are
used
The
pipe
and
filter
ha
one
type
of
component
filter
and
one
type
of
connector
pipe
and
component
can
be
connected
through
the
pipe
The
client-server
style
ha
two
type
of
component
client
and
server
and
there
is
one
connector
request/reply
A
client
can
only
communicate
with
the
server
and
an
interaction
is
initiated
by
a
client
In
shared
data
style
the
two
component
type
are
repository
and
data
accessors
Data
accessors
read/write
the
repository
and
share
information
among
themselves
through
the
repository
The
architecture
form
the
foundation
for
the
system
and
rest
of
the
design
and
development
activity
and
need
to
be
properly
documented
A
proper
architecture
document
should
describe
the
context
in
which
the
architecture
wa
designed
the
different
architectural
view
that
were
created
and
how
the
different
view
relate
to
each
other
The
architecture
description
should
specify
the
different
type
of
element
and
their
external
behavior
and
the
architecture
rationale
Architecture
should
be
evaluated
to
see
that
it
satisfies
the
requirement
A
common
approach
is
to
do
a
subjective
evaluation
with
respect
to
the
desired
property
Why
is
architecture
not
just
one
structure
consisting
of
different
part
and
their
relationship
What
are
the
different
architectural
style
for
the
component
and
connector
struc-
ture
of
a
system
Consider
an
interactive
website
which
provides
many
different
feature
to
perform
various
task
Show
that
the
architecture
for
this
can
be
represented
a
a
shared-
data
style
a
well
a
client-server
style
Which
one
will
you
prefer
and
why
What
should
an
architecture
document
for
a
system
contain
Suggest
how
you
will
evaluate
a
proposed
architecture
from
a
modifiability
per-
spective
The
design
activity
begin
when
the
requirement
document
for
the
software
to
be
developed
is
available
and
the
architecture
ha
been
designed
During
design
we
further
refine
the
architecture
Generally
design
focus
on
what
we
have
called
the
module
view
in
Chapter
5
That
is
during
design
we
determine
what
module
the
system
should
have
and
which
have
to
be
developed
Often
the
module
view
may
effectively
be
a
module
structure
of
each
component
in
the
architecture
In
that
case
the
design
exercise
determines
the
module
structure
of
the
component
However
this
simple
mapping
of
component
and
module
may
not
always
hold
In
that
case
we
have
to
ensure
that
the
module
view
created
in
design
is
consistent
with
the
architecture
The
design
of
a
system
is
essentially
a
blueprint
or
a
plan
for
a
solution
for
the
system
Here
we
consider
a
system
to
be
a
set
of
module
with
clearly
defined
behavior
which
interact
with
each
other
in
a
defined
manner
to
produce
some
behavior
or
service
for
it
environment
The
design
process
for
software
system
often
ha
two
level
At
the
first
level
the
focus
is
on
deciding
which
module
are
needed
for
the
system
the
specification
of
these
module
and
how
the
module
should
be
interconnected
This
is
what
may
be
called
the
module
design
or
the
high-level
design
In
the
second
level
the
internal
design
of
the
module
or
how
the
specification
of
the
module
can
be
satisfied
is
decided
This
design
level
is
often
called
detailed
design
or
logic
design
Detailed
design
essentially
expands
the
system
design
to
contain
a
more
detailed
description
of
the
processing
logic
and
data
structure
so
that
the
design
is
sufficiently
complete
for
coding
A
design
methodology
is
a
systematic
approach
to
creating
a
design
by
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
applying
of
a
set
of
technique
and
guideline
Most
design
methodology
focus
on
the
module
design
but
do
not
reduce
the
design
activity
to
a
sequence
of
step
that
can
be
blindly
followed
by
the
designer
The
key
design
concept
of
modularity
cohesion
coupling
and
open-closed
principle
The
structure
chart
notation
for
expressing
the
structure
of
a
function-
oriented
system
The
structured
design
methodology
for
designing
the
structure
chart
of
the
system
being
developed
Some
key
concept
related
to
object-orientation
and
the
unified
modeling
language
UML
that
can
be
used
to
express
an
object-oriented
design
A
methodology
of
creating
the
object-oriented
design
for
a
system
utilizing
UML
Some
metric
for
quantifying
the
complexity
of
a
design
The
design
of
a
system
is
correct
if
a
system
built
precisely
according
to
the
design
satisfies
the
requirement
of
that
system
Clearly
the
goal
during
the
design
phase
is
to
produce
correct
design
However
correctness
is
not
the
sole
criterion
during
the
design
phase
a
there
can
be
many
correct
design
The
goal
of
the
design
process
is
not
simply
to
produce
a
design
for
the
system
Instead
the
goal
is
to
find
the
best
possible
design
within
the
limitation
im-
posed
by
the
requirement
and
the
physical
and
social
environment
in
which
the
system
will
operate
To
evaluate
a
design
we
have
to
specify
some
evaluation
criterion
We
will
focus
on
modularity
of
a
system
which
is
decided
mostly
by
design
a
the
main
criterion
for
evaluation
A
system
is
considered
modular
if
it
consists
of
discrete
module
so
that
each
module
can
be
implemented
separately
and
a
change
to
one
module
ha
minimal
impact
on
other
module
Modularity
is
clearly
a
desirable
property
Modularity
help
in
system
debugging—isolating
the
system
problem
to
a
module
is
easier
if
the
system
is
modular
in
system
repair—changing
a
part
of
the
system
is
easy
a
it
affect
few
other
part
and
in
system
building—a
modular
system
can
be
easily
built
by
putting
it
module
together
A
software
system
can
not
be
made
modular
by
simply
chopping
it
into
a
set
of
module
For
modularity
each
module
need
to
support
a
well-defined
abstraction
and
have
a
clear
interface
through
which
it
can
interact
with
other
module
To
produce
modular
design
some
criterion
must
be
used
to
select
module
so
that
the
module
support
well-defined
abstraction
and
are
solv-
able
and
modifiable
separately
Coupling
and
cohesion
are
two
modularization
criterion
which
are
often
used
together
We
also
discus
the
open-closed
princi-
ple
which
is
another
criterion
for
modularity
Two
module
are
considered
independent
if
one
can
function
completely
with-
out
the
presence
of
the
other
Obviously
if
two
module
are
independent
they
are
solvable
and
modifiable
separately
However
all
the
module
in
a
system
can
not
be
independent
of
each
other
a
they
must
interact
so
that
together
they
produce
the
desired
external
behavior
of
the
system
The
more
connection
between
module
the
more
dependent
they
are
in
the
sense
that
more
knowl-
edge
about
one
module
is
required
to
understand
or
solve
the
other
module
Hence
the
fewer
and
simpler
the
connection
between
module
the
easier
it
is
to
understand
one
without
understanding
the
other
The
notion
of
coupling
79
88
attempt
to
capture
this
concept
of
how
strongly
different
module
are
interconnected
Coupling
between
module
is
the
strength
of
interconnection
between
mod-
ules
or
a
measure
of
interdependence
among
module
In
general
the
more
we
must
know
about
module
A
in
order
to
understand
module
B
the
more
closely
connected
A
is
to
B
Highly
coupled
module
are
joined
by
strong
interconnection
while
loosely
coupled
module
have
weak
interconnection
Independent
module
have
no
interconnection
To
solve
and
modify
a
module
separately
we
would
like
the
module
to
be
loosely
coupled
with
other
mod-
ules
The
choice
of
module
decides
the
coupling
between
module
Because
the
module
of
the
software
system
are
created
during
system
design
the
cou-
pling
between
module
is
largely
decided
during
system
design
and
can
not
be
reduced
during
implementation
Coupling
increase
with
the
complexity
and
obscurity
of
the
interface
be-
tween
module
To
keep
coupling
low
we
would
like
to
minimize
the
number
of
interface
per
module
and
the
complexity
of
each
interface
An
interface
of
a
module
is
used
to
pas
information
to
and
from
other
module
Coupling
is
reduced
if
only
the
defined
entry
interface
of
a
module
is
used
by
other
module
for
example
passing
information
to
and
from
a
module
exclusively
through
pa-
rameters
Coupling
would
increase
if
a
module
is
used
by
other
module
via
an
indirect
and
obscure
interface
like
directly
using
the
internals
of
a
module
or
using
shared
variable
Complexity
of
the
interface
is
another
factor
affecting
coupling
The
more
complex
each
interface
is
the
higher
will
be
the
degree
of
coupling
For
example
complexity
of
the
entry
interface
of
a
procedure
depends
on
the
number
of
item
being
passed
a
parameter
and
on
the
complexity
of
the
item
Some
level
of
complexity
of
interface
is
required
to
support
the
communication
needed
be-
tween
module
However
often
more
than
this
minimum
is
used
For
example
if
a
field
of
a
record
is
needed
by
a
procedure
often
the
entire
record
is
passed
rather
than
just
passing
that
field
of
the
record
By
passing
the
record
we
are
increasing
the
coupling
unnecessarily
Essentially
we
should
keep
the
interface
of
a
module
a
simple
and
small
a
possible
The
type
of
information
flow
along
the
interface
is
the
third
major
factor
affecting
coupling
There
are
two
kind
of
information
that
can
flow
along
an
interface
data
or
control
Passing
or
receiving
control
information
mean
that
the
action
of
the
module
will
depend
on
this
control
information
which
make
it
more
difficult
to
understand
the
module
and
provide
it
abstraction
Transfer
of
data
information
mean
that
a
module
pass
a
input
some
data
to
another
module
and
get
in
return
some
data
a
output
This
allows
a
module
to
be
treated
a
a
simple
input-output
function
that
performs
some
transformation
on
the
input
data
to
produce
the
output
data
In
general
interface
with
only
data
communication
result
in
the
lowest
degree
of
coupling
followed
by
interface
that
only
transfer
control
data
Coupling
is
considered
highest
if
the
data
is
hybrid
that
is
some
data
item
and
some
control
item
are
passed
between
module
The
effect
of
these
three
factor
on
coupling
is
summarized
in
Table
79
Type
of
Type
of
Complexity
Connection
Communication
The
manifestation
of
coupling
in
OO
system
is
somewhat
different
a
object
are
semantically
richer
than
function
In
OO
system
three
different
type
of
coupling
exist
between
module
30
Interaction
coupling
occurs
due
to
method
of
a
class
invoking
method
of
other
class
In
many
way
this
situation
is
similar
to
a
function
calling
an-
other
function
and
hence
this
coupling
is
similar
to
coupling
between
functional
module
discussed
above
Like
with
function
the
worst
form
of
coupling
here
is
if
method
directly
access
internal
part
of
other
method
Coupling
is
lowest
if
method
communicate
directly
through
parameter
Within
this
category
a
discussed
above
coupling
is
lower
if
only
data
is
passed
but
is
higher
if
control
information
is
passed
since
the
invoked
method
impact
the
execution
sequence
in
the
calling
method
Also
coupling
is
higher
if
the
amount
of
data
being
passed
is
increased
So
if
whole
data
structure
are
passed
when
only
some
part
are
needed
coupling
is
being
unnecessarily
increased
Similarly
if
an
object
is
passed
to
a
method
when
only
some
of
it
component
object
are
used
within
the
method
coupling
increase
unnecessarily
The
least
coupling
situation
therefore
is
when
communication
is
with
parameter
only
with
only
necessary
variable
being
passed
and
these
parameter
only
pas
data
Component
coupling
refers
to
the
interaction
between
two
class
where
a
class
ha
variable
of
the
other
class
Three
clear
situation
exist
a
to
how
this
can
happen
A
class
C
can
be
component
coupled
with
another
class
C1
if
C
ha
an
instance
variable
of
type
C1
or
C
ha
a
method
whose
parameter
is
of
type
C1
or
if
C
ha
a
method
which
ha
a
local
variable
of
type
C1
Note
that
when
C
is
component
coupled
with
C1
it
ha
the
potential
of
being
component
coupled
with
all
subclass
of
C.
It
should
be
clear
that
whenever
there
is
component
coupling
there
is
likely
to
be
interaction
coupling
Component
coupling
is
considered
to
be
weakest
i.e
most
desired
if
in
a
class
C
the
variable
of
class
C
or
are
some
attribute
of
C.
If
interaction
is
through
local
variable
then
this
interaction
is
not
visible
from
outside
and
therefore
increase
coupling
Inheritance
coupling
is
due
to
the
inheritance
relationship
between
class
Two
class
are
considered
inheritance
coupled
if
one
class
is
a
direct
or
indirect
subclass
of
the
other
If
inheritance
add
coupling
one
can
ask
the
question
why
not
do
away
with
inheritance
altogether
The
reason
is
that
inheritance
may
reduce
the
overall
coupling
in
the
system
Let
u
consider
two
situation
If
a
class
A
is
coupled
with
another
class
B
and
if
B
is
a
hierarchy
with
B
B
then
if
a
method
m
is
factored
out
of
B
the
coupling
drop
a
A
is
now
only
coupled
with
B
whereas
earlier
it
wa
coupled
with
both
B2
Similarly
if
B
is
a
class
hierarchy
which
support
specialization-generalization
relationship
then
if
new
subclass
are
added
to
B
no
change
need
to
be
made
to
a
class
A
which
call
method
in
B
That
is
for
changing
B
’
s
hierarchy
A
need
not
be
disturbed
Without
this
hierarchy
change
in
B
would
most
likely
result
in
change
in
A
Within
inheritance
coupling
there
are
some
situation
that
are
worse
than
others
The
worst
form
is
when
a
subclass
B
or
deletes
the
method
This
situation
can
easily
lead
to
a
runtime
error
besides
violating
the
true
spirit
of
the
is-a
relationship
If
the
signature
is
preserved
but
the
implementation
of
a
method
is
changed
that
also
violates
the
is-a
relationship
though
may
not
lead
to
a
runtime
error
and
should
be
avoided
The
least
coupling
scenario
is
when
a
subclass
only
add
instance
variable
and
method
but
doe
not
modify
any
inherited
one
We
have
seen
that
coupling
is
reduced
when
the
relationship
among
element
in
different
module
are
minimized
That
is
coupling
is
reduced
when
element
in
different
module
have
little
or
no
bond
between
them
Another
way
of
achieving
this
effect
is
to
strengthen
the
bond
between
element
of
the
same
module
by
maximizing
the
relationship
between
element
of
the
same
module
Cohesion
is
the
concept
that
try
to
capture
this
intramodule
79
88
With
cohesion
we
are
interested
in
determining
how
closely
the
element
of
a
module
are
related
to
each
other
Cohesion
of
a
module
represents
how
tightly
bound
the
internal
element
of
the
module
are
to
one
another
Cohesion
of
a
module
give
the
designer
an
idea
about
whether
the
different
element
of
a
module
belong
together
in
the
same
module
Cohesion
and
coupling
are
clearly
related
Usually
the
greater
the
cohesion
of
each
module
in
the
system
the
lower
the
coupling
between
module
is
This
correlation
is
not
perfect
but
it
ha
been
observed
in
practice
There
are
several
level
of
cohesion
Coincidental
is
the
lowest
level
and
functional
is
the
highest
Coincidental
co-
hesion
occurs
when
there
is
no
meaningful
relationship
among
the
element
of
a
module
Coincidental
cohesion
can
occur
if
an
existing
program
is
mod-
ularized
by
chopping
it
into
piece
and
making
different
piece
module
If
a
module
is
created
to
save
duplicate
code
by
combining
some
part
of
code
that
occurs
at
many
different
place
that
module
is
likely
to
have
coincidental
cohesion
A
module
ha
logical
cohesion
if
there
is
some
logical
relationship
between
the
element
of
a
module
and
the
element
perform
function
that
fall
in
the
same
logical
class
A
typical
example
of
this
kind
of
cohesion
is
a
module
that
performs
all
the
input
or
all
the
output
In
such
a
situation
if
we
want
to
input
or
output
a
particular
record
we
have
to
somehow
convey
this
to
the
module
Often
this
will
be
done
by
passing
some
kind
of
special
status
flag
which
will
be
used
to
determine
what
statement
to
execute
in
the
module
Besides
resulting
in
hybrid
information
flow
between
module
which
is
generally
the
worst
form
of
coupling
between
module
such
a
module
will
usually
have
tricky
and
clumsy
code
In
general
logically
cohesive
module
should
be
avoided
if
possible
Temporal
cohesion
is
the
same
a
logical
cohesion
except
that
the
element
are
also
related
in
time
and
are
executed
together
Modules
that
perform
activ-
ities
like
initialization
cleanup
and
termination
are
usually
temporally
bound
Even
though
the
element
in
a
temporally
bound
module
are
logically
related
temporal
cohesion
is
higher
than
logical
cohesion
because
the
element
are
all
executed
together
This
avoids
the
problem
of
passing
the
flag
and
the
code
is
usually
simpler
A
procedurally
cohesive
module
contains
element
that
belong
to
a
common
procedural
unit
For
example
a
loop
or
a
sequence
of
decision
statement
in
a
module
may
be
combined
to
form
a
separate
module
Procedurally
cohesive
module
often
occur
when
modular
structure
is
determined
from
some
form
of
flowchart
Procedural
cohesion
often
cut
across
functional
line
A
module
with
only
procedural
cohesion
may
contain
only
part
of
a
complete
function
or
part
of
several
function
A
module
with
communicational
cohesion
ha
element
that
are
related
by
a
reference
to
the
same
input
or
output
data
That
is
in
a
communicationally
bound
module
the
element
are
together
because
they
operate
on
the
same
input
or
output
data
An
example
of
this
could
be
a
module
to
print
and
punch
record.
Communicationally
cohesive
module
may
perform
more
than
one
function
However
communicational
cohesion
is
sufficiently
high
a
to
be
generally
acceptable
if
alternative
structure
with
higher
cohesion
can
not
be
easily
identified
When
the
element
are
together
in
a
module
because
the
output
of
one
form
the
input
to
another
we
get
sequential
cohesion
If
we
have
a
sequence
of
element
in
which
the
output
of
one
form
the
input
to
another
sequential
cohesion
doe
not
provide
any
guideline
on
how
to
combine
them
into
module
Functional
cohesion
is
the
strongest
cohesion
In
a
functionally
bound
mod-
ule
all
the
element
of
the
module
are
related
to
performing
a
single
function
By
function
we
do
not
mean
simply
mathematical
function
module
accom-
plishing
a
single
goal
are
also
included
Functions
like
compute
square
root
and
sort
the
array
are
clear
example
of
functionally
cohesive
module
How
doe
one
determine
the
cohesion
level
of
a
module
There
is
no
math-
ematical
formula
that
can
be
used
We
have
to
use
our
judgment
for
this
A
useful
technique
for
determining
if
a
module
ha
functional
cohesion
is
to
write
a
sentence
that
describes
fully
and
accurately
the
function
or
purpose
of
the
module
Modules
with
functional
cohesion
can
always
be
described
by
a
simple
sentence
If
we
can
not
describe
it
using
a
simple
sentence
the
module
is
not
likely
to
have
functional
cohesion
Cohesion
in
object-oriented
system
ha
three
aspect
30
Method
cohesion
is
the
same
a
cohesion
in
functional
module
It
focus
on
why
the
different
code
element
of
a
method
are
together
within
the
method
The
highest
form
of
cohesion
is
if
each
method
implement
a
clearly
defined
function
and
all
statement
in
the
method
contribute
to
implementing
this
function
Class
cohesion
focus
on
why
different
attribute
and
method
are
together
in
this
class
The
goal
is
to
have
a
class
that
implement
a
single
concept
or
abstraction
with
all
element
contributing
toward
supporting
this
concept
In
general
whenever
there
are
multiple
concept
encapsulated
within
a
class
the
cohesion
of
the
class
is
not
a
high
a
it
could
be
and
a
designer
should
try
to
change
the
design
to
have
each
class
encapsulate
a
single
concept
One
symptom
of
the
situation
where
a
class
ha
multiple
abstraction
is
that
the
set
of
method
can
be
partitioned
into
two
or
more
group
each
accessing
a
distinct
subset
of
the
attribute
That
is
the
set
of
method
and
attribute
can
be
partitioned
into
separate
group
each
encapsulating
a
different
concept
Clearly
in
such
a
situation
by
having
separate
class
encapsulating
separate
concept
we
can
have
module
with
improved
cohesion
In
many
situation
even
though
two
or
more
concept
may
be
encapsu-
lated
within
a
class
there
are
some
method
that
access
attribute
of
both
the
encapsulated
concept
This
happens
when
the
class
represents
different
entity
which
have
a
relationship
between
them
For
cohesion
it
is
best
to
represent
them
a
two
separate
class
with
relationship
among
them
That
is
we
should
have
multiple
class
with
some
method
in
these
class
accessing
object
of
the
other
class
In
a
way
this
improvement
in
cohesion
result
in
an
increased
coupling
However
for
modifiability
and
understandability
it
is
better
if
each
class
encapsulates
a
single
concept
Inheritance
cohesion
focus
on
the
reason
why
class
are
together
in
a
hierarchy
The
two
main
reason
for
inheritance
are
to
model
generalization-
specialization
relationship
and
for
code
reuse
Cohesion
is
considered
high
if
the
hierarchy
support
generalization-specialization
of
some
concept
which
is
likely
to
naturally
lead
to
reuse
of
some
code
It
is
considered
lower
if
the
hierarchy
is
primarily
for
sharing
code
with
weak
conceptual
relationship
between
superclass
and
subclass
In
other
word
it
is
desired
that
in
an
OO
system
the
class
hierarchy
should
be
such
that
they
support
clearly
identified
generalization-specialization
relationship
This
is
a
design
concept
which
came
into
existence
more
in
the
OO
context
Like
with
cohesion
and
coupling
the
basic
goal
here
is
again
to
promote
build-
ing
of
system
that
are
easily
modifiable
a
modification
and
change
happen
frequently
and
a
design
that
can
not
easily
accommodate
change
will
result
in
system
that
will
die
fast
and
will
not
be
able
to
easily
adapt
to
the
changing
world
The
basic
principle
a
stated
by
Bertrand
Meyer
is
Software
entity
should
be
open
for
extension
but
closed
for
modification
66
A
module
being
open
for
extension
mean
that
it
behavior
can
be
extended
to
accommodate
new
demand
placed
on
this
module
due
to
change
in
requirement
and
sys-
tem
functionality
The
module
being
closed
for
modification
mean
that
the
existing
source
code
of
the
module
is
not
changed
when
making
enhancement
Then
how
doe
one
make
enhancement
to
a
module
without
changing
the
existing
source
code
This
principle
restricts
the
change
to
module
to
extension
only
i.e
it
allows
addition
of
code
but
disallows
changing
of
existing
code
If
this
can
be
done
clearly
the
value
is
tremendous
Code
change
involve
heavy
risk
and
to
ensure
that
a
change
ha
not
broken
thing
that
were
working
often
requires
a
lot
of
regression
testing
This
risk
can
be
minimized
if
no
change
are
made
to
existing
code
But
if
change
are
not
made
how
will
enhancement
be
made
This
principle
say
that
enhancement
should
be
made
by
adding
new
code
rather
than
altering
old
code
There
is
another
side
benefit
of
this
Programmers
typically
prefer
writing
new
code
rather
than
modifying
old
code
But
the
reality
is
that
system
that
are
being
built
today
are
being
built
on
top
of
existing
software
If
this
principle
is
satisfied
then
we
can
expand
existing
system
by
mostly
adding
new
code
to
old
system
and
minimizing
the
need
for
changing
code
This
principle
can
be
satisfied
in
OO
design
by
properly
using
inheritance
and
polymorphism
Inheritance
allows
creating
new
class
that
will
extend
the
behavior
of
existing
class
without
changing
the
original
class
And
it
is
this
property
that
can
be
used
to
support
this
principle
As
an
example
consider
an
application
in
which
a
client
object
of
type
Client
interacts
with
a
printer
object
of
class
Printer1
and
invokes
the
necessary
method
for
completing
it
printing
need
The
class
diagram
for
this
will
be
a
shown
in
Figure
In
this
design
the
client
directly
call
the
method
on
the
printer
object
for
printing
something
Now
suppose
the
system
ha
to
be
enhanced
to
allow
another
printer
to
be
used
by
the
client
Under
this
design
to
implement
this
change
a
new
class
Printer
This
design
doe
not
support
the
open-closed
principle
a
the
Client
class
is
not
closed
against
change
The
design
for
this
system
however
can
be
done
in
another
manner
that
support
the
open-closed
principle
In
this
design
instead
of
directly
imple-
menting
the
Printer
we
create
an
abstract
class
Printer
that
defines
the
interface
of
a
printer
and
specifies
all
the
method
a
printer
object
should
sup-
port
Printer
In
this
design
when
Printer
it
is
added
a
another
subclass
of
type
Printer
The
client
doe
not
need
to
be
aware
of
this
subtype
a
it
interacts
with
ob-
jects
of
type
Printer
That
is
the
client
only
deal
with
a
generic
Printer
and
it
interaction
is
the
same
whether
the
object
is
actually
of
type
Printer2
The
class
diagram
for
this
is
shown
in
Figure
It
is
this
inheritance
property
of
OO
that
is
leveraged
to
support
the
open-
closed
principle
The
basic
idea
is
to
have
a
class
encapsulate
the
abstraction
of
some
concept
If
this
abstraction
is
to
be
extended
the
extension
is
done
by
creating
new
subclass
of
the
abstraction
thereby
keeping
all
the
existing
code
unchanged
If
inheritance
hierarchy
are
built
in
this
manner
they
are
said
to
satisfy
the
Liskov
Substitution
Principle
65
According
to
this
principle
if
a
program
is
using
object
o
base
class
C
that
program
should
remain
unchanged
if
o1
where
C.
If
this
principle
is
satisfied
for
class
hierarchy
and
hierarchy
are
used
prop-
erly
then
the
open-closed
principle
can
be
supported
It
should
also
be
noted
that
recommendation
for
both
inheritance
coupling
and
inheritance
cohesion
support
that
this
principle
be
followed
in
class
hierarchy
Creating
the
software
system
design
is
the
major
concern
of
the
design
phase
Many
design
technique
have
been
proposed
over
the
year
to
provide
some
discipline
in
handling
the
complexity
of
designing
large
system
The
aim
of
design
methodology
is
not
to
reduce
the
process
of
design
to
a
sequence
of
mechanical
step
but
to
provide
guideline
to
aid
the
designer
during
the
design
process
We
discus
the
structured
design
methodology
79
88
for
developing
function-oriented
system
design
The
methodology
employ
the
structure
chart
notation
for
creating
the
design
So
before
we
discus
the
methodology
we
describe
this
notation
Graphical
design
notation
are
frequently
used
during
the
design
process
to
represent
design
or
design
decision
so
the
design
can
be
communicated
to
stakeholder
in
a
succinct
manner
and
evaluated
For
a
function-oriented
design
the
design
can
be
represented
graphically
by
structure
chart
The
structure
of
a
program
is
made
up
of
the
module
of
that
program
together
with
the
interconnection
between
module
Every
computer
program
ha
a
structure
and
given
a
program
it
structure
can
be
determined
The
structure
chart
of
a
program
is
a
graphic
representation
of
it
structure
In
a
structure
chart
a
module
is
represented
by
a
box
with
the
module
name
written
in
the
box
An
arrow
from
module
A
to
module
B
represents
that
module
A
invokes
module
B
B
is
called
the
subordinate
of
A
and
A
is
called
the
superordinate
of
B
The
arrow
is
labeled
by
the
parameter
received
by
B
a
input
and
the
parameter
returned
by
B
a
output
with
the
direction
of
flow
of
the
input
and
output
parameter
represented
by
small
arrow
The
parameter
can
be
shown
to
be
data
unfilled
circle
at
the
tail
of
the
label
or
control
filled
circle
at
the
tail
As
an
example
consider
the
structure
of
the
following
program
whose
structure
is
shown
in
Figure
readnums
a
&
N
sort
a
N
scanf
&
n
sum
=
add_n
a
n
printf
sum
if
a
i
>
a
t
switch
a
i
a
t
/
Add
the
first
n
number
of
a
/
add_n
a
n
In
general
procedural
information
is
not
represented
in
a
structure
chart
and
the
focus
is
on
representing
the
hierarchy
of
module
However
there
are
situation
where
the
designer
may
wish
to
communicate
certain
procedural
information
explicitly
like
major
loop
and
decision
Such
information
can
also
be
represented
in
a
structure
chart
For
example
let
u
consider
a
situation
where
module
A
ha
subordinate
B
C
and
D
and
A
repeatedly
call
the
module
C
and
D.
This
can
be
represented
by
a
looping
arrow
around
the
arrow
joining
the
subordinate
C
and
D
to
A
a
shown
in
Figure
All
the
subordinate
module
activated
within
a
common
loop
are
enclosed
in
the
same
looping
arrow
Major
decision
can
be
represented
similarly
For
example
if
the
invocation
of
module
C
and
D
in
module
A
depends
on
the
outcome
of
some
decision
that
is
represented
by
a
small
diamond
in
the
box
for
A
with
the
arrow
joining
C
and
D
coming
out
of
this
diamond
a
shown
in
Figure
Modules
in
a
system
can
be
categorized
into
few
class
There
are
some
module
that
obtain
information
from
their
subordinate
and
then
pas
it
to
their
superordinate
This
kind
of
module
is
an
input
module
Similarly
there
are
output
module
which
take
information
from
their
superordinate
and
pas
it
on
to
it
subordinate
As
the
name
suggest
the
input
and
output
mod-
ules
are
typically
used
for
input
and
output
of
data
from
and
to
the
environ-
ment
The
input
module
get
the
data
from
the
source
and
get
it
ready
to
be
processed
and
the
output
module
take
the
output
produced
and
prepare
it
for
proper
presentation
to
the
environment
Then
there
are
module
that
exist
solely
for
the
sake
of
transforming
data
into
some
other
form
Such
a
module
is
called
a
transform
module
Most
of
the
computational
module
typically
fall
in
this
category
Finally
there
are
module
whose
primary
concern
is
managing
the
flow
of
data
to
and
from
different
subordinate
Such
module
are
called
coordinate
module
A
module
can
perform
function
of
more
than
one
type
of
module
A
structure
chart
is
a
nice
representation
for
a
design
that
us
functional
abstraction
It
show
the
module
and
their
call
hierarchy
the
interface
be-
tween
the
module
and
what
information
pass
between
module
So
for
a
software
system
once
it
structure
is
decided
the
module
and
their
interface
and
dependency
get
fixed
The
objective
of
the
structured
design
methodology
is
to
control
the
eventual
structure
of
the
system
by
fixing
the
structure
dur-
ing
design
The
aim
is
to
design
a
system
so
that
program
implementing
the
design
would
have
a
hierarchical
structure
with
functionally
cohesive
module
and
a
few
interconnection
between
module
a
possible
No
design
methodology
reduces
design
to
a
series
of
step
that
can
be
mechan-
ically
executed
All
design
methodology
are
at
best
a
set
of
guideline
that
if
applied
will
most
likely
produce
a
design
that
is
modular
and
simple
The
basic
principle
behind
the
structured
design
methodology
a
with
most
other
methodology
is
problem
partitioning
Structured
design
methodology
partition
the
system
at
the
very
top
level
into
various
subsystem
one
for
managing
each
major
input
one
for
managing
each
major
output
and
one
for
each
major
transformation
The
module
performing
the
transformation
deal
with
data
at
an
abstract
level
and
hence
can
focus
on
the
conceptual
problem
of
how
to
perform
the
transformation
without
bothering
with
how
to
obtain
clean
input
or
how
to
present
the
output
The
rationale
behind
this
partitioning
is
that
in
many
system
particularly
data
processing
system
a
good
part
of
the
system
code
deal
with
managing
the
input
and
output
The
module
dealing
with
input
have
to
deal
with
issue
of
screen
reading
data
format
error
exception
completeness
of
information
structure
of
the
information
etc
Similarly
the
module
dealing
with
output
have
to
prepare
the
output
in
presentation
format
make
chart
produce
report
etc
Hence
for
many
system
it
is
indeed
the
case
that
a
good
part
of
the
software
ha
to
deal
with
input
and
output
The
actual
transfor-
mation
in
the
system
is
frequently
not
very
complex—it
is
dealing
with
data
and
getting
it
in
proper
form
for
performing
the
transformation
or
producing
the
output
in
the
desired
form
that
requires
considerable
processing
This
partitioning
is
at
the
heart
of
the
structured
design
methodology
There
are
four
major
step
in
the
methodology
Factoring
of
input
output
and
transform
branch
We
will
now
discus
each
of
these
step
in
more
detail
Restate
the
Problem
a
a
Data
Flow
Diagram
To
use
this
methodology
the
first
step
is
to
construct
the
data
flow
diagram
for
the
problem
We
studied
data
flow
diagram
in
Chapter
3
However
there
is
a
fundamental
difference
between
the
DFDs
drawn
during
requirement
analysis
and
those
drawn
during
structured
design
In
the
requirement
analysis
a
DFD
is
drawn
to
model
the
problem
domain
The
analyst
ha
little
control
over
the
problem
and
hence
his
task
is
to
extract
from
the
problem
all
the
information
and
then
represent
it
a
a
DFD
During
design
activity
we
are
no
longer
modeling
the
problem
domain
but
are
dealing
with
the
solution
domain
and
developing
a
model
for
the
eventual
system
That
is
the
DFD
during
design
represents
how
the
data
will
flow
in
the
system
when
it
is
built
In
this
modeling
the
major
transforms
or
function
in
the
software
are
decided
and
the
DFD
show
the
major
transforms
that
the
software
will
have
and
how
the
data
will
flow
through
different
transforms
A
DFD
of
an
ATM
is
shown
in
Figure
There
are
two
major
stream
of
input
data
in
this
diagram
The
first
is
the
account
number
and
the
code
and
the
second
is
the
amount
to
be
debited
Notice
the
use
of
at
different
place
in
the
DFD
For
example
the
trans-
form
validate
which
verifies
if
the
account
number
and
code
are
valid
need
not
only
the
account
number
and
code
but
also
information
from
the
sys-
tem
database
to
do
the
validation
And
the
transform
debit
account
ha
two
output
one
used
for
recording
the
transaction
and
the
other
to
update
the
account
Identify
the
Most
Abstract
Input
and
Output
Data
Elements
Most
system
have
some
basic
transformation
that
perform
the
required
operation
How-
ever
in
most
case
the
transformation
can
not
be
easily
applied
to
the
actual
physical
input
and
produce
the
desired
physical
output
Instead
the
input
is
first
converted
into
a
form
on
which
the
transformation
can
be
applied
with
ease
Similarly
the
main
transformation
module
often
produce
output
that
have
to
be
converted
into
the
desired
physical
output
The
goal
of
this
second
step
is
to
separate
the
transforms
in
the
data
flow
diagram
that
convert
the
input
or
output
to
the
desired
format
from
the
one
that
perform
the
actual
transformation
For
this
separation
once
the
data
flow
diagram
is
ready
the
next
step
is
to
identify
the
highest
abstract
level
of
input
and
output
The
most
abstract
input
data
element
are
those
data
element
in
the
data
flow
diagram
that
are
farthest
removed
from
the
physical
input
but
can
still
be
considered
input
to
the
system
The
most
abstract
input
data
element
often
have
little
resem-
blance
to
the
actual
physical
data
These
are
often
the
data
element
obtained
after
operation
like
error
checking
data
validation
proper
formatting
and
conversion
are
complete
Most
abstract
input
data
element
are
recognized
by
starting
from
the
phys-
ical
input
and
traveling
toward
the
output
in
the
data
flow
diagram
until
the
data
element
are
reached
that
can
no
longer
be
considered
incoming
The
aim
is
to
go
a
far
a
possible
from
the
physical
input
without
losing
the
incoming
nature
of
the
data
element
This
process
is
performed
for
each
input
stream
Identifying
the
most
abstract
data
item
represents
a
value
judgment
on
the
part
of
the
designer
but
often
the
choice
is
obvious
Similarly
we
identify
the
most
abstract
output
data
element
by
starting
from
the
output
in
the
data
flow
diagram
and
traveling
toward
the
input
These
are
the
data
element
that
are
most
removed
from
the
actual
output
but
can
still
be
considered
outgoing
These
data
element
may
also
be
considered
the
logical
output
data
item
and
the
transforms
in
the
data
flow
diagram
after
these
data
item
are
basically
to
convert
the
logical
output
into
a
form
in
which
the
system
is
required
to
produce
the
output
There
will
usually
be
some
transforms
left
between
the
most
abstract
input
and
output
data
item
These
central
transforms
perform
the
basic
transforma-
tion
for
the
system
taking
the
most
abstract
input
and
transforming
it
into
the
most
abstract
output
The
purpose
of
having
central
transforms
deal
with
the
most
abstract
data
item
is
that
the
module
implementing
these
transforms
can
concentrate
on
performing
the
transformation
without
being
concerned
with
converting
the
data
into
proper
format
validating
the
data
and
so
forth
Consider
now
the
data
flow
diagram
of
the
automated
teller
shown
in
Figure
The
two
most
abstract
input
are
the
dollar
amount
and
the
validated
account
number
The
validated
account
number
is
the
most
abstract
input
rather
than
the
account
number
read
in
a
it
is
still
the
input—but
with
a
guarantee
that
the
account
number
is
valid
The
two
abstract
output
are
obvious
The
abstract
input
and
output
are
marked
in
the
data
flow
diagram
First-Level
Factoring
Having
identified
the
central
transforms
and
the
most
abstract
input
and
output
data
item
we
are
ready
to
identify
some
module
for
the
system
We
first
specify
a
main
module
whose
purpose
is
to
invoke
the
subordinate
The
main
module
is
therefore
a
coordinate
module
For
each
of
the
most
abstract
input
data
item
an
immediate
subordinate
module
to
the
main
module
is
specified
Each
of
these
module
is
an
input
module
whose
purpose
is
to
deliver
to
the
main
module
the
most
abstract
data
item
for
which
it
is
created
Similarly
for
each
most
abstract
output
data
item
a
subordinate
module
that
is
an
output
module
that
accepts
data
from
the
main
module
is
specified
Each
of
the
arrow
connecting
these
input
and
output
subordinate
module
is
labeled
with
the
respective
abstract
data
item
flowing
in
the
proper
direction
Finally
for
each
central
transform
a
module
subordinate
to
the
main
one
is
specified
These
module
will
be
transform
module
whose
purpose
is
to
accept
data
from
the
main
module
and
then
return
the
appropriate
data
back
to
the
main
module
The
data
item
coming
to
a
transform
module
from
the
main
module
are
on
the
incoming
arc
of
the
corresponding
transform
in
the
data
flow
diagram
The
data
item
returned
are
on
the
outgoing
arc
of
that
transform
Note
that
here
a
module
is
created
for
a
transform
while
input/output
module
are
created
for
data
item
Let
u
examine
the
data
flow
diagram
of
the
ATM
We
have
already
seen
that
this
ha
two
most
abstract
input
two
most
abstract
output
and
two
central
transforms
Drawing
a
module
for
each
of
these
we
get
the
structure
chart
shown
in
Figure
As
we
can
see
the
first-level
factoring
is
straightforward
after
the
most
abstract
input
and
output
data
item
are
identified
in
the
data
flow
diagram
The
main
module
is
the
overall
control
module
which
will
form
the
main
program
or
procedure
in
the
implementation
of
the
design
It
is
a
coordinate
module
that
invokes
the
input
module
to
get
the
most
abstract
data
item
pass
these
to
the
appropriate
transform
module
and
delivers
the
result
of
the
transform
module
to
other
transform
module
until
the
most
abstract
data
item
are
obtained
These
are
then
passed
to
the
output
module
Factoring
the
Input
Output
and
Transform
Branches
The
first-level
factor-
ing
result
in
a
very
high
level
structure
where
each
subordinate
module
ha
a
lot
of
processing
to
do
To
simplify
these
module
they
must
be
factored
into
subordinate
module
that
will
distribute
the
work
of
a
module
Each
of
the
input
output
and
transformation
module
must
be
considered
for
factoring
Let
u
start
with
the
input
module
The
purpose
of
an
input
module
a
viewed
by
the
main
program
is
to
produce
some
data
To
factor
an
input
module
the
transform
in
the
data
flow
diagram
that
produced
the
data
item
is
now
treated
a
a
central
transform
The
process
performed
for
the
first-level
factoring
is
repeated
here
with
this
new
central
transform
with
the
input
module
being
considered
the
main
module
A
subordinate
input
module
is
created
for
each
input
data
stream
coming
into
this
new
central
transform
and
a
subordinate
transform
module
is
created
for
the
new
central
transform
The
new
input
module
now
created
can
then
be
factored
again
until
the
physical
input
are
reached
Factoring
of
input
module
will
usually
not
yield
any
output
subordinate
module
The
factoring
of
the
output
module
is
symmetrical
to
the
factoring
of
the
input
module
For
an
output
module
we
look
at
the
next
transform
to
be
applied
to
the
output
to
bring
it
closer
to
the
ultimate
desired
output
This
now
becomes
the
central
transform
and
an
output
module
is
created
for
each
data
stream
going
out
of
this
transform
During
the
factoring
of
output
module
there
will
usually
be
no
input
module
If
the
data
flow
diagram
of
the
problem
is
sufficiently
detailed
factoring
of
the
input
and
output
module
is
straightforward
However
there
are
no
such
rule
for
factoring
the
central
transforms
The
goal
is
to
determine
subtrans-
form
that
will
together
compose
the
overall
transform
and
then
repeat
the
process
for
the
newly
found
transforms
until
we
reach
the
atomic
module
Factoring
the
central
transform
is
essentially
an
exercise
in
functional
decom-
position
and
will
depend
on
the
designer
’
experience
and
judgment
One
way
to
factor
a
transform
module
is
to
treat
it
a
a
problem
in
it
own
right
and
start
with
a
data
flow
diagram
for
it
The
input
to
the
data
flow
diagram
are
the
data
coming
into
the
module
and
the
output
are
the
data
being
returned
by
the
module
Each
transform
in
this
data
flow
diagram
represents
a
subtransform
of
this
transform
The
central
transform
can
be
factored
by
creating
a
subordinate
transform
module
for
each
of
the
transforms
in
this
data
flow
diagram
This
process
can
be
repeated
for
the
new
transform
module
that
are
created
until
we
reach
atomic
module
As
an
example
consider
the
problem
of
determining
the
number
of
different
word
in
an
input
file
The
data
flow
diagram
for
this
problem
is
shown
in
Figure
This
problem
ha
only
one
input
data
stream
the
input
file
while
the
desired
output
is
the
count
of
different
word
in
the
file
To
transform
the
input
to
the
desired
output
the
first
thing
we
do
is
form
a
list
of
all
the
word
in
the
file
It
is
best
to
then
sort
the
list
a
this
will
make
identifying
different
word
easier
This
sorted
list
is
then
used
to
count
the
number
of
different
word
and
the
output
of
this
transform
is
the
desired
count
which
is
then
printed
This
sequence
of
data
transformation
is
what
we
have
in
the
data
flow
diagram
The
arc
in
the
data
flow
diagram
are
the
most
abstract
input
and
most
abstract
output
The
choice
of
the
most
abstract
input
is
obvious
We
start
following
the
input
First
the
input
file
is
converted
into
a
word
list
which
is
essentially
the
input
in
a
different
form
The
sorted
word
list
is
still
basically
the
input
a
it
is
still
the
same
list
in
a
different
order
This
appears
to
be
the
most
abstract
input
because
the
next
data
i.e.
count
is
not
just
another
form
of
the
input
data
The
choice
of
the
most
abstract
output
is
even
more
obvious
count
is
the
natural
choice
a
data
that
is
a
form
of
input
will
not
usually
be
a
candidate
for
the
most
abstract
output
Thus
we
have
one
cen-
tral
transform
count-number-of-different-words
which
ha
one
input
and
one
output
data
item
The
structure
chart
after
the
first-level
factoring
of
the
word
counting
prob-
lem
is
shown
in
Figure
In
this
structure
there
is
one
input
module
which
return
the
sorted
word
list
to
the
main
module
The
output
module
take
from
the
main
module
the
value
of
the
count
There
is
only
one
central
transform
in
this
example
and
a
module
is
drawn
for
that
Note
that
the
data
item
traveling
to
and
from
this
transformation
module
are
the
same
a
the
data
item
going
in
and
out
of
the
central
transform
The
factoring
of
the
input
module
get-sorted-list
in
the
first-level
structure
is
shown
in
Figure
The
transform
producing
the
input
returned
by
this
module
i.e.
the
sort
transform
is
treated
a
a
central
transform
Its
input
is
the
word
list
Thus
in
the
first
factoring
we
have
an
input
module
to
get
the
list
and
a
transform
module
to
sort
the
list
The
input
module
can
be
factored
further
a
the
module
need
to
perform
two
function
getting
a
word
and
then
adding
it
to
the
list
Note
that
the
looping
arrow
is
used
to
show
the
iteration
In
this
example
there
is
only
one
transform
after
the
most
abstract
output
so
factoring
for
output
need
not
be
done
The
factoring
of
the
central
transform
count-the-number-of-different-words
is
shown
in
Figure
This
wa
a
relatively
simple
transform
and
we
did
not
need
to
draw
the
data
flow
diagram
To
determine
the
number
of
word
we
have
to
get
a
word
repeatedly
determine
if
it
is
the
same
a
the
previous
word
for
a
sorted
list
this
checking
is
sufficient
to
determine
if
the
word
is
different
from
other
word
and
then
count
the
word
if
it
is
different
For
each
of
the
three
different
function
we
have
a
subordinate
module
and
we
get
the
structure
shown
in
Figure
Object-oriented
OO
approach
for
software
development
have
become
ex-
tremely
popular
in
recent
year
Much
of
the
new
development
is
now
being
done
using
OO
technique
and
language
There
are
many
advantage
that
OO
system
offer
An
OO
model
closely
represents
the
problem
domain
which
make
it
easier
to
produce
and
understand
design
As
requirement
change
the
object
in
a
system
are
le
immune
to
these
change
thereby
permitting
change
more
easily
Inheritance
and
close
association
of
object
in
design
to
problem
domain
entity
encourage
more
re-use
i.e.
new
application
can
use
existing
module
more
effectively
thereby
reducing
development
cost
and
cycle
time
Object-oriented
approach
are
believed
to
be
more
natural
and
provide
richer
structure
for
thinking
and
abstraction
Common
design
pattern
have
also
been
uncovered
that
allow
reusability
at
a
higher
level
Design
pattern
is
an
advanced
topic
which
we
will
not
discus
further
interested
reader
are
referred
to
38
The
object-oriented
design
approach
is
fundamentally
different
from
the
function-oriented
design
approach
primarily
due
to
the
different
abstraction
that
is
used
It
requires
a
different
way
of
thinking
and
partitioning
It
can
be
said
that
thinking
in
object-oriented
term
is
most
important
for
producing
truly
object-oriented
design
In
this
section
we
will
first
discus
some
important
concept
that
form
the
basis
of
object-orientation
We
will
then
describe
the
UML
notation
that
can
be
used
while
doing
an
object-oriented
design
followed
by
an
OO
design
methodology
Here
we
very
briefly
discus
the
main
concept
behind
object-orientation
Read-
er
familiar
with
an
OO
language
will
be
familiar
with
these
concept
Classes
and
Objects
Classes
and
object
are
the
basic
building
block
of
an
OO
design
just
like
function
and
procedure
are
for
a
function-oriented
design
Objects
are
entity
that
encapsulate
some
state
and
provide
service
to
be
used
by
a
client
which
could
be
another
object
program
or
a
user
The
basic
property
of
an
object
is
encapsulation
it
encapsulates
the
data
and
information
it
contains
and
support
a
well-defined
abstraction
The
set
of
service
that
can
be
requested
from
outside
the
object
form
the
interface
of
the
object
An
object
may
have
operation
defined
only
for
internal
use
that
can
not
be
used
from
outside
Such
operation
do
not
form
part
of
the
interface
A
major
advantage
of
encapsulation
is
that
access
to
the
encapsulated
data
is
limited
to
the
operation
defined
on
the
data
Hence
it
becomes
much
easier
to
ensure
that
the
integrity
of
data
is
preserved
something
very
hard
to
do
if
any
program
from
outside
can
directly
manipulate
the
data
structure
of
an
object
Encapsulation
and
separation
of
the
interface
and
it
implementation
also
allows
the
implementation
to
be
changed
without
affecting
the
client
a
long
a
the
interface
is
preserved
The
encapsulated
data
for
an
object
defines
the
state
of
the
object
An
important
property
of
object
is
that
this
state
persists
in
contrast
to
the
data
defined
in
a
function
or
procedure
which
is
generally
lost
once
the
function
stop
being
active
finish
it
current
execution
In
an
object
the
state
is
preserved
and
it
persists
through
the
life
of
the
object
i.e.
unless
the
object
is
actively
destroyed
The
state
and
service
of
an
object
together
define
it
behavior
We
can
say
that
the
behavior
of
an
object
is
how
an
object
reacts
in
term
of
state
change
when
it
is
acted
on
and
how
it
act
on
other
object
by
requesting
service
and
operation
Generally
for
an
object
the
defined
operation
together
specify
the
behavior
of
the
object
Objects
represent
the
basic
runtime
entity
in
an
OO
system
they
occupy
space
in
memory
that
keep
it
state
and
is
operated
on
by
the
defined
opera-
tions
on
the
object
A
class
on
the
other
hand
defines
a
possible
set
of
object
We
have
seen
that
object
have
some
attribute
whose
value
constitute
much
of
the
state
of
an
object
What
attribute
an
object
ha
are
defined
by
the
class
of
the
object
Similarly
the
operation
allowed
on
an
object
or
the
service
it
provides
are
defined
by
the
class
of
the
object
But
a
class
is
merely
a
definition
that
doe
not
create
any
object
and
can
not
hold
any
value
Each
object
when
it
is
created
get
a
private
copy
of
the
instance
variable
and
when
an
operation
defined
on
the
class
is
performed
on
the
object
it
is
performed
on
the
state
of
the
particular
object
The
relationship
between
a
class
and
object
of
that
class
is
similar
to
the
relationship
between
a
type
and
element
of
that
type
A
class
represents
a
set
of
object
that
share
a
common
structure
and
a
common
behavior
whereas
an
object
is
an
instance
of
a
class
Relationships
among
Objects
An
object
a
a
stand-alone
entity
ha
very
limited
capabilities—it
can
only
provide
the
service
defined
on
it
Any
complex
system
will
be
composed
of
many
object
of
different
class
and
these
object
will
interact
with
each
other
so
that
the
overall
system
objective
are
met
In
object-oriented
system
an
object
interacts
with
another
by
sending
a
message
to
the
object
to
perform
some
service
it
provides
On
receiving
the
request
message
the
object
invokes
the
requested
service
or
the
method
and
sends
the
result
if
needed
This
form
of
client-server
interaction
is
a
direct
fall
out
of
encapsulation
and
abstraction
supported
by
object
If
an
object
invokes
some
service
in
other
object
we
can
say
that
the
two
object
are
related
in
some
way
to
each
other
If
an
object
us
some
service
of
another
object
there
is
an
association
between
the
two
object
This
association
is
also
called
a
link—a
link
exists
from
one
object
to
another
if
the
object
us
some
service
of
the
other
object
Links
frequently
show
up
a
pointer
when
programming
A
link
capture
the
fact
that
a
message
is
flowing
from
one
object
to
another
However
when
a
link
exists
though
the
message
flow
in
the
direction
of
the
link
information
can
flow
in
both
direction
e.g.
the
server
may
return
some
result
With
association
come
the
issue
of
visibility
that
is
which
object
is
visible
to
whom
The
basic
issue
here
is
that
if
there
is
a
link
from
object
A
to
object
B
for
A
client
object
to
be
able
to
send
a
message
to
B
supplier
object
B
must
be
visible
to
A
in
the
final
program
There
are
different
way
to
provide
this
visibility
Some
of
the
important
possibility
are
15
The
supplier
object
is
a
parameter
to
some
operation
of
the
client
that
sends
the
message
The
supplier
object
is
a
part
of
the
client
object
The
supplier
object
is
locally
declared
in
some
operation
Links
between
object
capture
the
client/server
type
of
relationship
An-
other
type
of
relationship
between
object
is
aggregation
which
reflects
the
whole/part-of
relationship
Though
not
necessary
aggregation
generally
im-
ply
containment
That
is
if
an
object
A
is
an
aggregation
of
object
B
and
C
then
object
B
and
C
will
generally
be
within
object
A
though
there
are
situa-
tions
where
the
conceptual
relationship
of
aggregation
may
not
get
reflected
a
actual
containment
of
object
The
main
implication
of
this
is
that
a
contained
object
can
not
survive
without
it
containing
object
With
link
that
is
not
the
case
Inheritance
and
Polymorphism
Inheritance
is
a
relation
between
class
that
allows
for
definition
and
implementation
of
one
class
based
on
the
definition
of
existing
class
62
When
a
class
B
inherits
from
another
class
A
B
is
referred
to
a
the
subclass
or
the
derived
class
and
A
is
referred
to
a
the
superclass
or
the
base
class
In
general
a
subclass
B
will
have
two
part
a
derived
part
and
an
incremental
part
62
The
derived
part
is
the
part
inherited
from
A
and
the
incremental
part
is
the
new
code
and
definition
that
have
been
specifically
added
for
B
This
is
shown
in
Figure
62
Objects
of
type
B
have
the
derived
part
a
well
a
the
incremental
part
Hence
by
defining
only
the
incremental
part
and
inheriting
the
derived
part
from
an
existing
class
we
can
define
object
that
contain
both
Inheritance
is
often
called
an
is
a
relation
implying
that
an
object
of
type
B
is
also
an
instance
of
type
A
That
is
an
instance
of
a
subclass
though
more
than
an
instance
of
the
superclass
is
also
an
instance
of
the
superclass
The
inheritance
relation
between
class
form
a
hierarchy
As
discussed
earlier
it
is
important
that
the
hierarchy
represent
a
structure
present
in
the
application
domain
and
is
not
created
simply
to
reuse
some
part
of
an
existing
class
And
the
hierarchy
should
be
such
that
an
object
of
a
class
is
also
an
object
of
all
it
superclass
in
the
problem
domain
The
power
of
inheritance
lie
in
the
fact
that
all
common
feature
of
the
subclass
can
be
accumulated
in
the
superclass
In
other
word
a
feature
is
placed
in
the
higher
level
of
abstraction
Once
this
is
done
such
feature
can
be
inherited
from
the
parent
class
and
used
in
the
subclass
directly
This
implies
that
if
there
are
many
abstract
class
definition
available
when
a
new
class
is
needed
it
is
possible
that
the
new
class
is
a
specialization
of
one
or
more
of
the
existing
class
In
that
case
the
existing
class
can
be
tailored
through
inheritance
to
define
the
new
class
Inheritance
can
be
broadly
classified
a
being
of
two
type
strict
inheritance
and
nonstrict
inheritance
77
In
strict
inheritance
a
subclass
take
all
the
feature
from
the
parent
class
and
add
additional
feature
to
specialize
it
That
is
all
data
member
and
operation
available
in
the
base
class
are
also
available
in
the
derived
class
This
form
support
the
is-a
relation
and
is
the
easiest
form
of
inheritance
Nonstrict
inheritance
occurs
when
the
subclass
doe
not
have
all
the
feature
of
the
parent
class
or
some
feature
have
been
redefined
These
form
do
not
satisfy
Liskov
’
s
substitution
principle
A
class
hierarchy
need
not
be
a
simple
tree
structure
It
may
be
a
graph
which
implies
that
a
class
may
inherit
from
multiple
class
This
type
of
in-
heritance
when
a
subclass
inherits
from
many
superclass
is
called
multiple
inheritance
Multiple
inheritance
complicates
matter
and
it
use
is
generally
discouraged
We
will
assume
that
multiple
inheritance
is
not
to
be
used
Inheritance
brings
in
polymorphism
a
general
concept
widely
used
in
type
theory
that
deal
with
the
ability
of
an
object
to
be
of
different
type
In
OO
programming
polymorphism
come
in
the
form
that
a
reference
can
refer
to
object
of
different
type
at
different
time
In
object-oriented
system
with
inheritance
polymorphism
can
not
be
avoided—it
must
be
supported
The
rea-
son
is
the
is
a
relation
supported
by
inheritance—an
object
x
declared
to
be
of
class
B
is
also
an
object
of
any
class
A
that
is
the
superclass
of
B
Hence
anywhere
an
instance
of
A
is
expected
x
can
be
used
With
polymorphism
an
entity
ha
a
static
type
and
a
dynamic
type
62
The
static
type
of
an
object
is
the
type
of
which
the
object
is
declared
in
the
program
text
and
it
remains
unchanged
The
dynamic
type
of
an
entity
on
the
other
hand
can
change
from
time
to
time
and
is
known
only
at
reference
time
Once
an
entity
is
declared
at
compile
time
the
set
of
type
that
this
entity
belongs
to
can
be
determined
from
the
inheritance
hierarchy
that
ha
been
defined
The
dynamic
type
of
the
object
will
be
one
of
this
set
but
the
actual
dynamic
type
will
be
defined
at
the
time
of
reference
of
the
object
This
type
of
polymorphism
requires
dynamic
binding
of
operation
Dy-
namic
binding
mean
that
the
code
associated
with
a
given
procedure
call
is
not
known
until
the
moment
of
the
call
62
Let
u
illustrate
with
an
example
Suppose
x
is
a
polymorphic
reference
whose
static
type
is
B
but
whose
dynamic
type
could
be
either
A
or
B
Suppose
that
an
operation
O
is
defined
in
the
class
A
which
is
redefined
in
the
class
B
Now
when
the
operation
O
is
in-
voked
on
x
it
is
not
known
statically
what
code
will
be
executed
That
is
the
code
to
be
executed
for
the
statement
x.O
is
decided
at
runtime
depending
on
the
dynamic
type
of
x—if
the
dynamic
type
is
A
the
code
for
the
operation
O
in
class
A
will
be
executed
if
the
dynamic
type
is
B
the
code
for
opera-
tion
O
in
class
B
will
be
executed
This
dynamic
binding
can
be
used
quite
effectively
during
application
development
to
reduce
the
size
of
the
code
This
feature
polymorphism
which
is
essentially
overloading
of
the
feature
i.e.
a
feature
can
mean
different
thing
in
different
context
and
it
exact
meaning
is
determined
only
at
runtime
cause
no
problem
in
strict
inheri-
tance
because
all
feature
of
a
superclass
are
available
in
the
subclass
But
in
nonstrict
inheritance
it
can
cause
problem
because
a
child
may
lose
a
feature
Because
the
binding
of
the
feature
is
determined
at
runtime
this
can
cause
a
runtime
error
a
a
situation
may
arise
where
the
object
is
bound
to
the
superclass
in
which
the
feature
is
not
present
UML
is
a
graphical
notation
for
expressing
object-oriented
design
35
It
is
called
a
modeling
language
and
not
a
design
notation
a
it
allows
representing
various
aspect
of
the
system
not
just
the
design
that
ha
to
be
implemented
For
an
OO
design
a
specification
of
the
class
that
exist
in
the
system
might
suffice
However
while
modeling
during
the
design
process
the
designer
also
try
to
understand
how
the
different
class
are
related
and
how
they
interact
to
provide
the
desired
functionality
This
aspect
of
modeling
help
build
design
that
are
more
likely
to
satisfy
the
requirement
of
the
system
Due
to
the
ability
of
UML
to
create
different
model
it
ha
become
an
aid
for
understanding
the
system
designing
the
system
a
well
a
a
notation
for
representing
design
Though
UML
ha
now
evolved
into
a
fairly
comprehensive
and
large
mod-
eling
notation
we
will
focus
on
a
few
central
concept
and
notation
relating
to
class
and
their
relationship
and
interaction
For
a
more
detailed
discussion
on
UML
the
reader
is
referred
to
35
Class
Diagram
The
class
diagram
of
UML
is
the
central
piece
in
a
design
or
model
As
the
name
suggests
these
diagram
describe
the
class
that
are
there
in
the
design
As
the
final
code
of
an
OO
implementation
is
mostly
class
these
diagram
have
a
very
close
relationship
with
the
final
code
There
are
many
tool
that
translate
the
class
diagram
to
code
skeleton
thereby
avoiding
error
that
might
get
introduced
if
the
class
diagram
are
manually
translated
to
class
definition
by
programmer
A
class
diagram
defines
Classes
that
exist
in
the
system—besides
the
class
name
the
diagram
are
capable
of
describing
the
key
field
a
well
a
the
important
method
of
the
class
Associations
between
classes—what
type
of
association
exist
between
dif-
ferent
class
Subtype
supertype
relationship—classes
may
also
form
subtypes
giving
type
hierarchy
using
polymorphism
The
class
diagram
can
represent
these
hierarchy
also
A
class
itself
is
represented
a
a
rectangular
box
which
is
divided
into
three
area
The
top
part
give
the
class
name
By
convention
the
class
name
is
a
word
with
the
first
letter
in
uppercase
In
general
if
the
class
name
is
a
combination
of
many
word
then
the
first
letter
of
each
word
is
in
uppercase
The
middle
part
list
the
key
attribute
or
field
of
the
class
These
attribute
are
the
state
holder
for
the
object
of
the
class
By
convention
the
name
of
the
attribute
start
in
lowercase
and
if
multiple
word
are
joined
then
each
new
word
start
in
uppercase
The
bottom
part
list
the
method
or
operation
of
the
class
These
represent
the
behavior
that
the
class
can
provide
Naming
convention
is
same
a
for
attribute
but
to
show
that
it
is
a
function
the
name
end
with
The
parameter
of
the
method
can
also
be
specified
if
desired
If
a
class
is
an
interface
having
specification
but
no
body
this
can
be
specified
by
marking
the
class
with
the
stereotype
<
<
interface
>
>
which
is
generally
written
above
the
class
name
Similarly
if
a
class/method/attribute
ha
some
property
that
we
want
to
specify
it
can
be
done
by
tagging
the
entity
by
specifying
the
property
next
to
the
entity
name
within
and
or
by
using
some
special
symbol
Examples
of
a
class
with
some
tagged
value
and
an
interface
are
shown
in
Figure
Figure
6.12
Class
stereotype
and
tagged
value
The
divided-box
notation
is
to
describe
the
key
feature
of
a
class
a
a
stand-alone
entity
However
class
have
relationship
between
them
and
ob-
jects
of
different
class
interact
Therefore
to
model
a
system
or
an
applica-
tion
we
must
represent
relationship
between
class
One
common
relationship
is
the
generalization-specialization
relationship
between
class
which
finally
get
reflected
a
the
inheritance
hierarchy
In
this
hierarchy
property
of
gen-
eral
significance
are
assigned
to
a
more
general
class—the
superclass—while
property
which
can
specialize
an
object
further
are
put
in
the
subclass
All
property
of
the
superclass
are
inherited
by
the
subclass
so
a
subclass
contains
it
own
property
a
well
a
those
of
the
superclass
The
generalization-specialization
relationship
is
specified
by
having
arrow
coming
from
the
subclass
to
the
superclass
with
the
empty
triangle-shaped
arrowhead
touching
the
superclass
Often
when
there
are
multiple
subclass
of
a
class
this
may
be
specified
by
having
one
arrowhead
on
the
superclass
and
then
drawing
line
from
this
to
the
different
subclass
In
this
hierarchy
often
specialization
is
done
on
the
basis
of
some
discriminator—a
distinguishing
property
that
is
used
to
specialize
superclass
into
different
subclass
In
other
word
by
using
the
discriminator
object
of
the
superclass
type
are
partitioned
into
set
of
object
of
different
subclass
type
The
discriminator
used
for
the
generalization-specialization
relationship
can
be
specified
by
labeling
the
arrow
An
example
of
how
this
relationship
is
modeled
in
UML
is
shown
in
Figure
In
this
example
the
IITKPerson
class
represents
all
people
belonging
to
the
IITK
These
are
broadly
divided
into
two
subclasses—Student
and
Employee
a
both
these
type
have
many
different
property
some
com-
mon
one
also
and
different
behavior
Similarly
student
have
two
different
subclass
UnderGraduate
and
PostGraduate
both
requiring
some
different
attribute
and
having
different
constraint
The
Employee
class
ha
subtypes
representing
the
faculty
staff
and
research
staff
This
hierarchy
is
from
an
actual
working
system
developed
for
the
Institute
Besides
the
generalization-specialization
relationship
another
common
re-
lationship
is
association
which
allows
object
to
communicate
with
each
other
An
association
between
two
class
mean
that
an
object
of
one
class
need
some
service
from
object
of
the
other
class
to
perform
it
own
service
The
relationship
is
that
of
peer
in
that
object
of
both
the
class
can
use
service
of
the
other
The
association
is
shown
by
a
line
between
the
two
class
An
association
may
have
a
name
which
can
be
specified
by
labeling
the
association
line
The
association
can
also
be
assigned
some
attribute
of
it
own
And
if
the
role
of
the
two
end
of
the
association
need
to
be
named
that
can
also
be
done
In
an
association
an
end
may
also
have
multiplicity
allowing
rela-
tionships
like
1
etc.
to
be
modeled
Where
there
is
a
fixed
multiplicity
it
is
represented
by
putting
a
number
at
that
end
a
zero
or
many
multiplicity
is
represented
by
a
Figure
6.14
Aggregation
and
association
among
class
Another
type
of
relationship
is
the
part-whole
relationship
which
represents
the
situation
when
an
object
is
composed
of
many
part
each
part
itself
being
an
object
This
situation
represents
containment
or
aggregation
i.e.
object
of
a
class
is
contained
inside
the
object
of
another
class
Containment
and
ag-
gregation
can
be
treated
separately
and
shown
differently
but
we
will
consider
them
a
the
same
For
representing
this
aggregation
relationship
the
class
which
represents
the
whole
is
shown
at
the
top
and
a
line
emanating
from
a
little
diamond
connecting
it
to
class
which
represent
the
part
Often
in
an
implementation
this
relationship
is
implemented
in
the
same
manner
a
an
as-
sociation
hence
this
relationship
is
also
sometimes
modeled
a
an
association
The
association
and
aggregation
are
shown
in
Figure
expanding
the
example
given
above
An
object
of
IITKPerson
type
contains
two
object
of
type
Address
representing
the
permanent
address
and
the
current
address
It
also
contains
an
object
of
type
BiometricInfo
which
keep
information
like
the
person
’
s
picture
and
signature
As
these
object
are
common
to
all
people
they
belong
in
the
parent
class
rather
than
a
subclass
An
IITKPerson
is
allowed
to
take
some
advance
from
the
Institute
to
meet
expense
for
travel
medical
etc
Hence
Advances
is
a
different
class
which
incidently
ha
a
hierarchy
of
it
own
to
which
IITKPerson
class
ha
a
1-to-m
association
These
relation
Class
diagram
focus
on
class
and
should
not
be
confused
with
object
diagram
Objects
are
specific
instance
of
class
Sometimes
it
is
desirable
to
model
specific
object
and
the
relationship
between
them
and
for
that
object
diagram
are
used
An
object
is
represented
like
a
class
except
that
it
name
also
specifies
the
name
of
the
class
to
which
it
belongs
Generally
the
object
name
start
in
lowercase
and
the
class
name
is
specified
after
a
colon
To
further
clarify
the
entire
name
is
underlined
An
example
is
myList
List
The
attribute
of
an
object
may
have
specific
value
These
value
can
be
specified
by
giving
them
along
with
the
attribute
name
e.g.
name
=
John
Sequence
and
Collaboration
Diagrams
Class
diagram
represent
the
static
structure
of
the
system
That
is
they
capture
the
structure
of
the
code
that
may
implement
it
and
how
the
different
class
in
the
code
are
related
Class
diagram
however
do
not
represent
the
dynamic
behavior
of
the
system
That
is
how
the
system
behaves
when
it
performs
some
of
it
function
can
not
be
represented
by
class
diagram
This
is
done
through
sequence
diagram
or
collaboration
diagram
together
called
interaction
diagram
An
interaction
diagram
typically
capture
the
behavior
of
a
use
case
and
model
how
the
different
object
in
the
system
collaborate
to
implement
the
use
case
Let
u
first
discus
sequence
diagram
which
is
perhaps
more
common
of
the
two
interaction
diagram
A
sequence
diagram
show
the
series
of
message
exchanged
between
some
object
and
their
temporal
ordering
when
object
collaborate
to
provide
some
desired
system
functionality
or
implement
a
use
case
The
sequence
diagram
is
generally
drawn
to
model
the
interaction
between
object
for
a
particular
use
case
Note
that
in
a
sequence
diagram
and
also
in
collaboration
diagram
it
is
object
that
participate
and
not
class
When
capturing
dynamic
behavior
the
role
of
class
are
limited
a
during
execution
it
is
object
that
exist
In
a
sequence
diagram
all
the
object
that
participate
in
the
interaction
are
shown
at
the
top
a
box
with
object
name
For
each
object
a
vertical
bar
representing
it
lifeline
is
drawn
downwards
A
message
from
one
object
to
another
is
represented
a
an
arrow
from
the
lifeline
of
one
to
the
lifeline
of
the
other
Each
message
is
labeled
with
the
message
name
which
typically
should
be
the
name
of
a
method
in
the
class
of
the
target
object
An
object
can
also
make
a
self
call
which
is
shown
a
a
message
starting
and
ending
in
the
same
object
’
s
lifeline
To
clarify
the
sequence
of
message
and
relative
timing
of
each
time
is
represented
a
increasing
a
one
move
farther
away
downwards
from
the
object
name
in
the
object
life
That
is
time
is
represented
by
the
y-axis
increasing
downwards
Figure
6.15
Sequence
diagram
for
printing
a
graduation
report
Using
the
lifeline
of
object
and
arrow
one
can
model
object
’
life
and
how
message
flow
from
one
object
to
another
However
frequently
a
message
is
sent
from
one
object
to
another
only
under
some
condition
This
condition
can
be
represented
in
the
sequence
diagram
by
specifying
it
within
bracket
before
the
message
name
If
a
message
is
sent
to
multiple
receiver
object
then
this
multiplicity
is
shown
by
having
a
before
the
message
name
Each
message
ha
a
return
which
is
when
the
operation
finish
and
return
the
value
if
any
to
the
invoking
object
Though
often
this
message
can
be
implied
sometimes
it
may
be
desirable
to
show
the
return
message
explicitly
This
is
done
by
using
a
dashed
arrow
An
example
sequence
diagram
is
shown
in
Figure
This
example
concern
printing
the
graduation
report
for
student
The
object
for
GradReport
which
ha
the
responsibility
for
printing
the
report
sends
a
message
to
the
Student
object
for
the
relevant
information
which
request
the
CourseTaken
object
for
the
course
the
student
ha
taken
These
object
get
information
about
the
course
from
the
Course
object
A
collaboration
diagram
also
show
how
object
communicate
Instead
of
using
a
timeline-based
representation
that
is
used
by
sequence
diagram
a
col-
laboration
diagram
look
more
like
a
state
diagram
Each
object
is
represented
in
the
diagram
and
the
message
sent
from
one
object
to
another
are
shown
a
numbered
arrow
from
one
object
to
the
other
In
other
word
the
chrono-
logical
ordering
of
message
is
captured
by
message
numbering
in
contrast
to
a
sequence
diagram
where
ordering
of
message
is
shown
pictorially
As
should
be
clear
the
two
type
of
interaction
diagram
are
semantically
equivalent
and
have
the
same
representation
power
The
collaboration
diagram
for
the
above
example
is
shown
in
Figure
Over
the
year
however
sequence
diagram
have
become
more
popular
a
people
find
the
visual
representation
of
sequenc-
ing
quicker
to
grasp
Figure
6.16
Collaboration
diagram
for
printing
a
graduation
report
As
we
can
see
an
interaction
diagram
model
the
internal
dynamic
behavior
of
the
system
when
the
system
performs
some
function
The
internal
dynam-
ic
of
the
system
is
represented
in
term
of
how
the
object
interact
with
each
other
Through
an
interaction
diagram
one
can
clearly
see
how
a
system
inter-
nally
implement
an
operation
and
what
message
are
sent
between
different
object
If
a
convincing
interaction
diagram
can
not
be
constructed
for
a
system
operation
with
the
class
that
have
been
identified
in
the
class
diagram
then
it
is
safe
to
say
that
the
system
structure
is
not
capable
of
supporting
this
operation
and
that
it
must
be
enhanced
So
it
can
be
used
to
validate
if
the
system
structure
being
designed
through
class
diagram
is
capable
of
providing
the
desired
service
As
a
system
ha
many
function
each
involving
different
object
in
different
way
there
will
be
a
dynamic
model
for
each
of
these
function
or
use
case
In
other
word
whereas
one
class
diagram
can
capture
the
structure
of
the
system
’
s
code
for
the
dynamic
behavior
many
diagram
are
needed
However
it
may
not
be
feasible
or
practical
to
draw
the
interaction
diagram
for
each
use
case
scenario
Typically
during
design
an
interaction
diagram
of
some
key
use
case
or
function
will
be
drawn
to
make
sure
that
the
class
that
exist
can
indeed
support
the
desired
use
case
and
to
understand
their
dynamic
Other
Diagrams
and
Capabilities
UML
is
an
extensible
and
quite
elaborate
modeling
notation
Above
we
have
discussed
notation
related
to
two
of
the
most
common
model
developed
while
modeling
a
system—class
diagram
and
interaction
diagram
These
two
together
help
model
the
static
structure
of
the
system
a
well
a
the
dynamic
behavior
There
are
however
many
other
aspect
that
might
need
to
be
modeled
for
which
extra
notation
is
required
UML
provides
notation
for
many
different
type
of
model
In
modeling
and
building
system
a
we
have
seen
component
may
also
be
used
Components
encapsulate
larger
element
and
are
semantically
simpler
than
class
Components
often
encapsulate
subsystem
and
provide
clearly
defined
interface
through
which
these
component
can
be
used
by
other
com-
ponents
in
the
system
While
designing
an
architecture
a
we
have
seen
com-
ponents
are
very
useful
UML
provides
a
notation
for
specifying
a
component
and
a
separate
notation
for
specifying
a
subsystem
In
a
large
system
many
class
may
be
combined
together
to
form
package
where
a
package
is
a
collec-
tion
of
many
element
possibly
of
different
type
UML
also
provides
a
notation
to
specify
package
These
are
shown
in
Figure
As
discussed
in
Chapter
5
the
deployment
view
of
the
system
is
distinct
from
the
component
or
module
view
In
a
deployment
view
the
focus
is
on
what
software
element
us
which
hardware
that
is
how
the
system
is
deployed
UML
ha
notation
for
representing
a
deployment
view
The
main
element
is
a
node
represented
a
a
named
cube
which
represents
a
computing
resource
like
the
CPU
which
physically
exists
The
name
of
the
cube
identifies
the
resource
a
well
a
it
type
Within
the
cube
for
the
node
the
software
element
it
deploys
which
can
be
component
package
class
etc
are
shown
using
their
respective
notation
If
different
node
communicate
with
each
other
this
is
shown
by
connecting
the
node
with
line
The
notation
for
package
deployment
view
etc.
provides
structural
view
of
the
system
from
different
perspective
UML
also
provides
notation
to
ex-
press
different
type
of
behavior
A
state
diagram
is
a
model
in
which
the
entity
being
modeled
is
viewed
a
a
set
of
state
with
transition
between
the
state
taking
place
when
some
event
occurs
A
state
is
represented
a
a
rectangle
with
rounded
edge
or
a
ellipsis
or
circle
transition
are
represented
by
arrow
con-
necting
two
state
Details
can
also
be
attached
to
transition
State
diagram
are
often
used
to
model
the
behavior
of
object
of
a
class—the
state
represents
the
different
state
of
the
object
and
transition
capture
the
performing
of
the
different
operation
on
that
object
So
whereas
interaction
diagram
capture
how
object
collaborate
a
state
diagram
model
how
an
object
itself
evolves
a
operation
are
performed
on
it
This
can
help
clearly
understand
and
specify
the
behavior
of
a
class
Activity
Diagram
This
is
another
diagram
for
modeling
dynamic
behavior
It
aim
to
model
a
system
by
modeling
the
activity
that
take
place
in
it
when
the
system
executes
for
performing
some
function
Each
activity
is
represented
a
an
oval
with
the
name
of
the
activity
within
it
From
the
activity
the
system
proceeds
to
other
activity
Often
which
activity
to
perform
next
depends
on
some
decision
This
decision
is
shown
a
a
diamond
leading
to
multiple
activity
which
are
the
option
for
this
decision
Repeated
execution
of
some
activity
can
also
be
shown
These
diagram
are
like
flow-charts
but
also
have
notation
to
specify
parallel
execution
of
activity
in
a
system
by
specifying
an
activity
splitting
into
multiple
activity
or
many
activity
joining
synchronizing
after
their
completion
UML
is
an
extensible
notation
allowing
a
modeler
the
flexibility
to
represent
newer
concept
a
well
There
are
many
situation
in
which
a
modeler
need
some
notation
which
is
similar
to
an
existing
one
but
is
not
exactly
the
same
For
example
in
some
case
one
may
want
to
specify
if
a
class
is
an
abstract
class
or
an
interface
Instead
of
having
special
notation
for
these
concept
UML
ha
the
concept
of
a
stereotype
through
which
existing
notation
can
be
used
to
model
different
concept
An
existing
notation
for
example
of
a
class
can
be
used
to
represent
some
other
similar
concept
by
specifying
it
a
a
stereotype
by
giving
the
name
of
the
new
concept
within
<
<
and
>
>
We
have
already
seen
an
example
earlier
A
metaclass
can
be
specified
in
a
similar
manner
and
so
can
a
utility
class
one
which
ha
some
utility
function
which
are
directly
used
and
whose
object
are
not
created
Tagged
value
can
be
used
to
specify
additional
property
of
the
element
to
which
they
are
attached
They
can
be
attached
to
any
name
and
are
spec-
ified
within
Though
tagged
value
can
be
anything
a
modeler
want
it
is
best
to
limit
it
use
to
a
few
clearly
defined
and
pre-agreed
property
like
private
abstract
query
readonly
etc
Notes
can
also
be
attached
to
the
differ-
ent
element
in
a
model
We
have
earlier
seen
the
use
of
some
tagged
value
in
Figure
Use
case
diagram
are
also
a
part
of
the
UML
We
discussed
use
case
in
an
earlier
chapter
In
a
use
case
diagram
each
use
case
is
shown
a
a
node
and
the
relationship
between
actor
and
use
case
is
shown
by
arc
They
are
mostly
used
to
provide
a
high-level
summary
of
use
case
Many
design
and
analysis
methodology
have
been
proposed
As
we
stated
earlier
a
methodology
basically
us
the
concept
of
OO
in
this
case
to
pro-
vide
guideline
for
the
design
activity
Though
methodology
are
useful
they
do
not
reduce
the
activity
of
design
to
a
sequence
of
step
that
can
be
followed
mechanically
We
briefly
discus
one
methodology
here
Even
though
it
is
one
of
the
earlier
methodology
it
basic
concept
are
still
applicable
15
We
assume
that
during
architecture
design
the
system
ha
been
divided
into
high-level
subsystem
or
component
The
problem
we
address
is
how
to
produce
an
object-oriented
design
for
a
subsystem
As
we
discussed
earlier
the
OO
design
consists
of
specification
of
all
the
class
and
object
that
will
exist
in
the
system
implementation
A
complete
OO
design
should
be
such
that
in
the
implementation
phase
only
further
detail
about
method
or
attribute
need
to
be
added
A
few
low-level
object
may
be
added
later
but
most
of
the
class
and
object
and
their
relationship
are
identified
during
design
An
approach
for
creating
an
OO
design
consists
of
the
following
sequence
of
step
Develop
the
dynamic
model
and
use
it
to
define
operation
on
class
Develop
the
functional
model
and
use
it
to
define
operation
on
class
Identifying
Classes
and
Relationships
Identifying
the
class
and
their
re-
lationships
requires
identification
of
object
type
in
the
problem
domain
the
structure
between
class
both
inheritance
and
aggregation
attribute
of
the
different
class
association
between
the
different
class
and
the
service
each
class
need
to
provide
to
support
the
system
Basically
in
this
step
we
are
trying
to
define
the
initial
class
diagram
of
the
design
To
identify
analysis
object
start
by
looking
at
the
problem
and
it
descrip-
tion
In
the
description
consider
the
phrase
that
represent
entity
Include
an
entity
a
an
object
if
the
system
need
to
remember
something
about
it
the
system
need
some
service
from
it
to
perform
it
own
service
or
it
ha
mul-
tiple
attribute
If
the
system
doe
not
need
to
keep
information
about
some
real-world
entity
or
doe
not
need
any
service
from
the
entity
it
need
not
be
considered
a
an
object
for
design
Carefully
consider
object
that
have
only
one
attribute
such
object
can
frequently
be
included
a
attribute
in
other
object
Though
in
the
analysis
we
focus
on
identifying
object
in
modeling
class
for
these
object
are
represented
Classes
have
attribute
Attributes
add
detail
about
the
class
and
are
the
repository
of
data
for
an
object
For
example
for
an
object
of
class
Person
the
attribute
could
be
the
name
sex
and
address
The
data
stored
in
form
of
value
of
attribute
are
hidden
from
outside
the
object
and
are
accessed
and
manipulated
only
by
the
service
function
for
that
object
Which
attribute
should
be
used
to
define
the
class
of
an
object
depends
on
the
problem
and
what
need
to
be
done
For
example
while
modeling
a
hospital
system
for
the
class
Person
attribute
of
height
weight
and
date
of
birth
may
be
needed
although
these
may
not
be
needed
for
a
database
for
a
county
that
keep
track
of
population
in
various
neighborhood
To
identify
attribute
consider
each
class
and
see
which
attribute
are
needed
by
the
problem
domain
This
is
frequently
a
simple
task
Then
posi-
tion
each
attribute
properly
using
the
structure
if
the
attribute
is
a
common
attribute
it
should
be
placed
in
the
superclass
while
if
it
is
specific
to
a
special-
ized
object
it
should
be
placed
with
the
subclass
While
identifying
attribute
new
class
may
also
get
defined
or
old
class
may
disappear
e.g.
if
you
find
that
a
class
really
is
an
attribute
of
another
For
a
class
diagram
we
also
need
to
identify
the
structure
and
association
between
class
To
identify
the
classification
structure
consider
the
class
that
have
been
identified
a
a
generalization
and
see
if
there
are
other
class
that
can
be
considered
a
specialization
of
this
The
specialization
should
be
meaningful
for
the
problem
domain
For
example
if
the
problem
domain
doe
not
care
about
the
material
used
to
make
some
object
there
is
no
point
in
specializing
the
class
based
on
the
material
they
are
made
of
Similarly
consider
class
a
specialization
and
see
if
there
are
other
class
that
have
similar
attribute
If
so
see
if
a
generalized
class
can
be
identified
of
which
these
are
specialization
Once
again
the
structure
obtained
must
naturally
reflect
the
hierarchy
in
the
problem
domain
it
should
not
be
extracted
simply
because
some
class
have
some
attribute
with
the
same
name
To
identify
assembly
structure
a
similar
approach
is
taken
Consider
each
object
of
a
class
a
an
assembly
and
identify
it
part
See
if
the
system
need
to
keep
track
of
the
part
If
it
doe
then
the
part
must
be
reflected
a
object
if
not
then
the
part
should
not
be
modeled
a
separate
object
Then
consider
an
object
of
a
class
a
a
part
and
see
to
which
class
’
s
object
it
can
be
considered
a
belonging
Once
again
this
separation
is
maintained
only
if
the
system
need
it
As
before
the
structure
identified
should
naturally
reflect
the
hierarchy
in
the
problem
domain
and
should
not
be
forced
For
association
we
need
to
identify
the
relationship
between
instance
of
various
class
For
example
an
instance
of
the
class
Company
may
be
related
to
an
instance
of
the
class
Person
by
an
employ
relationship
This
is
similar
to
what
is
done
in
ER
modeling
And
like
in
ER
modeling
an
instance
connec-
tion
may
be
of
1
Or
it
could
be
1
M
indicating
that
one
instance
of
this
class
may
be
related
to
many
instance
of
the
other
class
There
are
M
M
connection
and
there
are
sometimes
multiway
connection
but
these
are
not
very
common
The
association
between
object
are
derived
from
the
problem
domain
directly
once
the
object
have
been
identified
An
association
may
have
attribute
of
it
own
these
are
typically
attribute
that
do
not
naturally
belong
to
either
object
Although
in
many
situation
they
can
be
forced
to
belong
to
one
of
the
two
object
without
losing
any
information
it
should
not
be
done
unless
the
attribute
naturally
belongs
to
the
object
Dynamic
Modeling
The
class
diagram
obtained
give
the
initial
module-level
design
This
design
will
be
further
shaped
by
the
event
in
the
system
a
the
design
ha
to
ensure
that
the
expected
behavior
for
the
event
can
be
supported
Modeling
the
dynamic
behavior
of
the
system
will
help
in
further
refining
the
design
The
dynamic
model
of
a
system
aim
to
specify
how
the
state
of
various
object
change
when
event
occur
An
event
is
something
that
happens
at
some
time
instance
For
an
object
an
event
is
essentially
a
request
for
an
operation
An
event
typically
is
an
occurrence
of
something
and
ha
no
time
duration
associated
with
it
Each
event
ha
an
initiator
and
a
responder
Events
can
be
internal
to
the
system
in
which
case
the
event
initiator
and
the
event
responder
are
both
within
the
system
An
event
can
be
an
external
event
in
which
case
the
event
initiator
is
outside
the
system
e.g.
the
user
or
a
sensor
A
scenario
is
a
sequence
of
event
that
occur
in
a
particular
execution
of
the
system
a
we
have
seen
while
discussing
use
case
in
Chapter
3
From
the
scenario
the
different
event
being
performed
on
different
object
can
be
identified
which
are
then
used
to
identify
service
on
object
The
different
scenario
together
can
completely
characterize
the
behavior
of
the
system
If
the
design
is
such
that
it
can
support
all
the
scenario
we
can
be
sure
that
the
desired
dynamic
behavior
of
the
system
can
be
supported
by
the
design
This
is
the
basic
reason
for
performing
dynamic
modeling
With
use
case
dynamic
modeling
involves
preparing
interaction
diagram
for
the
important
scenario
It
is
best
to
start
by
modeling
scenario
being
triggered
by
external
event
The
scenario
should
not
necessarily
cover
all
possibility
but
the
major
one
should
be
considered
First
the
main
success
scenario
should
be
modeled
then
scenario
for
exceptional
case
should
be
modeled
For
example
in
a
restau-
rant
the
main
success
scenario
for
placing
an
order
could
be
the
following
sequence
of
action
customer
read
the
menu
customer
place
the
order
order
is
sent
to
the
kitchen
for
preparation
ordered
item
are
served
customer
re-
quest
a
bill
for
the
order
bill
is
prepared
for
this
order
customer
is
given
the
bill
customer
pay
the
bill
An
exception
scenario
could
be
if
the
ordered
item
wa
not
available
or
if
the
customer
cancel
his
order
From
each
scenario
event
have
to
be
identified
Events
are
interaction
with
the
outside
world
and
object-to-object
interaction
All
the
event
that
have
the
same
effect
on
the
flow
of
control
in
the
system
are
grouped
a
a
single
event
type
Each
event
type
is
then
allocated
to
the
object
class
that
initiate
it
and
that
service
the
event
With
this
done
a
scenario
can
be
represented
a
a
sequence
or
collaboration
diagram
showing
the
event
that
will
take
place
on
the
different
object
in
the
execution
corresponding
to
the
scenario
A
possible
sequence
diagram
of
the
main
success
scenario
of
the
restaurant
is
given
in
Figure
Figure
6.18
A
sequence
diagram
for
the
restaurant
Once
the
main
scenario
are
modeled
various
event
on
object
that
are
needed
to
support
execution
corresponding
to
the
various
scenario
are
known
This
information
is
then
used
to
expand
our
view
of
the
class
in
the
design
Generally
speaking
for
each
event
in
the
sequence
diagram
there
will
be
an
operation
on
the
object
on
which
the
event
is
invoked
So
by
using
the
scenario
and
sequence
diagram
we
can
further
refine
our
view
of
the
object
and
add
operation
that
are
needed
to
support
some
scenario
but
may
not
have
been
identified
during
initial
modeling
For
example
from
the
event
trace
diagram
in
Figure
we
can
see
that
place
order
and
getBill
will
be
two
operation
required
on
the
object
of
type
Order
if
this
interaction
is
to
be
supported
The
effect
of
these
different
event
on
a
class
itself
can
be
modeled
using
the
state
diagram
We
believe
that
the
state
transition
diagram
is
of
limited
use
during
system
design
but
may
be
more
useful
during
detailed
design
Hence
we
will
discus
state
modeling
of
class
later
Functional
Modeling
A
functional
model
of
a
system
specifies
how
the
output
value
are
computed
in
the
system
from
the
input
value
without
considering
the
control
aspect
of
the
computation
This
represents
the
functional
view
of
the
system—the
mapping
from
input
to
output
and
the
various
step
involved
in
the
mapping
Generally
when
the
transformation
from
the
input
to
output
is
complex
consisting
of
many
step
the
functional
modeling
is
likely
to
be
useful
In
system
where
the
transformation
of
input
to
output
is
not
complex
functional
model
is
likely
to
be
straightforward
As
we
have
seen
the
functional
model
of
a
system
can
be
represented
by
a
data
flow
diagram
DFD
We
have
used
DFDs
in
problem
modeling
and
the
structured
design
methodology
discussed
earlier
Just
a
with
dynamic
modeling
the
basic
purpose
of
doing
functional
modeling
is
to
use
the
model
to
make
sure
that
the
object
model
can
perform
the
transformation
required
from
the
system
As
process
represent
operation
and
in
an
object-oriented
system
most
of
the
processing
is
done
by
operation
on
class
all
process
should
show
up
a
operation
on
class
Some
operation
might
appear
a
single
operation
on
an
object
others
might
appear
a
multiple
operation
on
different
class
depending
on
the
level
of
abstraction
of
the
DFD
If
the
DFD
is
sufficiently
detailed
most
process
will
occur
a
operation
on
class
The
DFD
also
specifies
the
abstract
signature
of
the
operation
by
identifying
the
input
and
output
Defining
Internal
Classes
and
Operations
The
class
identified
so
far
are
the
one
that
come
from
the
problem
domain
The
method
identified
on
the
object
are
the
one
needed
to
satisfy
all
the
interaction
with
the
environment
and
the
user
and
to
support
the
desired
functionality
However
the
final
design
is
a
blueprint
for
implementation
Hence
implementation
issue
have
to
be
con-
sidered
While
considering
implementation
issue
algorithm
and
optimization
issue
arise
These
issue
are
handled
in
this
step
First
each
class
is
critically
evaluated
to
see
if
it
is
needed
in
it
present
form
in
the
final
implementation
Some
of
the
class
might
be
discarded
if
the
designer
feel
they
are
not
needed
during
implementation
Then
the
implementation
of
operation
on
the
class
is
considered
For
this
rough
algorithm
for
implementation
might
be
considered
While
doing
this
a
complex
operation
may
get
defined
in
term
of
lower-level
operation
on
simpler
class
In
other
word
effective
implementation
of
operation
may
require
heavy
interaction
with
some
data
structure
and
the
data
structure
to
be
considered
an
object
in
it
own
right
These
class
that
are
identified
while
considering
implementation
concern
are
largely
support
class
that
may
be
needed
to
store
intermediate
result
or
to
model
some
aspect
of
the
object
whose
operation
is
to
be
implemented
Once
the
implementation
of
each
class
and
each
operation
on
the
class
ha
been
considered
and
it
ha
been
satisfied
that
they
can
be
implemented
the
system
design
is
complete
The
detailed
design
might
also
uncover
some
very
low-level
object
but
most
such
object
should
be
identified
during
system
Optimize
and
Package
During
design
some
inefficiency
may
have
crept
in
In
this
final
step
the
issue
of
efficiency
is
considered
keeping
in
mind
that
the
final
structure
should
not
deviate
too
much
from
the
logical
structure
produced
Various
optimization
are
possible
and
a
designer
can
exercise
his
judgment
keeping
in
mind
the
modularity
aspect
also
Before
we
apply
the
methodology
on
some
example
it
should
be
remembered
again
that
no
design
methodology
reduces
the
activity
of
producing
a
design
to
a
series
of
step
that
can
be
mechanically
executed
each
step
requires
some
amount
of
engineering
judgment
Methodologies
are
essentially
guideline
to
help
the
designer
in
the
design
activity
they
are
not
hard-and-fast
rule
The
example
we
present
here
are
relatively
small
and
all
aspect
of
the
methodol-
ogy
do
not
get
reflected
in
them
The
Word
Counting
Problem
Let
u
first
consider
the
word
counting
problem
discussed
earlier
a
an
example
for
Structured
Design
methodology
The
initial
analysis
clearly
show
that
there
is
a
File
object
which
is
an
aggregation
of
many
Word
object
Further
one
can
consider
that
there
is
a
Counter
object
which
keep
track
of
the
number
of
different
word
It
is
a
matter
of
preference
and
opinion
whether
Counter
should
be
an
object
or
counting
should
be
im-
plemented
a
an
operation
If
counting
is
treated
a
an
operation
the
question
will
be
to
which
object
it
belongs
As
it
doe
not
belong
naturally
to
either
the
class
Word
or
the
class
File
it
will
have
to
be
forced
into
one
of
the
class
For
this
reason
we
have
kept
Counter
a
a
separate
object
The
basic
problem
statement
find
only
these
three
object
However
further
analysis
for
service
reveals
that
some
history
mechanism
is
needed
to
check
if
the
word
is
unique
The
class
diagram
obtained
after
doing
the
initial
modeling
is
shown
in
Figure
Now
let
u
consider
the
dynamic
modeling
for
this
problem
This
is
essen-
tially
a
batch
processing
problem
where
a
file
is
given
a
input
and
some
output
is
given
by
the
system
Hence
the
use
case
and
scenario
for
this
problem
are
straightforward
For
example
the
scenario
for
the
normal
case
can
be
System
prompt
for
the
file
name
user
enters
the
file
name
Figure
6.19
Class
diagram
for
the
word
counting
problem
From
this
simple
scenario
no
new
operation
are
uncovered
and
our
class
diagram
stay
unchanged
Now
we
consider
the
functional
model
One
possible
functional
model
is
shown
in
Figure
The
model
reinforces
the
need
for
some
object
where
the
history
of
what
word
have
been
seen
is
recorded
This
object
is
used
to
check
the
uniqueness
of
the
word
It
also
show
that
various
operation
like
increment
isunique
and
addToHistory
are
needed
These
operation
should
appear
a
operation
in
class
or
should
be
supported
by
a
combination
of
operation
In
this
example
most
of
these
process
are
reflected
a
operation
on
class
and
are
already
incorporated
in
the
design
Now
we
are
at
the
last
two
step
of
design
methodology
where
implemen-
tation
and
optimization
concern
are
used
to
enhance
the
object
model
First
decision
we
take
is
that
the
history
mechanism
will
be
implemented
by
a
bi-
nary
search
tree
Hence
instead
of
the
class
History
we
have
a
different
class
Btree
Then
for
the
class
Word
various
operation
are
needed
to
compare
dif-
ferent
word
Operations
are
also
needed
to
set
the
string
value
for
a
word
and
retrieve
it
The
final
class
diagram
is
similar
in
structure
to
the
one
shown
in
Figure
except
for
these
change
The
final
step
of
the
design
activity
is
to
specify
this
design
This
is
not
a
part
of
the
design
methodology
but
it
is
an
essential
step
a
the
design
specification
is
what
form
the
major
part
of
the
design
document
The
design
specification
a
mentioned
earlier
should
specify
all
the
class
that
are
in
the
design
all
method
of
the
class
along
with
their
interface
We
use
C++
class
Figure
6.20
Functional
model
for
the
word
counting
problem
structure
for
our
specification
The
final
specification
of
this
design
is
given
below
This
specification
can
be
used
a
a
basis
of
implementing
the
design
char
string
//
string
representing
the
word
public
bool
operator
==
Word
//
Checks
for
equality
bool
operator
<
Word
Word
operator
=
Word
//
The
assignment
operator
void
setWord
char
//
Sets
the
string
for
the
word
char
getWord
//
get
the
string
for
the
word
Word
getWord
//
get
a
word
Invokes
operation
of
Word
bool
isEof
//
Checks
for
end
of
file
class
Btree
GENERIC
in
<
ELEMENT_TYPE
>
private
Btree
<
ELEMENT_TYPE
>
left
Btree
<
ELEMENT_TYPE
>
right
void
insert
ELEMENT_TYPE
//
to
insert
an
element
bool
lookup
ELEMENT_TYPE
//
to
check
if
an
element
exists
As
we
can
see
all
the
class
definition
complete
with
data
member
and
operation
and
all
the
major
declaration
are
given
in
the
design
specification
Only
the
implementation
of
the
method
is
not
provided
This
design
wa
later
implemented
in
C++
The
conversion
to
code
required
only
minor
addition
and
modification
to
the
design
The
final
code
wa
about
240
line
of
C++
code
counting
noncomment
and
nonblank
line
only
Rate
of
Returns
Problem
Let
u
consider
a
slightly
larger
problem
of
deter-
mining
the
rate
of
return
on
investment
An
investor
ha
made
investment
in
some
company
For
each
investment
in
a
file
the
name
of
the
company
all
the
money
he
ha
invested
in
the
initial
purchase
a
well
a
in
subsequent
purchase
and
all
the
money
he
ha
withdrawn
through
sale
of
share
or
div-
idends
are
given
along
with
the
date
of
each
transaction
The
current
value
of
the
investment
is
given
at
the
end
along
with
the
date
The
goal
is
to
find
the
rate
of
return
the
investor
is
getting
for
each
investment
a
well
a
the
rate
of
return
for
the
entire
portfolio
In
addition
the
amount
he
ha
invested
initially
amount
he
ha
invested
subsequently
amount
he
ha
withdrawn
and
the
current
value
of
the
portfolio
also
is
to
be
output
This
is
a
practical
problem
that
is
frequently
needed
by
investor
and
a
rate
of
return
calculator
can
easily
form
an
important
component
of
a
larger
financial
management
system
The
computation
of
rate
of
return
is
not
straight-
forward
and
can
not
be
easily
done
through
spreadsheet
Hence
such
a
software
can
be
of
practical
use
Initial
problem
analysis
easily
throw
up
a
few
object
class
of
interest—
Portfolio
Investment
and
Transaction
A
portfolio
consists
of
many
in-
vestment
and
an
investment
consists
of
many
transaction
Hence
the
class
Portfolio
is
an
aggregation
of
many
Investments
and
an
Investment
is
an
aggregation
of
many
Transactions
A
transaction
can
be
of
Withdrawal
type
or
Deposit
type
resulting
in
a
class
hierarchy
with
Investment
being
the
superclass
and
Withdrawal
and
Deposit
subclass
For
an
object
of
class
Investment
the
major
operation
we
need
to
perform
is
to
find
the
rate
of
return
For
the
class
Portfolio
we
need
to
have
operation
to
compute
rate
of
return
total
initial
investment
total
withdrawal
and
total
current
value
of
the
portfolio
Hence
we
need
operation
for
these
The
class
diagram
obtained
from
analysis
of
the
problem
is
shown
in
Figure
Figure
6.21
Class
diagram
for
rate
of
return
problem
In
this
problem
a
the
interaction
with
the
environment
is
not
much
the
dynamic
model
is
not
significant
Hence
we
omit
the
dynamic
modeling
for
this
problem
A
possible
functional
model
is
given
in
Figure
The
class
are
then
enhanced
to
make
sure
that
each
of
the
process
of
the
functional
model
is
reflected
a
operation
on
various
object
As
we
can
see
most
of
the
process
already
exist
a
operation
Now
we
have
to
perform
the
last
two
step
of
the
design
methodology
where
implementation
and
optimization
concern
are
used
to
enhance
the
class
While
considering
the
implementation
of
computation
of
total
initial
invest-
ment
computation
of
overall
return
rate
overall
withdrawal
and
so
on
we
notice
that
for
all
of
these
appropriate
data
from
each
investment
is
needed
Figure
6.22
Functional
model
for
the
rate
of
return
problem
Hence
to
the
class
Investments
appropriate
operation
need
to
be
added
Further
we
note
that
all
the
computation
for
total
initial
investment
total
current
value
and
so
on
are
all
done
together
and
each
of
these
is
essentially
adding
value
from
various
investment
Hence
we
combine
them
in
a
single
operation
in
Portfolio
and
a
corresponding
single
operation
in
Investment
Studying
the
class
hierarchy
we
observe
that
the
only
difference
in
the
two
sub-
class
Withdrawal
and
Deposit
is
that
in
one
case
the
amount
is
subtracted
and
in
the
other
it
is
added
In
such
a
situation
the
two
type
can
be
easily
considered
a
single
type
by
keeping
the
amount
a
negative
for
a
withdrawal
and
positive
for
a
deposit
So
we
remove
the
subclass
thereby
simplifying
the
design
and
implementation
Instead
of
giving
the
class
diagram
for
the
final
design
we
provide
the
specification
of
the
class
int
amount
//
money
amount
for
the
transaction
int
month
//
month
of
the
transaction
char
investmentName
//
Name
of
the
company
Transaction
transactArray
//
List
of
transaction
int
noOfTransacts
//
Total
number
of
transaction
float
rateOfReturn
//
rate
of
return
getTransactDetails
//
Set
detail
of
transaction
computeRate
float
getRate
//
Return
the
rate
of
the
return
compute
initVal
totWithdrawls
totCurVal
totDeposits
int
noOfInvestments
//
Total
number
of
investment
int
totalInitInvest
int
totalDeposits
int
totalCurVal
int
totalWithdrawl
float
RateOfReturns
//
Overall
rate
of
return
public
getInvestDetails
char
fname
//
Parse
the
input
file
computeRate
//
Compute
rate
of
return
printResults
//
Print
return
rate
total
value
etc
The
design
is
self-explanatory
This
design
wa
later
implemented
in
C++
code
and
we
found
that
only
minor
implementation
detail
got
added
during
the
implementation
showing
the
correctness
and
completeness
of
the
design
The
final
size
of
the
program
wa
about
470
line
of
C++
code
counting
noncomment
and
nonblank
line
only
In
the
previous
two
section
we
discussed
two
different
approach
for
system
design—one
based
on
functional
abstraction
and
one
based
on
object
In
sys-
tem
design
we
concentrate
on
the
module
in
a
system
and
how
they
interact
with
each
other
Once
the
module
are
identified
and
specified
during
the
high-
level
design
the
internal
logic
that
will
implement
the
given
specification
can
be
designed
and
is
the
focus
of
this
section
The
detailed
design
activity
is
often
not
performed
formally
and
archived
a
it
is
almost
impossible
to
keep
the
detailed
design
document
consistent
with
the
code
Due
to
this
developing
the
detailed
design
is
useful
for
the
more
complex
and
important
module
and
is
often
done
informally
by
the
programmer
a
part
of
the
personal
process
of
developing
code
The
basic
goal
in
detailed
design
is
to
specify
the
logic
for
the
different
module
that
have
been
specified
during
system
design
Specifying
the
logic
will
require
developing
an
algorithm
that
will
implement
the
given
specification
Here
we
consider
some
principle
for
designing
algorithm
or
logic
that
will
implement
the
given
specification
The
term
algorithm
is
quite
general
and
is
applicable
to
a
wide
variety
of
area
For
software
we
can
consider
an
algorithm
to
be
an
unambiguous
proce-
dure
for
solving
a
problem
42
A
procedure
is
a
finite
sequence
of
well-defined
step
or
operation
each
of
which
requires
a
finite
amount
of
memory
and
time
to
complete
In
this
definition
we
assume
that
termination
is
an
essential
prop-
erty
of
procedure
From
now
on
we
will
use
procedure
algorithm
and
logic
interchangeably
There
are
a
number
of
step
that
one
ha
to
perform
while
developing
an
algorithm
42
The
starting
step
in
the
design
of
algorithm
is
statement
of
the
problem
The
problem
for
which
an
algorithm
is
being
devised
ha
to
be
precisely
and
clearly
stated
and
properly
understood
by
the
person
responsible
for
designing
the
algorithm
For
detailed
design
the
problem
statement
come
from
the
system
design
That
is
the
problem
statement
is
already
available
when
the
detailed
design
of
a
module
commences
The
next
step
is
develop-
ment
of
a
mathematical
model
for
the
problem
In
modeling
one
ha
to
select
the
mathematical
structure
that
are
best
suited
for
the
problem
It
can
help
to
look
at
other
similar
problem
that
have
been
solved
In
most
case
model
are
constructed
by
taking
model
of
similar
problem
and
modifying
the
model
to
suit
the
current
problem
The
next
step
is
the
design
of
the
algorithm
Dur-
ing
this
step
the
data
structure
and
program
structure
are
decided
Once
the
algorithm
is
designed
it
correctness
should
be
verified
No
clear
procedure
can
be
given
for
designing
algorithm
Having
such
a
pro-
cedure
amount
to
automating
the
problem
of
algorithm
development
which
is
not
possible
with
the
current
method
However
some
heuristic
or
method
can
be
provided
to
help
the
designer
design
algorithm
for
module
The
most
common
method
for
designing
algorithm
or
the
logic
for
a
module
is
to
use
the
stepwise
refinement
technique
84
The
stepwise
refinement
technique
break
the
logic
design
problem
into
a
series
of
step
so
that
the
development
can
be
done
gradually
The
process
start
by
converting
the
specification
of
the
module
into
an
abstract
descrip-
tion
of
an
algorithm
containing
a
few
abstract
statement
In
each
step
one
or
several
statement
in
the
algorithm
developed
so
far
are
decomposed
into
more
detailed
instruction
The
successive
refinement
terminates
when
all
in-
structions
are
sufficiently
precise
that
they
can
easily
be
converted
into
pro-
gramming
language
statement
During
refinement
both
data
and
instruction
have
to
be
refined
A
guideline
for
refinement
is
that
in
each
step
the
amount
of
decomposition
should
be
such
that
it
can
be
easily
handled
and
that
rep-
resents
one
or
two
design
decision
Generally
detailed
design
is
not
specified
using
formal
programming
language
but
using
language
that
have
formal
programming
language
like
outer
structure
like
loop
conditionals
etc
but
a
freer
format
for
internal
description
This
allows
the
designer
to
focus
on
the
logic
and
not
it
representation
in
the
programming
language
For
object-oriented
design
the
approach
just
discussed
for
obtaining
the
de-
tailed
design
can
be
used
for
designing
the
logic
of
method
But
a
class
is
not
a
functional
abstraction
and
can
not
be
viewed
a
merely
a
collection
of
function
method
The
technique
for
getting
a
more
detailed
understanding
of
the
class
a
a
whole
without
talking
about
the
logic
of
different
method
ha
to
be
different
from
the
refinement-based
approach
An
object
of
a
class
ha
some
state
and
many
operation
on
it
To
better
understand
a
class
the
relationship
between
the
state
and
various
operation
and
the
effect
of
interaction
of
various
opera-
tions
have
to
be
understood
This
can
be
viewed
a
one
of
the
objective
of
the
detailed
design
activity
for
object-oriented
development
Once
the
overall
class
is
better
understood
the
algorithm
for
it
various
method
can
be
developed
A
method
to
understand
the
behavior
of
a
class
is
to
view
it
a
a
finite
state
automaton
which
consists
of
state
and
transition
between
state
When
modeling
an
object
the
state
is
the
value
of
it
attribute
and
an
event
is
the
performing
of
an
operation
on
the
object
A
state
diagram
relates
event
and
state
by
showing
how
the
state
change
when
an
event
is
performed
A
state
diagram
for
an
object
will
generally
have
an
initial
state
from
which
all
state
are
reachable
i.e.
there
is
a
path
from
the
initial
state
to
all
other
state
A
state
diagram
for
an
object
doe
not
represent
all
the
actual
state
of
the
object
a
there
are
many
possible
state
A
state
diagram
attempt
to
represent
only
the
logical
state
of
the
object
A
logical
state
of
an
object
is
a
combination
of
all
those
state
from
which
the
behavior
of
the
object
is
similar
for
all
possible
event
Two
logical
state
will
have
different
behavior
for
at
least
one
event
For
example
for
an
object
that
represents
a
stack
all
state
that
represent
a
stack
of
size
more
than
0
and
le
than
some
defined
maximum
are
similar
a
the
behavior
of
all
operation
defined
on
the
stack
will
be
similar
in
all
such
state
e.g.
push
will
add
an
element
pop
will
remove
one
etc.
However
the
state
representing
an
empty
stack
is
different
a
the
behavior
of
top
and
pop
operation
are
different
now
an
error
message
may
be
returned
Similarly
the
state
representing
a
full
stack
is
different
The
state
model
for
this
bounded
size
stack
is
shown
in
Figure
The
finite
state
modeling
of
object
is
an
aid
to
understand
the
effect
of
various
operation
defined
on
the
class
on
the
state
of
the
object
A
good
un-
derstanding
of
this
can
aid
in
developing
the
logic
for
each
of
the
operation
To
develop
the
logic
of
operation
regular
approach
for
algorithm
development
can
be
used
The
model
can
also
be
used
to
validate
if
the
logic
for
an
operation
is
correct
As
we
will
see
later
a
state
model
can
be
used
for
generating
test
case
for
validation
The
output
of
the
design
activity
should
be
verified
before
proceeding
with
the
activity
of
the
next
phase
If
the
design
is
expressed
in
some
formal
notation
for
which
analysis
tool
are
available
then
through
tool
it
can
be
checked
for
internal
consistency
e.g.
those
module
used
by
another
are
defined
the
interface
of
a
module
is
consistent
with
the
way
others
use
it
data
usage
is
consistent
with
declaration
etc
If
the
design
is
not
specified
in
a
formal
executable
language
it
can
not
be
processed
through
tool
and
other
mean
for
verification
have
to
be
used
The
most
common
approach
for
verification
is
The
purpose
of
design
review
is
to
ensure
that
the
design
satisfies
the
re-
quirements
and
is
of
good
quality
If
error
are
made
during
the
design
process
they
will
ultimately
reflect
themselves
in
the
code
and
the
final
system
As
the
cost
of
removing
fault
caused
by
error
that
occur
during
design
increase
with
the
delay
in
detecting
the
error
it
is
best
if
design
error
are
detected
early
before
they
manifest
themselves
in
the
system
Detecting
error
in
design
is
the
purpose
of
design
review
The
system
design
review
process
is
similar
to
the
inspection
process
in
that
a
group
of
people
get
together
to
discus
the
design
with
the
aim
of
revealing
design
error
or
undesirable
property
The
review
group
must
include
a
member
of
both
the
system
design
team
and
the
detailed
design
team
the
author
of
the
requirement
document
the
author
responsible
for
maintaining
the
design
document
and
an
independent
software
quality
engineer
As
with
any
review
it
should
be
kept
in
mind
that
the
aim
of
the
meeting
is
to
uncover
design
error
not
to
try
to
fix
them
fixing
is
done
later
The
number
of
way
in
which
error
can
enter
a
design
is
limited
only
by
the
creativity
of
the
designer
The
most
important
design
error
however
is
that
the
design
doe
not
fully
support
some
requirement
For
example
some
exception
case
scenario
can
not
be
handled
or
some
design
constraint
ha
not
been
satisfied
For
design
quality
modularity
is
the
main
criterion
However
since
there
is
a
need
to
validate
whether
performance
requirement
can
be
met
by
a
design
efficiency
is
another
key
property
for
which
a
design
is
evaluated
Here
we
discus
some
of
the
metric
that
can
be
extracted
from
a
design
and
that
could
be
useful
for
evaluating
the
design
We
do
not
discus
the
standard
metric
of
effort
or
defect
that
are
collected
a
per
the
project
plan
for
project
monitoring
Size
is
always
a
product
metric
of
interest
For
size
of
a
design
the
total
number
of
module
is
a
commonly
used
metric
By
using
an
average
size
of
a
module
from
this
metric
the
final
size
in
LOC
can
be
estimated
and
compared
with
project
estimate
Another
metric
of
interest
is
complexity
A
possible
use
of
complexity
met-
rics
at
design
time
is
to
improve
the
design
by
reducing
the
complexity
of
the
module
that
have
been
found
to
be
most
complex
This
will
directly
improve
the
testability
and
maintainability
We
describe
some
of
the
metric
that
have
been
proposed
to
quantify
the
complexity
of
design
We
first
discus
metric
for
function-oriented
design
and
then
for
OO
design
Network
Metrics
Network
metric
is
a
complexity
metric
that
try
to
capture
how
good
the
structure
chart
is
As
coupling
of
a
module
increase
if
it
is
called
by
more
module
a
good
structure
is
considered
one
that
ha
exactly
one
caller
That
is
the
call
graph
structure
is
simplest
if
it
is
a
pure
tree
The
more
the
structure
chart
deviate
from
a
tree
the
more
complex
the
system
Deviation
of
the
tree
is
then
defined
a
the
graph
impurity
of
the
design
87
Graph
impurity
can
be
defined
a
where
n
is
the
number
of
node
in
the
structure
chart
and
e
is
the
number
of
edge
As
in
a
pure
tree
the
total
number
of
node
is
one
more
than
the
number
of
edge
the
graph
impurity
for
a
tree
is
0
Each
time
a
module
ha
a
fan-in
of
more
than
one
the
graph
impurity
increase
The
major
drawback
of
this
approach
is
that
it
ignores
the
common
use
of
some
routine
like
library
or
support
routine
An
approach
to
handle
this
is
not
to
consider
the
lowest-level
node
for
graph
impurity
because
most
often
the
lowest-level
module
are
the
one
that
are
used
by
many
different
module
particularly
if
the
structure
chart
wa
factored
Library
routine
are
also
at
the
lowest
level
of
the
structure
chart
even
if
they
have
a
structure
of
their
own
it
doe
not
show
in
the
structure
chart
of
the
application
using
the
routine
Other
network
metric
have
also
been
defined
For
most
of
these
metric
significant
correlation
with
property
of
interest
have
not
been
established
Hence
their
use
is
limited
to
getting
some
idea
about
the
structure
of
the
design
Information
Flow
Metrics
The
network
metric
of
graph
impurity
had
the
basis
that
a
the
graph
impurity
increase
the
coupling
increase
However
it
is
not
a
very
good
approximation
for
coupling
a
coupling
of
a
module
increase
with
the
complexity
of
the
interface
and
the
total
number
of
module
a
module
is
coupled
with
whether
it
is
the
caller
or
the
callee
So
if
we
want
a
metric
that
is
better
at
quantifying
coupling
between
module
it
should
handle
these
The
information
flow
metric
attempt
to
define
the
complexity
in
term
of
the
total
information
flowing
through
a
module
In
one
of
the
earliest
work
on
information
flow
metric
48
49
the
com-
plexity
of
a
module
is
considered
a
depending
on
the
intramodule
complexity
and
the
intermodule
complexity
The
intramodule
complexity
is
approximated
by
the
size
of
the
module
in
line
of
code
The
intermodule
complexity
of
a
module
depends
on
the
total
information
flowing
in
the
module
inflow
and
the
total
information
flowing
out
of
the
module
outflow
The
inflow
of
a
mod-
ule
is
the
total
number
of
abstract
data
element
flowing
in
the
module
i.e.
whose
value
are
used
by
the
module
and
the
outflow
is
the
total
number
of
abstract
data
element
that
are
flowing
out
of
the
module
i.e.
whose
value
are
defined
by
this
module
and
used
by
other
module
The
module
design
complexity
Dc
is
defined
a
The
term
inflow
outflow
refers
to
the
total
number
of
combination
of
input
source
and
output
destination
This
term
is
squared
a
the
interconnection
between
the
module
is
considered
a
more
important
factor
compared
to
the
internal
complexity
determining
the
complexity
of
a
module
This
is
based
on
the
common
experience
that
the
module
with
more
interconnection
are
harder
to
test
or
modify
compared
to
other
similar-size
module
with
fewer
interconnection
The
metric
defined
earlier
defines
the
complexity
of
a
module
purely
in
term
of
the
total
amount
of
data
flowing
in
and
out
of
the
module
and
the
module
size
A
variant
of
this
wa
proposed
based
on
the
hypothesis
that
the
module
complexity
depends
not
only
on
the
information
flowing
in
and
out
but
also
on
the
number
of
module
to
or
from
which
it
is
flowing
The
module
size
is
considered
an
insignificant
factor
and
complexity
Dc
for
a
module
is
defined
a
89
where
fan
in
represents
the
number
of
module
that
call
this
module
and
fan
out
is
the
number
of
module
this
module
call
How
can
this
metric
be
used
in
design
One
method
for
highlighting
the
module
wa
suggested
in
89
Let
avg
complexity
be
the
average
complexity
of
the
module
in
the
design
being
evaluated
and
let
std
deviation
be
the
standard
deviation
in
the
design
complexity
of
the
module
of
the
system
The
proposed
method
classifies
the
module
in
three
category
error-prone
complex
and
normal
If
Dc
is
the
complexity
of
a
module
it
can
be
classified
a
follows
Note
that
this
definition
of
error-prone
and
complex
is
independent
of
the
met-
ric
definition
used
to
compute
the
complexity
of
module
With
this
approach
a
design
can
be
evaluated
by
itself
not
for
overall
design
quality
but
to
draw
at-
tention
to
the
error-prone
and
complex
module
This
information
can
then
be
used
to
redesign
the
system
to
reduce
the
complexity
of
these
module
which
also
result
in
overall
complexity
reduction
This
approach
ha
been
found
to
be
very
effective
in
identifying
error-prone
module
89
A
few
attempt
have
been
made
to
propose
metric
for
object-oriented
software
1
21
64
Here
we
present
some
metric
that
have
been
proposed
for
evaluating
the
complexity
of
an
OOD
As
design
of
class
is
the
central
issue
in
OOD
and
the
major
output
of
any
OOD
methodology
is
the
class
definition
these
metric
focus
on
evaluating
class
The
metric
discussed
here
were
proposed
in
21
We
discus
some
of
these
metric
that
were
experimentally
found
to
be
most
useful
in
predicting
fault-proneness
3
Weighted
Methods
per
Class
WMC
The
effort
in
developing
a
class
will
in
some
sense
will
be
determined
by
the
number
of
method
the
class
ha
and
the
complexity
of
the
method
Hence
a
complexity
metric
that
combine
the
number
of
method
and
the
complexity
of
method
can
be
useful
in
estimating
the
overall
complexity
of
the
class
The
weighted
method
per
class
WMC
metric
doe
precisely
this
Suppose
a
class
C
ha
method
M1
M2
...
Mn
defined
on
it
Let
the
com-
plexity
of
the
method
Mi
be
ci
As
a
method
is
like
a
regular
function
or
pro-
cedure
any
complexity
metric
that
is
applicable
for
function
can
be
used
to
define
ci
e.g.
estimated
size
interface
complexity
and
data
flow
complexity
The
WMC
is
defined
a
If
the
complexity
of
each
method
is
considered
to
be
1
WMC
give
the
total
number
of
method
in
the
class
Depth
of
Inheritance
Tree
DIT
Inheritance
is
a
we
have
mentioned
one
of
the
unique
feature
of
the
object-oriented
paradigm
As
we
have
said
before
inheritance
is
one
of
the
main
mechanism
for
reuse
in
OOD—the
deeper
a
particular
class
is
in
a
class
hierarchy
the
more
method
it
ha
available
for
reuse
thereby
providing
a
larger
reuse
potential
At
the
same
time
a
we
have
mentioned
inheritance
increase
coupling
which
make
changing
a
class
harder
In
other
word
a
class
deep
in
the
hierarchy
ha
a
lot
of
method
it
can
inherit
which
make
it
difficult
to
predict
it
behavior
For
both
these
reason
it
is
useful
to
have
some
metric
to
quantify
inheritance
The
depth
of
inheritance
tree
DIT
is
one
such
metric
The
DIT
of
a
class
C
in
an
inheritance
hierarchy
is
the
depth
from
the
root
class
in
the
inheritance
tree
In
other
word
it
is
the
length
of
the
shortest
path
from
the
root
of
the
tree
to
the
node
representing
C
or
the
number
of
ancestor
C
ha
In
case
of
multiple
inheritance
the
DIT
metric
is
the
maximum
length
from
a
root
to
C
Coupling
between
Classes
CBC
Coupling
between
class
CBC
is
a
metric
that
try
to
quantify
coupling
that
exists
between
class
The
CBC
value
for
a
class
C
is
the
total
number
of
other
class
to
which
the
class
is
coupled
Two
class
are
considered
coupled
if
method
of
one
class
use
method
or
instance
variable
defined
in
the
other
class
In
general
whether
two
class
are
coupled
can
easily
be
determined
by
looking
at
the
code
and
the
definition
of
all
the
method
of
the
two
class
However
note
that
there
are
indirect
form
of
coupling
through
pointer
etc
that
are
hard
to
identify
by
evaluating
the
code
Response
for
a
Class
RFC
Although
the
CBC
for
a
class
capture
the
number
of
other
class
to
which
this
class
is
coupled
it
doe
not
quantify
the
strength
of
interconnection
In
other
word
it
doe
not
explain
the
degree
of
connection
of
method
of
a
class
with
other
class
Response
for
a
class
RFC
try
to
quantify
this
by
capturing
the
total
number
of
method
that
can
be
invoked
from
an
object
of
this
class
The
RFC
value
for
a
class
C
is
the
cardinality
of
the
response
set
for
a
class
The
response
set
of
a
class
C
is
the
set
of
all
method
that
can
be
invoked
if
a
message
is
sent
to
an
object
of
this
class
This
includes
all
the
method
of
C
and
of
other
class
to
which
any
method
of
C
sends
a
message
It
is
clear
that
even
if
the
CBC
value
of
a
class
is
1
that
is
it
is
coupled
with
only
one
class
the
RFC
value
may
be
quite
high
indicating
that
the
volume
of
interaction
between
the
two
class
is
very
high
It
should
be
clear
that
it
is
likely
to
be
harder
to
test
class
that
have
higher
RFC
value
The
design
of
a
system
is
a
plan
for
a
solution
such
that
if
the
plan
is
implemented
the
implemented
system
will
satisfy
the
requirement
of
the
system
and
will
preserve
it
architecture
The
module-level
design
specifies
the
module
that
should
be
there
in
the
system
to
implement
the
architecture
and
the
detailed
design
the
processing
logic
of
module
A
system
is
considered
modular
if
each
module
ha
a
well-defined
abstrac-
tion
and
if
change
in
one
module
ha
minimal
impact
on
other
module
Two
criterion
used
for
evaluating
the
modularity
of
a
design
are
coupling
and
cohe-
sion
Coupling
reflects
how
interdependent
are
module
on
each
other
while
cohesion
is
a
measure
of
the
strength
with
which
the
different
element
of
a
module
are
related
In
general
in
a
design
coupling
should
be
minimized
and
cohesion
maximized
A
design
should
also
support
the
open-closed
principle
that
is
the
module
are
such
that
they
are
open
for
extension
but
closed
for
modification
A
structure
chart
for
a
procedural
system
represents
the
module
in
the
system
and
the
call
relationship
between
them
The
structured
design
methodology
give
guideline
on
how
to
create
a
design
represented
a
a
structure
chart
such
that
the
module
have
minimum
dependence
on
each
other
low
coupling
and
a
high
level
of
cohesion
For
this
the
methodology
partition
the
system
at
the
very
top
level
into
various
subsystem
one
for
managing
each
major
input
one
for
managing
each
major
output
and
one
for
each
major
transformation
This
cleanly
partition
the
system
into
part
each
independently
dealing
with
different
concern
The
Universal
Modeling
Language
UML
ha
various
type
of
diagram
to
model
different
property
which
allow
both
static
structure
a
well
a
dynamic
behavior
to
be
modeled
For
representing
the
static
structure
the
main
diagram
is
the
class
diagram
which
represents
the
class
in
the
system
and
relationship
between
the
class
For
modeling
the
dynamic
behavior
sequence
or
collaboration
diagram
may
be
used
to
show
how
a
scenario
is
implemented
by
involving
different
object
Using
the
UML
notation
OO
design
of
a
system
can
be
created
The
OO
design
methodology
focus
on
identifying
class
and
relationship
between
them
and
validating
the
class
definition
using
dynamic
and
functional
mod-
eling
In
detailed
design
of
procedure
the
logic
for
implementing
the
procedure
is
specified
in
a
semiformal
notation
For
class
state
diagram
can
be
used
to
model
the
relationship
between
method
The
most
common
method
for
verifying
a
design
is
design
review
in
which
a
team
of
people
review
the
design
for
the
purpose
of
finding
defect
There
are
a
number
of
metric
that
can
be
used
to
evaluate
the
complex-
ity
of
a
design
Network
metric
evaluate
the
structure
chart
and
consider
deviation
from
the
tree
a
the
metric
signifying
the
quality
of
design
The
information
flow
complexity
metric
define
design
complexity
based
on
the
internal
complexity
of
the
module
and
the
number
of
connection
between
module
These
can
be
used
to
identify
error-prone
module
and
improve
the
design
by
reducing
their
complexity
For
the
complexity
of
an
object-oriented
design
metric
like
weighted
method
per
class
the
depth
of
inheritance
tree
of
a
class
and
maximum
depth
in
the
class
hierarchy
of
this
class
can
be
used
What
is
the
relationship
between
an
architecture
and
module-level
design
Given
a
design
that
implement
the
SRS
what
criterion
will
you
use
to
evaluate
the
quality
of
this
design
Consider
a
program
containing
many
module
If
a
global
variable
x
must
be
used
to
share
data
between
two
module
A
and
B
how
would
you
design
the
interface
of
these
module
to
minimize
coupling
What
is
the
cohesion
of
the
following
module
How
would
you
change
the
module
to
increase
cohesion
procedure
file
file
ptr
file
name
op
name
begin
open
perform
activity
for
opening
the
file
close
perform
activity
for
opening
the
file
print
print
the
file
Draw
the
structure
chart
for
the
following
program
How
would
you
modify
this
program
to
improve
the
modularity
List
some
practice
that
you
will
follow
while
developing
a
software
system
using
an
object-oriented
approach
to
increase
cohesion
and
reduce
coupling
Finally
in
an
object-oriented
implementation
mostly
class
are
coded
Then
during
design
what
is
the
role
of
dynamic
modeling
using
UML
How
will
you
measure
the
information
flow
complexity
of
a
full
design
specified
a
a
structure
chart
Describe
two
metric
for
quantifying
complexity
of
an
object-oriented
design
How
will
you
use
one
of
them
to
identify
highly-complex
or
error-prone
module
The
goal
of
the
coding
or
programming
activity
is
to
implement
the
design
in
the
best
possible
manner
The
coding
activity
affect
both
testing
and
main-
tenance
profoundly
As
we
saw
earlier
the
time
spent
in
coding
is
a
small
percentage
of
the
total
software
cost
while
testing
and
maintenance
consume
the
major
portion
Thus
it
should
be
clear
that
the
goal
during
coding
should
not
be
just
to
reduce
the
implementation
cost
but
help
reduce
the
cost
of
later
phase
During
coding
it
should
be
kept
in
mind
that
the
program
should
not
be
constructed
so
that
they
are
easy
to
write
but
in
a
manner
that
they
are
easy
to
read
and
understand
A
program
is
read
a
lot
more
often
and
by
a
lot
more
people
during
the
later
phase
Having
readability
and
understandability
a
a
clear
objective
of
the
cod-
ing
activity
can
itself
help
in
achieving
it
A
famous
experiment
by
Weinberg
showed
that
if
programmer
are
specified
a
clear
objective
for
the
program
they
usually
satisfy
it
82
In
the
experiment
five
different
team
were
given
the
same
problem
for
which
they
had
to
develop
program
However
each
of
the
team
wa
specified
a
different
objective
which
it
had
to
satisfy
The
different
objective
given
were
minimize
the
effort
required
to
complete
the
program
minimize
the
number
of
statement
minimize
the
memory
required
maximize
the
program
clarity
and
maximize
the
output
clarity
It
wa
found
that
in
most
case
each
team
did
the
best
for
the
objective
that
wa
specified
to
it
The
rank
of
the
different
team
for
the
different
objective
is
shown
in
Table
The
experiment
clearly
show
that
if
objective
are
clear
programmer
tend
to
achieve
that
objective
Hence
if
readability
is
an
objective
of
the
coding
activity
then
it
is
likely
that
programmer
will
develop
easily
understandable
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
program
It
also
show
that
if
the
focus
is
on
minimizing
coding
effort
program
clarity
take
a
big
hit
For
our
purpose
ease
of
understanding
and
modification
are
the
basic
goal
of
the
programming
activity
Some
principle
like
structured
programming
information
hiding
use
of
cod-
ing
standard
which
can
help
develop
more
readable
program
Some
programmer-level
process
like
incremental
development
and
test-
driven
development
for
efficiently
developing
high-quality
code
How
to
manage
evolving
code
by
using
proper
source
code
control
and
refac-
toring
How
to
unit
test
module
using
unit
testing
framework
A
structured
code
inspection
process
that
can
be
used
effectively
to
improve
the
quality
of
the
code
The
main
task
before
a
programmer
is
to
write
readable
code
with
few
bug
in
it
An
additional
goal
is
to
write
code
quickly
Writing
solid
code
is
a
skill
that
can
only
be
acquired
by
practice
However
based
on
experience
some
general
rule
and
guideline
can
be
given
for
the
programmer
Good
programming
producing
correct
and
simple
program
is
a
practice
independent
of
the
target
programming
language
although
well-structured
programming
language
make
the
programmer
’
s
job
simpler
In
this
section
we
will
discus
some
concept
and
practice
that
can
help
a
programmer
write
higher-quality
code
that
is
also
easier
to
understand
As
stated
earlier
the
basic
objective
of
the
coding
activity
is
to
produce
pro-
gram
that
are
easy
to
understand
It
ha
been
argued
by
many
that
structured
programming
practice
help
develop
program
that
are
easier
to
understand
The
structured
programming
movement
started
in
the
1970s
and
much
ha
been
said
and
written
about
it
Now
the
concept
pervades
so
much
that
it
is
generally
accepted—even
implied—that
programming
should
be
structured
Though
a
lot
of
emphasis
ha
been
placed
on
structured
programming
the
con-
cept
and
motivation
behind
structured
programming
are
often
not
well
under-
stood
Structured
programming
is
often
regarded
a
goto-less
programming
Although
extensive
use
of
gotos
is
certainly
not
desirable
structured
program
can
be
written
with
the
use
of
gotos
Here
we
provide
a
brief
discussion
on
what
structured
programming
is
A
program
ha
a
static
structure
a
well
a
a
dynamic
structure
The
static
structure
is
the
structure
of
the
text
of
the
program
which
is
just
a
linear
orga-
nization
of
statement
of
the
program
The
dynamic
structure
of
the
program
is
the
sequence
of
statement
executed
during
the
execution
of
the
program
In
other
word
both
the
static
structure
and
the
dynamic
behavior
are
sequence
of
statement
while
the
sequence
representing
the
static
structure
of
a
program
is
fixed
the
sequence
of
statement
it
executes
can
change
from
execution
to
execution
It
will
clearly
be
easier
to
understand
the
dynamic
behavior
if
the
structure
in
the
dynamic
behavior
resembles
the
static
structure
The
closer
the
corre-
spondence
between
execution
and
text
structure
the
easier
the
program
is
to
understand
and
the
more
different
the
structure
during
execution
the
harder
it
will
be
to
argue
about
the
behavior
from
the
program
text
The
goal
of
structured
programming
is
to
ensure
that
the
static
structure
and
the
dynamic
structure
are
the
same
That
is
the
objective
of
structured
programming
is
to
write
program
so
that
the
sequence
of
statement
executed
during
the
ex-
ecution
of
a
program
is
the
same
a
the
sequence
of
statement
in
the
text
of
that
program
As
the
statement
in
a
program
text
are
linearly
organized
the
objective
of
structured
programming
becomes
developing
program
whose
control
flow
during
execution
is
linearized
and
follows
the
linear
organization
of
the
program
text
The
real
motivation
for
structured
programming
however
wa
formal
ver-
ification
of
program
To
show
that
a
program
is
correct
we
need
to
show
that
when
the
program
executes
it
behavior
is
what
is
expected
For
specifying
the
behavior
we
need
to
specify
the
condition
the
output
of
the
program
should
satisfy
As
a
program
will
usually
not
operate
on
an
arbitrary
set
of
input
data
and
may
produce
valid
result
only
for
some
range
of
input
we
generally
need
to
also
state
the
input
condition
in
which
the
program
is
to
be
invoked
and
for
which
the
program
is
expected
to
produce
valid
result
The
assertion
about
the
expected
final
state
of
a
program
is
called
the
post-condition
of
that
pro-
gram
and
the
assertion
about
the
input
condition
is
called
the
pre-condition
of
the
program
Often
in
program
verification
determining
the
pre-condition
for
which
the
post-condition
will
be
satisfied
is
the
goal
of
proof
Using
Hoare
’
s
notation
50
for
verifying
a
program
the
basic
assertion
about
a
program
segment
is
of
the
form
The
interpretation
of
this
is
that
if
assertion
P
is
true
before
executing
S
then
assertion
Q
will
be
true
after
executing
S
if
the
execution
of
S
terminates
Assertion
P
is
the
pre-condition
of
the
program
and
Q
is
the
post-condition
These
assertion
are
about
the
value
taken
by
the
variable
in
the
program
before
and
after
it
execution
The
assertion
generally
do
not
specify
a
par-
ticular
value
for
the
variable
but
they
specify
the
general
property
of
the
value
and
the
relationship
among
them
For
verification
of
larger
program
we
would
like
to
make
assertion
about
the
program
from
assertion
about
it
component
or
statement
If
the
program
is
a
sequence
of
statement
then
determining
the
semantics
of
a
composite
program
becomes
easier
If
a
statement
S
is
composed
of
two
statement
S2
which
are
to
be
executed
in
sequence
then
from
the
semantics
of
S2
we
can
easily
determine
the
semantics
of
S
this
rule
is
called
rule
of
composition
in
50
a
follows
The
explanation
of
this
notation
is
that
if
what
is
stated
in
the
numerator
can
be
proved
the
denominator
can
be
inferred
Using
this
rule
if
we
have
proved
P2
then
to
determine
the
semantics
of
S1
S2
all
we
need
to
show
is
that
Q2
and
then
we
can
claim
that
if
before
execution
the
pre-condition
P
then
after
execution
of
S1
S-condition
Q
In
other
word
to
prove
P
S1
S
once
we
have
determined
the
behavior
of
S2
we
just
need
one
additional
step
This
allows
building
proof
of
larger
program
from
proof
of
it
element
Note
that
the
rule
handle
a
strict
sequence
of
statement
only
So
if
we
want
to
apply
this
we
need
to
construct
the
program
a
a
sequence
of
statement
And
that
is
the
verification
motivation
of
wanting
to
linearize
the
control
flow
in
a
program
Clearly
no
meaningful
program
can
be
written
a
a
sequence
of
simple
statement
without
any
branching
or
repetition
which
also
involves
branching
So
how
is
the
objective
of
linearizing
the
control
flow
to
be
achieved
By
making
use
of
structured
construct
In
structured
programming
a
statement
is
not
a
simple
assignment
statement
it
is
a
structured
statement
The
key
property
of
a
structured
statement
is
that
it
ha
a
single-entry
and
a
single-exit
That
is
during
execution
the
execution
of
the
structured
statement
start
from
one
defined
point
and
the
execution
terminates
at
one
defined
point
With
single-entry
and
single-exit
statement
we
can
view
a
program
a
a
sequence
of
structured
statement
And
if
all
statement
are
structured
statement
then
during
execution
the
sequence
of
execution
of
these
statement
will
be
the
same
a
the
sequence
in
the
program
text
Hence
by
using
single-entry
and
single-exit
statement
the
correspondence
between
the
static
and
dynamic
structure
can
be
obtained
The
most
commonly
used
single-entry
and
single-exit
statement
are
It
can
be
shown
that
these
three
basic
construct
are
sufficient
to
program
any
conceivable
algorithm
Modern
language
have
other
such
construct
that
help
linearize
the
control
flow
of
a
program
which
generally
speaking
make
it
easier
to
understand
a
program
Hence
program
should
be
written
so
that
a
far
a
possible
single-entry
single-exit
control
construct
are
used
The
basic
goal
a
we
have
tried
to
emphasize
is
to
make
the
logic
of
the
program
simple
to
understand
No
hard-and-fast
rule
can
be
formulated
that
will
be
applicable
under
all
circumstance
Structured
programming
practice
form
a
good
basis
and
guideline
for
writing
program
clearly
A
final
note
about
the
structured
construct
Any
piece
of
code
with
a
single-
entry
and
single-exit
can
not
be
considered
a
structured
construct
If
that
is
the
case
one
could
always
define
appropriate
unit
in
any
program
to
make
it
ap-
pear
a
a
sequence
of
these
unit
in
the
worst
case
the
whole
program
could
be
defined
to
be
a
unit
The
basic
objective
of
using
structured
construct
is
to
linearize
the
control
flow
so
that
the
execution
behavior
is
easier
to
under-
stand
and
argue
about
In
linearized
control
flow
if
we
understand
the
behavior
of
each
of
the
basic
construct
properly
the
behavior
of
the
program
can
be
considered
a
composition
of
the
behavior
of
the
different
statement
For
this
approach
to
work
it
is
implied
that
we
can
clearly
understand
and
specify
the
behavior
of
each
construct
This
requires
that
we
be
able
to
succinctly
capture
or
describe
the
behavior
of
each
construct
Unless
we
can
do
this
it
will
not
be
possible
to
compose
them
Clearly
for
an
arbitrary
structure
we
can
not
do
this
merely
because
it
ha
a
single-entry
and
single-exit
It
is
from
this
view-
point
that
the
structure
mentioned
earlier
are
chosen
a
structured
statement
There
are
well-defined
rule
that
specify
how
these
statement
behave
during
execution
which
allows
u
to
argue
about
larger
program
Overall
it
can
be
said
that
structured
programming
lead
to
program
that
are
easier
to
understand
than
unstructured
program
and
that
such
program
are
easier
to
formally
prove
However
it
should
be
kept
in
mind
that
structured
programming
is
not
an
end
in
itself
Our
basic
objective
is
that
the
program
be
easy
to
understand
And
structured
programming
is
a
safe
approach
for
achieving
this
objective
Still
there
are
some
common
programming
practice
that
are
now
well
understood
that
make
use
of
unstructured
construct
e.g.
break
statement
continue
statement
Although
effort
should
be
made
to
avoid
using
statement
that
effectively
violate
the
single-entry
single-exit
property
if
the
use
of
such
statement
is
the
simplest
way
to
organize
the
program
then
from
the
point
of
view
of
readability
the
construct
should
be
used
The
main
point
is
that
any
unstructured
construct
should
be
used
only
if
the
structured
alternative
is
harder
to
understand
This
view
can
be
taken
only
because
we
are
focusing
on
readability
If
the
objective
wa
formal
verifiability
structured
programming
will
probably
be
necessary
A
software
solution
to
a
problem
always
contains
data
structure
that
are
meant
to
represent
information
in
the
problem
domain
That
is
when
software
is
developed
to
solve
a
problem
the
software
us
some
data
structure
to
capture
the
information
in
the
problem
domain
In
general
only
certain
operation
are
performed
on
some
information
That
is
a
piece
of
information
in
the
problem
domain
is
used
only
in
a
limited
number
of
way
in
the
problem
domain
For
example
a
ledger
in
an
accountant
’
s
office
ha
some
very
defined
us
debit
credit
check
the
current
balance
etc
An
operation
where
all
debit
are
multiplied
together
and
then
divided
by
the
sum
of
all
credit
is
typically
not
performed
So
any
information
in
the
problem
domain
typically
ha
a
small
number
of
defined
operation
performed
on
it
When
the
information
is
represented
a
data
structure
the
same
principle
should
be
applied
and
only
some
defined
operation
should
be
performed
on
the
data
structure
This
essentially
is
the
principle
of
information
hiding
The
information
captured
in
the
data
structure
should
be
hidden
from
the
rest
of
the
system
and
only
the
access
function
on
the
data
structure
that
represent
the
operation
performed
on
the
information
should
be
visible
In
other
word
when
the
information
is
captured
in
data
structure
and
then
on
the
data
structure
that
represent
some
information
for
each
operation
on
the
information
an
access
function
should
be
provided
And
a
the
rest
of
the
system
in
the
problem
domain
only
performs
these
defined
operation
on
the
information
the
rest
of
the
module
in
the
software
should
only
use
these
access
function
to
access
and
manipulate
the
data
structure
Information
hiding
can
reduce
the
coupling
between
module
and
make
the
system
more
maintainable
Information
hiding
is
also
an
effective
tool
for
managing
the
complexity
of
developing
software—by
using
information
hiding
we
have
separated
the
concern
of
managing
the
data
from
the
concern
of
using
the
data
to
produce
some
desired
result
Many
of
the
older
language
like
Pascal
C
and
FORTRAN
do
not
pro-
vide
mechanism
to
support
data
abstraction
With
such
language
informa-
tion
hiding
can
be
supported
only
by
a
disciplined
use
of
the
language
That
is
the
access
restriction
will
have
to
be
imposed
by
the
programmer
the
lan-
guage
doe
not
provide
them
Most
modern
OO
language
provide
linguistic
mechanism
to
implement
information
hiding
The
concept
discussed
above
can
help
in
writing
simple
and
clear
code
with
few
bug
There
are
many
programming
practice
that
can
also
help
toward
that
objective
We
discus
here
a
few
rule
that
have
been
found
to
make
code
easier
to
read
a
well
a
avoid
some
of
the
error
Control
Constructs
As
discussed
earlier
it
is
desirable
that
a
much
a
possible
single-entry
single-exit
construct
be
used
It
is
also
desirable
to
use
a
few
standard
control
construct
rather
than
using
a
wide
variety
of
construct
just
because
they
are
available
in
the
language
Gotos
Gotos
should
be
used
sparingly
and
in
a
disciplined
manner
Only
when
the
alternative
to
using
gotos
is
more
complex
should
the
gotos
be
used
In
any
case
alternative
must
be
thought
of
before
finally
using
a
goto
If
a
goto
must
be
used
forward
transfer
or
a
jump
to
a
later
statement
is
more
acceptable
than
a
backward
jump
Information
Hiding
As
discussed
earlier
information
hiding
should
be
supported
where
possible
Only
the
access
function
for
the
data
structure
should
be
made
visible
while
hiding
the
data
structure
behind
these
function
User-Defined
Types
Modern
language
allow
user
to
define
type
like
the
enumerated
type
When
such
facility
are
available
they
should
be
ex-
ploited
where
applicable
For
example
when
working
with
date
a
type
can
be
defined
for
the
day
of
the
week
Using
such
a
type
make
the
program
much
clearer
than
defining
code
for
each
day
and
then
working
with
code
Nesting
If
nesting
of
if-then-else
construct
becomes
too
deep
then
the
logic
become
harder
to
understand
In
case
of
deeply
nested
if-then-elses
it
is
often
difficult
to
determine
the
if
statement
to
which
a
particular
else
clause
is
associated
Where
possible
deep
nesting
should
be
avoided
even
if
it
mean
a
little
inefficiency
For
example
consider
the
following
construct
of
nested
if-then-elses
If
the
different
condition
are
disjoint
a
they
often
are
this
structure
can
be
converted
into
the
following
structure
This
sequence
of
statement
will
produce
the
same
result
a
the
earlier
sequence
if
the
condition
are
disjoint
but
it
is
much
easier
to
understand
The
price
is
a
little
inefficiency
Module
Size
We
discussed
this
issue
during
system
design
A
programmer
should
carefully
examine
any
function
with
too
many
statement
say
more
than
100
Large
module
often
will
not
be
functionally
cohesive
There
can
be
no
hard-and-fast
rule
about
module
size
the
guiding
principle
should
be
cohesion
and
coupling
Module
Interface
A
module
with
a
complex
interface
should
be
carefully
examined
As
a
rule
of
thumb
any
module
whose
interface
ha
more
than
five
parameter
should
be
carefully
examined
and
broken
into
multiple
module
with
a
simpler
interface
if
possible
Side
Effects
When
a
module
is
invoked
it
sometimes
ha
side
effect
of
modifying
the
program
state
beyond
the
modification
of
parameter
listed
in
the
module
interface
definition
for
example
modifying
global
variable
Such
side
effect
should
be
avoided
where
possible
and
if
a
module
ha
side
effect
they
should
be
properly
documented
Robustness
A
program
is
robust
if
it
doe
something
planned
even
for
exceptional
condition
A
program
might
encounter
exceptional
condition
in
such
form
a
incorrect
input
the
incorrect
value
of
some
variable
and
overflow
If
such
situation
do
arise
the
program
should
not
just
crash
or
core
dump
it
should
produce
some
meaningful
message
and
exit
gracefully
Switch
Case
with
Default
If
there
is
no
default
case
in
a
switch
statement
the
behavior
can
be
unpredictable
if
that
case
arises
at
some
point
of
time
which
wa
not
predictable
at
development
stage
Such
a
practice
can
result
in
a
bug
like
NULL
dereference
memory
leak
a
well
a
other
type
of
serious
bug
It
is
a
good
practice
to
always
include
a
default
case
s
0
=
y
/
NULL
dereference
if
default
occurs
/
Empty
Catch
Block
An
exception
is
caught
but
if
there
is
no
action
it
may
represent
a
scenario
where
some
of
the
operation
to
be
done
are
not
performed
Whenever
exception
are
caught
it
is
a
good
practice
to
take
some
default
action
even
if
it
is
just
printing
an
error
message
File
InputStream
fis
=
new
File
InputStream
``
InputFile
``
Empty
if
while
Statement
A
condition
is
checked
but
nothing
is
done
based
on
the
check
This
often
occurs
due
to
some
mistake
and
should
be
caught
Other
similar
error
include
empty
finally
try
synchronized
empty
static
method
etc
Such
useless
check
should
be
avoided
if
x
==
0
/
nothing
is
done
after
checking
x
/
else
Read
Return
to
Be
Checked
Often
the
return
value
from
read
is
not
checked
assuming
that
the
read
return
the
desired
value
Sometimes
the
result
from
a
read
can
be
different
from
what
is
expected
and
this
can
cause
failure
later
There
may
be
some
case
where
neglecting
this
condition
may
result
in
some
serious
error
For
example
if
read
from
scanf
is
more
than
expected
then
it
may
cause
a
buffer
overflow
Hence
the
value
of
read
should
be
checked
before
accessing
the
data
read
This
is
the
reason
why
most
language
provide
a
return
value
for
the
read
operation
Return
from
Finally
Block
One
should
not
return
from
finally
block
a
it
can
create
false
belief
For
example
consider
the
code
throw
new
Exception
``
An
Exception
``
In
this
example
a
value
is
returned
both
in
exception
and
nonexception
sce-
narios
Hence
at
the
caller
site
the
user
will
not
be
able
to
distinguish
between
the
two
Another
interesting
case
arises
when
we
have
a
return
from
try
block
In
this
case
if
there
is
a
return
in
finally
also
then
the
value
from
finally
is
returned
instead
of
the
value
from
try
Correlated
Parameters
Often
there
is
an
implicit
correlation
between
the
parameter
For
example
in
the
code
segment
given
below
length
rep-
resents
the
size
of
BUFFER
If
the
correlation
doe
not
hold
we
can
run
into
a
serious
problem
like
buffer
overflow
illustrated
in
the
code
fragment
below
Hence
it
is
a
good
practice
to
validate
this
correlation
rather
than
assuming
that
it
hold
In
general
it
is
desirable
to
do
some
counter
check
on
implicit
assumption
about
parameter
void
char
src
int
length
char
destn
strcpy
destn
src
/
Can
cause
buffer
overflow
Trusted
Data
Sources
Counter
check
should
be
made
before
accessing
the
input
data
particularly
if
the
input
data
is
being
provided
by
the
user
or
is
being
obtained
over
the
network
For
example
while
doing
the
string
copy
operation
we
should
check
that
the
source
string
is
null
terminated
or
that
it
size
is
a
we
expect
Similar
is
the
case
with
some
network
data
which
may
be
sniffed
and
prone
to
some
modification
or
corruption
To
avoid
problem
due
to
these
change
we
should
put
some
check
like
parity
check
hash
etc.
to
ensure
the
validity
of
the
incoming
data
Give
Importance
to
Exceptions
Most
programmer
tend
to
give
le
attention
to
the
possible
exceptional
case
and
tend
to
work
with
the
main
flow
of
event
control
and
data
Though
the
main
work
is
done
in
the
main
path
it
is
the
exceptional
path
that
often
cause
software
system
to
fail
To
make
a
software
system
more
reliable
a
programmer
should
consider
all
possibility
and
write
suitable
exception
handler
to
prevent
failure
or
loss
when
such
Programmers
spend
far
more
time
reading
code
than
writing
code
Over
the
life
of
the
code
the
author
spends
a
considerable
time
reading
it
during
debugging
and
enhancement
People
other
than
the
author
also
spend
considerable
effort
in
reading
code
because
the
code
is
often
maintained
by
someone
other
than
the
author
In
short
it
is
of
prime
importance
to
write
code
in
a
manner
that
is
easy
to
read
and
understand
Coding
standard
provide
rule
and
guideline
for
some
aspect
of
programming
in
order
to
make
code
easier
to
read
Most
organization
who
develop
software
regularly
develop
their
own
standard
In
general
coding
standard
provide
guideline
for
programmer
regarding
naming
file
organization
statement
and
declaration
and
layout
and
com-
ments
To
give
an
idea
of
coding
standard
often
called
convention
or
style
guideline
we
discus
some
guideline
for
Java
based
on
publicly
available
standard
from
or
java.sun.com/docs
Naming
Conventions
Some
of
the
standard
naming
convention
that
are
fol-
lowed
often
are
Package
name
should
be
in
lowercase
e.g.
mypackage
edu.iitk.maths
Type
name
should
be
noun
and
should
start
with
uppercase
e.g.
Day
DateOfBirth
EventHandler
Variable
name
should
be
noun
starting
with
lowercase
e.g.
name
amount
Constant
name
should
be
all
uppercase
e.g.
PI
MAX
ITERATIONS
Method
name
should
be
verb
starting
with
lowercase
e.g.
getValue
Private
class
variable
should
have
the
suffix
e.g.
private
int
value
Some
standard
will
require
this
to
be
a
prefix
Variables
with
a
large
scope
should
have
long
name
variable
with
a
small
scope
can
have
short
name
loop
iterators
should
be
named
i
j
k
etc
The
prefix
is
should
be
used
for
Boolean
variable
and
method
to
avoid
confusion
e.g.
isStatus
should
be
used
instead
of
status
negative
boolean
variable
name
e.g.
isNotCorrect
should
be
avoided
The
term
compute
can
be
used
for
method
where
something
is
being
com-
puted
the
term
find
can
be
used
where
something
is
being
looked
up
e.g.
computeMean
findMin
Exception
class
should
be
suffixed
with
Exception
e.g.
OutOfBoundEx-
ception
Files
There
are
convention
on
how
file
should
be
named
and
what
file
should
contain
such
that
a
reader
can
get
some
idea
about
what
the
file
con-
tains
Some
example
of
these
convention
are
Java
source
file
should
have
the
extension
.java—this
is
enforced
by
most
compiler
and
tool
Each
file
should
contain
one
outer
class
and
the
class
name
should
be
the
same
a
the
file
name
Line
length
should
be
limited
to
le
than
80
column
and
special
charac-
ters
should
be
avoided
If
the
line
is
longer
it
should
be
continued
and
the
continuation
should
be
made
very
clear
Statements
These
guideline
are
for
the
declaration
and
executable
state-
ments
in
the
source
code
Some
example
are
given
below
Note
however
that
not
everyone
will
agree
on
these
That
is
why
organization
generally
develop
their
own
guideline
that
can
be
followed
without
restricting
the
flexibility
of
programmer
for
the
type
of
work
the
organization
doe
Variables
should
be
initialized
where
declared
and
they
should
be
declared
in
the
smallest
possible
scope
Declare
related
variable
together
in
a
common
statement
Unrelated
vari-
ables
should
not
be
declared
in
the
same
statement
Loop
variable
should
be
initialized
immediately
before
the
loop
Avoid
complex
conditional
expressions—introduce
temporary
boolean
vari-
ables
instead
Commenting
and
Layout
Comments
are
textual
statement
that
are
meant
for
the
program
reader
to
aid
the
understanding
of
code
The
purpose
of
com-
ments
is
not
to
explain
in
English
the
logic
of
the
program—if
the
logic
is
so
complex
that
it
requires
comment
to
explain
it
it
is
better
to
rewrite
and
simplify
the
code
instead
In
general
comment
should
explain
what
the
code
is
doing
or
why
the
code
is
there
so
that
the
code
can
become
almost
stan-
dalone
for
understanding
the
system
Comments
should
generally
be
provided
for
block
of
code
and
in
many
case
only
comment
for
the
module
need
to
be
provided
Providing
comment
for
module
is
most
useful
a
module
form
the
unit
of
testing
compiling
verification
and
modification
Comments
for
a
module
are
often
called
prologue
for
the
module
which
describes
the
functionality
and
the
purpose
of
the
module
it
public
interface
and
how
the
module
is
to
be
used
parameter
of
the
interface
assumption
it
make
about
the
parameter
and
any
side
effect
it
ha
Other
feature
may
also
be
included
It
should
be
noted
that
prologue
are
useful
only
if
they
are
kept
consistent
with
the
logic
of
the
module
If
the
module
is
modified
then
the
prologue
should
also
be
modified
if
necessary
Java
provides
documentation
comment
that
are
delimited
by
/
...
/
and
which
could
be
extracted
to
HTML
file
These
comment
are
mostly
used
a
prologue
for
class
and
it
method
and
field
and
are
meant
to
provide
documentation
to
user
of
the
class
who
may
not
have
access
to
the
source
code
In
addition
to
prologue
for
module
coding
standard
may
specify
how
and
where
comment
should
be
located
Some
such
guideline
are
Single
line
comment
for
a
block
of
code
should
be
aligned
with
the
code
they
are
meant
for
There
should
be
comment
for
all
major
variable
explaining
what
they
rep-
resent
A
block
of
comment
should
be
preceded
by
a
blank
comment
line
with
just
/
and
ended
with
a
line
containing
just
/
Trailing
comment
after
statement
should
be
short
on
the
same
line
and
shifted
far
enough
to
separate
them
from
statement
Layout
guideline
focus
on
how
a
program
should
be
indented
how
it
should
use
blank
line
white
space
etc.
to
make
it
more
easily
readable
Indentation
guideline
are
sometimes
provided
for
each
type
of
programming
construct
However
most
programmer
learn
these
by
seeing
the
code
of
others
and
the
code
fragment
in
book
and
document
and
many
of
these
have
become
fairly
standard
over
the
year
We
will
not
discus
them
further
except
to
say
that
a
programmer
should
use
some
convention
and
use
them
consistently
The
coding
activity
start
when
some
form
of
design
ha
been
done
and
the
specification
of
the
module
to
be
developed
are
available
With
the
design
module
are
assigned
to
developer
for
coding
When
module
are
assigned
to
developer
they
use
some
process
for
developing
the
code
Clearly
a
wide
range
of
process
are
possible
for
achieving
this
goal
Here
we
discus
some
effective
process
that
developer
use
for
incrementally
developing
code
The
process
followed
by
many
developer
is
to
write
the
code
for
the
currently
assigned
module
and
when
done
perform
unit
testing
on
it
and
fix
the
bug
found
Then
the
code
is
checked
in
the
project
repository
to
make
it
available
to
others
in
the
project
We
will
explain
the
process
of
checking
in
later
A
better
process
for
coding
which
is
often
followed
by
experienced
developer
is
to
develop
the
code
incrementally
That
is
write
code
for
im-
plementing
only
part
of
the
functionality
of
the
module
This
code
is
compiled
and
tested
with
some
quick
test
to
check
the
code
that
ha
been
written
so
far
When
the
code
pass
these
test
the
developer
proceeds
to
add
further
functionality
to
the
code
which
is
then
tested
again
In
other
word
the
code
is
developed
incrementally
testing
it
a
it
is
built
This
coding
process
is
shown
in
Figure
The
basic
advantage
of
developing
code
incrementally
with
testing
being
done
after
every
round
of
coding
is
to
facilitate
debugging—an
error
found
in
some
testing
can
be
safely
attributed
to
code
that
wa
added
since
the
last
successful
testing
However
for
following
this
process
it
is
essential
that
testing
be
done
through
test
script
that
can
be
run
easily
With
these
test
script
testing
can
be
done
a
frequently
a
desired
and
new
test
case
can
be
added
easily
These
test
script
are
also
a
tremendous
aid
when
code
is
enhanced
in
the
future—through
the
test
script
it
can
be
quickly
checked
that
the
earlier
functionality
is
still
working
These
test
script
can
also
be
used
with
some
enhancement
for
the
final
unit
testing
that
is
often
done
before
checking
in
the
module
Test-Driven
Development
TDD
8
is
a
coding
process
that
turn
around
the
common
approach
to
coding
Instead
of
writing
code
and
then
developing
test
case
to
check
the
code
in
TDD
it
is
the
other
way
around—a
programmer
first
writes
the
test
script
and
then
writes
the
code
to
pas
the
test
The
whole
process
is
done
incrementally
with
test
being
written
based
on
the
specification
and
code
being
written
to
pas
the
test
The
TDD
process
is
shown
in
Figure
This
is
a
relatively
new
approach
which
ha
been
adopted
in
the
extreme
programming
XP
methodology
7
The
concept
of
TDD
is
however
general
and
not
tied
to
any
particular
methodology
A
few
point
are
worth
noting
about
TDD
First
the
approach
say
that
you
write
just
enough
code
to
pas
the
test
By
following
this
the
code
is
always
in
sync
with
the
test
This
is
not
always
the
case
with
the
code-first
approach
in
which
it
is
all
too
common
to
write
a
long
piece
of
code
but
then
only
write
a
few
test
which
cover
only
some
part
of
the
code
By
encouraging
that
code
is
written
only
to
pas
the
test
the
responsibility
of
ensuring
that
required
functionality
is
built
is
shifted
to
the
activity
of
designing
the
test
case
That
is
it
is
the
task
of
test
case
to
check
that
the
code
that
will
be
developed
ha
all
the
functionality
needed
This
writing
of
test
case
before
the
code
is
written
make
the
development
usage-driven
Since
test
case
have
to
be
written
first
from
the
specification
how
the
code
is
to
be
used
get
the
attention
first
This
help
ensure
that
the
interface
are
from
the
perspective
of
the
user
of
the
code
and
that
key
usage
scenario
This
can
help
reduce
interface
error
In
TDD
some
type
of
prioritization
for
code
development
naturally
hap-
pen
It
is
most
likely
that
the
first
few
test
are
likely
to
focus
on
using
the
main
functionality
Generally
the
test
case
for
lower-priority
feature
or
func-
tionality
will
be
developed
later
Consequently
code
for
high-priority
feature
will
be
developed
first
and
lower-priority
item
will
be
developed
later
This
ha
the
benefit
that
higher-priority
item
get
done
first
but
ha
the
drawback
that
some
of
the
lower-priority
feature
or
some
special
case
for
which
test
case
are
not
written
may
not
get
handled
in
the
code
As
the
code
is
written
to
satisfy
the
test
case
the
completeness
of
the
code
depends
on
the
thoroughness
of
the
test
case
Often
it
is
hard
and
tedious
to
write
test
case
for
all
the
scenario
or
special
condition
and
it
is
highly
unlikely
that
a
developer
will
write
test
case
for
all
the
special
case
In
TDD
a
the
goal
is
to
write
enough
code
to
pas
the
test
case
such
special
case
may
not
get
handled
Also
a
at
each
step
code
is
being
written
primarily
to
pas
the
test
it
may
later
be
found
that
earlier
algorithm
were
not
well
suited
In
that
case
the
code
should
be
improved
before
new
functionality
is
added
a
shown
in
Figure
Pair
programming
is
also
a
coding
process
that
ha
been
proposed
a
a
key
tech-
nique
in
extreme
programming
XP
methodology
7
In
pair
programming
code
is
not
written
by
individual
programmer
but
by
a
pair
of
programmer
That
is
the
coding
work
is
assigned
not
to
an
individual
but
to
a
pair
of
individual
This
pair
together
writes
the
code
The
process
envisaged
is
that
one
person
will
type
the
program
while
the
other
will
actively
participate
and
constantly
review
what
is
being
typed
When
error
are
noticed
they
are
pointed
out
and
corrected
When
needed
the
pair
discus
the
algorithm
data
structure
or
strategy
to
be
used
in
the
code
to
be
written
The
role
are
rotated
frequently
making
both
equal
partner
and
having
similar
role
The
basic
motivation
for
pair
programming
is
that
a
code
reading
and
code
review
have
been
found
to
be
very
effective
in
detecting
defect
by
having
a
pair
do
the
programming
we
have
the
situation
where
the
code
is
getting
reviewed
a
it
is
being
typed
That
is
instead
of
writing
code
and
then
getting
it
reviewed
by
another
programmer
we
have
a
programmer
who
is
constantly
reviewing
the
code
being
written
Like
incremental
development
and
testing
we
now
have
incremental
reviewing
taking
place
Besides
ongoing
code
review
having
two
programmer
apply
themselves
to
the
programming
task
at
hand
is
likely
to
result
in
better
decision
being
taken
about
the
data
structure
algorithm
interface
logic
etc
Special
condition
which
frequently
result
in
error
are
also
more
likely
to
be
dealt
with
in
a
better
manner
The
potential
drawback
of
pair
programming
is
that
it
may
result
in
loss
of
productivity
by
assigning
two
people
for
a
programming
task
It
is
clear
that
a
pair
will
produce
better
code
a
compared
to
code
being
developed
by
a
single
programmer
The
open
question
is
whether
this
increase
in
productivity
due
to
improved
code
quality
offset
the
loss
incurred
by
putting
two
people
on
a
task
There
are
also
issue
of
accountability
and
code
ownership
particularly
when
the
pair
are
not
fixed
and
rotate
a
ha
been
proposed
in
XP
Impact
of
pair
programming
is
an
active
area
of
research
particularly
for
experimental
software
engineering
During
the
coding
process
the
code
being
written
by
a
programmer
or
a
pair
evolves—starting
from
nothing
to
eventually
having
a
well-tested
module
During
this
process
the
code
undergoes
change
Besides
the
change
due
to
the
development
process
code
change
are
also
needed
due
to
change
in
module
specification
which
may
come
about
due
to
requirement
change
In
such
a
dynamic
scenario
managing
evolving
code
is
a
challenge
Here
we
discus
two
aspect
of
this—how
to
manage
the
different
version
that
get
created
and
how
to
maintain
code
quality
under
change
In
a
project
many
different
people
develop
source
code
Each
programmer
cre-
ate
different
source
file
which
are
eventually
combined
together
to
create
executables
Programmers
keep
changing
their
source
file
a
the
code
evolves
a
we
have
seen
in
the
process
discussed
above
and
often
make
change
in
other
source
file
a
well
In
order
to
keep
control
over
the
source
and
their
evolution
source
code
control
is
almost
always
used
in
project
using
tool
like
the
CVS
on
UNIX
www.cvshome.org
or
visual
source
safe
VSS
on
Windows
msdn.microsoft.com/vstudio/previous/ssafe
Here
we
give
a
brief
description
of
how
these
tool
are
used
in
the
coding
process
Our
discussion
is
based
on
CVS
A
modern
source
code
control
system
contains
a
repository
which
is
essen-
tially
a
controlled
directory
structure
which
keep
the
full
revision
history
of
all
the
file
produced
by
the
different
programmer
in
the
project
team
For
efficiency
a
file
history
is
generally
kept
a
delta
or
increment
from
the
base
file
This
allows
any
older
version
of
the
file
to
be
re-created
thereby
giving
the
flexibility
to
easily
discard
a
change
should
the
need
arise
The
repository
is
also
the
official
source
for
all
the
file
For
a
project
a
repository
ha
to
be
set
up
with
permission
for
different
people
in
the
project
The
file
the
repository
will
contain
are
also
specified—
these
are
the
file
whose
evolution
the
repository
maintains
Programmers
use
the
repository
to
make
their
source
file
change
available
a
well
a
obtain
other
source
file
Some
of
the
type
of
command
that
are
generally
performed
by
a
programmer
are
Get
a
local
copy
A
programmer
in
a
project
work
on
a
local
copy
of
the
file
Commands
are
provided
to
make
a
local
copy
from
the
repository
Making
a
local
copy
is
generally
called
a
checkout
An
example
command
is
cv
checkout
<
module
>
which
copy
a
set
of
file
that
belongs
to
the
<
module
>
on
the
local
machine
A
user
will
get
the
latest
copy
of
the
file
However
if
a
user
want
any
older
version
of
a
file
can
be
obtained
from
the
repository
a
the
complete
history
is
maintained
Many
user
can
check
out
a
file
Make
change
to
file
s
The
change
made
to
the
local
file
by
a
programmer
remain
local
until
the
change
are
committed
back
on
the
repository
By
committing
e.g.
by
cv
commit
<
file
>
the
change
made
to
the
local
file
are
made
to
the
repository
and
are
hence
available
to
others
This
operation
is
also
referred
to
a
check
in
Update
a
local
copy
Changes
committed
by
project
member
to
the
repository
are
not
reflected
in
the
local
copy
that
were
made
before
the
change
were
committed
For
getting
the
change
the
local
copy
of
the
file
have
to
be
updated
e.g.
by
cv
update
command
By
an
update
all
the
change
made
to
the
file
are
reflected
in
the
local
copy
Get
report
Source
control
tool
provide
a
host
of
command
to
provide
dif-
ferent
report
on
the
evolution
of
the
file
These
include
report
like
the
difference
between
the
local
file
and
the
latest
version
of
the
file
all
change
made
to
a
file
along
with
the
date
and
reason
for
change
which
are
typ-
ically
provided
while
committing
a
change
Note
that
once
the
change
are
committed
they
become
available
to
all
member
of
the
team
who
are
supposed
to
use
the
source
file
from
the
reposi-
tory
Hence
it
is
essential
that
a
programmer
commits
a
source
file
only
when
it
is
in
a
state
that
is
usable
by
others
The
normal
behavior
of
a
project
member
will
be
a
follows
check
out
the
latest
version
of
the
file
to
be
changed
make
the
planned
change
to
them
validate
that
the
change
have
the
desired
effect
for
which
all
the
file
may
be
copied
and
the
system
tried
out
locally
commit
the
change
back
to
the
repository
It
should
be
clear
that
if
two
people
check
out
some
file
and
then
make
change
there
is
a
possibility
of
a
conflict—different
change
are
made
to
the
same
part
of
the
file
All
tool
will
detect
the
conflict
when
the
second
person
try
to
commit
the
change
and
will
inform
the
user
The
user
ha
to
manually
resolve
the
conflit
i.e.
make
the
file
such
that
the
change
do
not
conflict
with
existing
change
and
then
commit
the
file
Conflicts
are
usually
rare
a
they
occur
only
if
different
change
are
made
to
the
same
line
in
a
file
With
a
source
code
control
system
a
programmer
doe
not
need
to
maintain
all
the
versions—at
any
time
if
some
change
need
to
be
undone
older
version
can
be
easily
recovered
The
repository
are
always
backed
up
so
they
also
provide
protection
against
accidental
loss
Furthermore
a
record
of
change
is
maintained—who
made
the
change
and
when
why
wa
the
change
made
what
were
the
actual
change
etc
Most
importantly
the
repository
provides
a
central
place
for
the
latest
and
authoritative
file
of
the
project
This
is
invaluable
for
product
that
have
a
long
life
and
that
evolve
over
many
year
Besides
using
the
repository
for
maintaining
the
different
version
it
is
also
used
for
constructing
the
software
system
from
the
sources—an
activity
often
called
build
The
build
get
the
latest
version
or
the
desired
version
number
of
the
source
from
the
repository
and
creates
the
executables
from
the
source
Building
the
final
executables
from
the
source
file
is
often
done
through
tool
like
the
Makefile
34
which
specify
the
dependence
between
file
and
how
the
final
executables
are
constructed
from
the
source
file
These
tool
are
capable
of
recognizing
that
file
have
changed
and
will
recompile
whenever
file
are
changed
for
creating
the
executables
With
source
code
control
these
tool
will
generally
get
the
latest
copy
from
the
repository
then
use
it
for
creating
What
we
discussed
here
is
one
of
the
simplest
approach
to
source
code
control
and
build
Often
when
large
system
are
being
built
more
elaborate
method
for
source
code
control
may
be
needed
We
have
seen
that
coding
often
involves
making
change
to
some
existing
code
Code
also
change
when
requirement
change
or
when
new
functionality
is
added
Due
to
the
change
being
done
to
module
even
if
we
started
with
a
good
design
with
time
we
often
end
up
with
code
whose
design
is
not
a
good
a
it
could
be
And
once
the
design
embodied
in
the
code
becomes
complex
then
enhancing
the
code
to
accommodate
required
change
becomes
more
complex
time
consuming
and
error
prone
In
other
word
the
productivity
and
quality
start
decreasing
Refactoring
is
the
technique
to
improve
existing
code
and
prevent
this
design
decay
with
time
Refactoring
is
part
of
coding
in
that
it
is
performed
during
the
coding
activity
but
is
not
regular
coding
Refactoring
ha
been
practiced
in
the
past
by
programmer
but
recently
it
ha
taken
a
more
concrete
shape
and
ha
been
proposed
a
a
key
step
in
the
XP
practice
7
Refactoring
also
play
an
important
role
in
test-driven
development—code
improvement
step
in
the
TDD
process
is
really
doing
refactoring
Here
we
discus
some
key
concept
and
some
method
for
doing
refactoring
36
More
detail
are
available
on
Refactoring
is
defined
a
a
change
made
to
the
internal
structure
of
software
to
make
it
easier
to
understand
and
cheaper
to
modify
without
changing
it
observable
behavior
36
A
key
point
here
is
that
the
change
is
being
made
to
the
design
embodied
in
the
source
code
i.e.
the
internal
structure
exclusively
for
improvement
purpose
The
basic
objective
of
refactoring
is
to
improve
the
design
However
note
that
this
is
not
about
improving
a
design
during
the
design
stage
for
creating
a
design
which
is
to
be
later
implemented
which
is
the
focus
of
design
method-
ology
but
about
improving
the
design
of
code
that
already
exists
In
other
word
refactoring
though
done
on
source
code
ha
the
objective
of
improving
the
design
that
the
code
implement
Therefore
the
basic
principle
of
design
guide
the
refactoring
process
Consequently
a
refactoring
generally
result
in
one
or
more
of
the
following
Refactoring
involves
changing
the
code
to
improve
one
of
the
design
proper-
tie
while
keeping
the
external
behavior
the
same
Refactoring
is
often
triggered
by
some
coding
change
that
have
to
be
done
If
some
enhancement
are
to
be
made
to
the
existing
code
and
it
is
felt
that
if
the
code
structure
wa
different
better
then
the
change
could
have
been
done
easier
that
is
the
time
to
do
refactoring
to
improve
the
code
structure
Even
though
refactoring
is
triggered
by
the
need
to
change
the
software
and
it
external
behavior
it
should
not
be
confused
or
mixed
with
the
change
for
enhancement
It
is
best
to
keep
these
two
type
of
change
separate
So
while
developing
code
if
refactoring
is
needed
the
programmer
should
cease
to
write
new
functionality
and
first
do
the
refactoring
and
then
add
new
code
The
main
risk
of
refactoring
is
that
existing
working
code
may
break
due
to
the
change
being
made
This
is
the
main
reason
why
often
refactoring
is
not
done
The
other
reason
is
that
it
may
be
viewed
a
an
additional
and
unnecessary
cost
To
mitigate
this
risk
the
two
golden
rule
are
Have
test
script
available
to
test
existing
functionality
If
a
good
test
suite
is
available
then
whether
refactoring
preserve
existing
functionality
can
be
checked
easily
Refactoring
can
not
be
done
effectively
with-
out
an
automated
test
suite
a
without
such
a
suite
determining
if
the
external
behavior
ha
changed
or
not
will
become
a
costly
affair
By
doing
refactoring
in
a
series
of
small
step
and
testing
after
each
step
mistake
in
refactoring
can
be
easily
identified
and
rectified
With
this
each
refactoring
make
only
a
small
change
but
a
series
of
refactorings
can
significantly
transform
the
program
structure
With
refactoring
code
becomes
continuously
improving
That
is
the
design
rather
than
decaying
with
time
evolves
and
improves
with
time
With
refac-
toring
the
quality
of
the
design
improves
making
it
easier
to
make
change
to
the
code
a
well
a
find
bug
The
extra
cost
of
refactoring
is
paid
for
by
the
saving
achieved
later
in
reduced
testing
and
debugging
cost
higher
quality
and
reduced
effort
in
making
change
If
refactoring
is
to
be
practiced
it
usage
can
also
ease
the
design
task
in
the
design
stage
Often
the
designer
spend
considerable
effort
in
trying
to
make
the
design
a
good
a
possible
try
to
think
of
future
change
and
try
to
make
the
design
flexible
enough
to
accommodate
all
type
of
future
change
they
can
envisage
This
make
the
design
activity
very
complex
and
often
result
in
complex
design
With
refactoring
the
designer
doe
not
have
to
be
terribly
worried
about
making
the
best
or
most
flexible
design—the
goal
is
to
try
to
come
up
with
a
good
and
simple
design
And
later
if
new
change
are
required
that
were
not
thought
of
before
or
if
shortcoming
are
found
in
the
design
the
design
is
changed
through
refactoring
More
often
than
not
the
extra
flexibility
envisaged
and
designed
is
never
needed
resulting
in
a
system
that
is
unduly
complex
Note
that
refactoring
is
not
a
technique
for
bug
fixing
or
for
improving
code
that
is
in
very
bad
shape
It
is
done
to
code
that
is
mostly
working—the
basic
purpose
is
to
make
the
code
live
longer
by
making
it
structure
healthier
It
start
with
healthy
code
and
instead
of
letting
it
become
weak
it
continues
to
keep
it
healthy
When
is
refactoring
needed
There
are
some
easy-to-spot
sign
in
the
code
which
are
sometimes
called
bad
smell
36
that
often
indicate
that
some
of
the
desirable
design
property
may
be
getting
violated
or
that
there
is
potential
of
improving
the
design
In
other
word
if
you
smell
one
of
these
bad
smell
it
may
be
a
sign
that
refactoring
is
needed
Some
of
these
bad
smell
from
36
are
given
here
Duplicate
Code
This
is
quite
common
One
reason
for
this
is
that
some
small
functionality
is
being
executed
at
multiple
place
e.g.
the
age
from
date
of
birth
may
be
computed
in
each
place
that
need
the
date
Another
common
reason
is
that
when
there
are
multiple
subclass
of
a
class
then
each
subclass
may
end
up
doing
a
similar
thing
Duplicate
code
mean
that
if
this
logic
or
function
ha
to
be
changed
it
will
have
to
be
changed
in
all
the
place
it
exists
making
change
much
harder
and
costlier
Long
Method
If
a
method
is
large
it
often
represents
the
situation
where
it
is
trying
to
do
too
many
thing
and
therefore
is
not
cohesive
Long
Class
Similarly
a
large
class
may
indicate
that
it
is
encapsulating
multiple
concept
making
the
class
not
cohesive
Long
Parameter
List
Complex
interface
are
clearly
not
desirable—they
make
the
code
harder
to
understand
Often
the
complexity
is
not
intrinsic
but
a
sign
of
improper
design
Switch
Statements
In
object-oriented
program
if
the
polymorphism
is
not
being
used
properly
it
is
likely
to
result
in
a
switch
statement
everywhere
the
behavior
is
to
be
different
depending
on
the
property
Presence
of
simi-
lar
switch
statement
in
different
place
is
a
sign
that
instead
of
using
class
hierarchy
switch
statement
is
being
used
Presence
of
switch
statement
make
it
much
harder
to
extend
code—if
a
new
category
is
to
be
added
all
the
switch
statement
will
have
to
be
modified
Speculative
Generality
Some
class
hierarchy
may
exist
because
the
object
in
subclass
seem
to
be
different
However
if
the
behavior
of
object
of
the
different
subclass
is
the
same
and
there
is
no
immediate
reason
to
think
that
behavior
might
change
then
it
is
a
case
of
unnecessary
complexity
Too
Much
Communication
Between
Objects
If
method
in
one
class
are
making
many
call
to
method
of
another
object
to
find
out
about
it
state
this
is
a
sign
of
strong
coupling
It
is
possible
that
this
may
be
unnecessary
and
hence
such
situation
should
be
examined
for
refactoring
Message
Chaining
One
method
call
another
method
which
simply
pass
this
call
to
another
object
and
so
on
This
chain
potentially
result
in
unnecessary
coupling
These
bad
smell
in
general
are
indicative
of
a
poor
design
Clearly
there
are
unlimited
possibility
of
how
code
can
be
refactored
to
improve
it
design
There
are
however
some
standard
refactorings
that
can
be
applied
in
many
of
the
situation
For
a
catalog
of
common
refactorings
and
step
for
performing
each
of
them
the
reader
is
referred
to
36
Once
a
programmer
ha
written
the
code
for
a
module
it
ha
to
be
verified
before
it
is
used
by
others
Testing
remains
the
most
common
method
of
this
verification
At
the
programmer
level
the
testing
done
for
checking
the
code
the
programmer
ha
developed
a
compared
to
checking
the
entire
software
system
is
called
unit
testing
Unit
testing
is
like
regular
testing
where
program
are
executed
with
some
test
case
except
that
the
focus
is
on
testing
smaller
program
or
module
which
are
typically
assigned
to
one
programmer
or
a
pair
for
coding
In
the
programming
process
we
discussed
earlier
the
testing
wa
essentially
unit
testing
A
unit
may
be
a
function
or
a
small
collection
of
function
for
proce-
dural
language
or
a
class
or
a
small
collection
of
class
for
object-oriented
language
Testing
of
module
or
software
system
is
a
difficult
and
challenging
task
Selection
of
test
case
is
a
key
issue
in
any
form
of
testing
We
will
discus
the
problem
of
test
case
selection
in
detail
in
the
next
chapter
when
we
discus
testing
For
now
it
suffices
that
during
unit
testing
the
tester
who
is
generally
the
programmer
will
execute
the
unit
with
a
variety
of
test
case
and
study
the
actual
behavior
of
the
unit
being
tested
for
these
test
case
Based
on
the
behavior
the
tester
decides
whether
the
unit
is
working
correctly
or
not
If
the
behavior
is
not
a
expected
for
some
test
case
then
the
programmer
find
the
defect
in
the
program
an
activity
called
debugging
and
fix
it
After
removing
the
defect
the
programmer
will
generally
execute
the
test
case
that
caused
the
unit
to
fail
again
to
ensure
that
the
fixing
ha
indeed
made
the
unit
behave
correctly
An
issue
with
unit
testing
is
that
a
the
unit
being
tested
is
not
a
com-
plete
system
but
just
a
part
it
is
not
executable
by
itself
Furthermore
in
it
execution
it
may
use
other
module
that
have
not
been
developed
yet
Due
to
this
unit
testing
often
requires
driver
or
stub
to
be
written
Drivers
play
the
role
of
the
calling
module
and
are
often
responsible
for
getting
the
test
data
executing
the
unit
with
the
test
data
and
then
reporting
the
result
Stubs
are
essentially
dummy
module
that
are
used
in
place
of
the
actual
module
to
facilitate
unit
testing
So
if
a
module
M
us
service
from
another
module
N
that
ha
not
yet
been
developed
then
for
unit
testing
M
some
stub
for
N
will
have
to
be
written
so
M
can
invoke
the
service
in
some
manner
on
N
so
that
unit
testing
can
proceed
The
need
for
stub
can
be
avoided
if
coding
and
testing
proceeds
in
a
bottom-up
manner—the
module
at
lower
level
are
coded
and
tested
first
such
that
when
module
at
higher
level
of
hierarchy
are
tested
the
code
for
lower-level
module
is
already
available
If
incremental
coding
is
practiced
a
discussed
above
then
unit
testing
need
to
be
performed
every
time
the
programmer
add
some
code
Clearly
it
will
be
much
more
efficient
if
instead
of
executing
the
unit
and
giving
the
input
manually
the
execution
of
test
case
is
automated
Then
test
case
can
be
executed
easily
every
time
testing
need
to
be
done
Some
tool
are
available
to
facilitate
this
Here
we
discus
some
approach
for
unit
testing
using
testing
framework
In
the
previous
chapter
we
have
seen
that
when
using
procedural
unit
for
module
a
program
can
be
viewed
a
a
structure
chart
in
which
node
are
function
and
edge
represent
a
calling
relationship
In
unit
testing
one
or
a
small
collection
of
these
module
is
to
be
tested
with
a
set
of
test
case
As
the
behavior
of
a
module
depends
on
the
value
of
it
parameter
a
well
a
the
overall
state
of
the
system
of
which
it
is
a
part
e.g.
state
of
global
variable
a
test
case
for
a
module
f
will
involve
setting
both
the
state
of
the
system
on
which
the
behavior
of
f
depends
a
well
a
the
value
of
parameter
The
actual
value
and
the
state
of
the
system
for
a
test
case
depend
on
the
purpose
for
which
the
tester
ha
designed
this
test
case
In
addition
if
a
module
f
ha
other
module
below
it
in
the
structure
chart
that
is
the
module
call
other
module
say
g
and
h
then
for
executing
f
we
must
have
code
for
g
and
h
also
available
This
is
handled
in
two
way
First
is
to
test
in
a
bottom-up
manner
i.e.
test
the
module
at
the
bottom
of
the
structure
chart
first
and
then
move
up
In
this
approach
when
testing
f
the
tested
code
for
g
and
h
will
be
available
and
will
be
used
during
testing
The
other
approach
is
to
write
stub
for
g
and
h
Stubs
are
throwaway
code
written
for
the
called
function
only
to
facilitate
testing
of
the
caller
function
Often
the
stub
for
a
function
will
just
print
some
statement
if
no
value
is
to
be
returned
If
some
value
is
to
be
returned
for
the
caller
to
proceed
then
often
a
few
value
will
be
hard
coded
in
the
stub
and
then
the
caller
will
ensure
that
the
stub
is
called
with
the
value
for
which
it
ha
been
coded
We
will
assume
that
the
former
approach
is
being
followed
that
is
for
testing
f
tested
code
for
g
and
h
is
already
available
Testing
of
a
module
f
by
a
test
case
will
then
involve
the
following
step
Declare
whether
the
test
case
ha
succeeded
or
failed
The
simplest
and
commonly
used
approach
for
executing
this
sequence
of
step
is
to
write
a
main
program
which
executes
the
first
three
step
with
the
value
being
given
a
input
by
the
tester
or
read
from
a
file
or
hard
coded
in
the
program
and
then
print
out
the
important
value
The
programmer
then
executes
the
last
two
step
namely
comparing
the
result
with
expected
and
deciding
whether
the
test
ha
succeeded
or
failed
Due
to
the
need
of
programmer
intervention
for
evaluating
the
output
and
possibly
also
for
giving
input
this
approach
is
not
easy
to
scale
Once
a
test
case
is
designed
this
sequence
of
step
remains
fixed
and
hence
is
ideal
for
complete
automation
In
a
testing
framework
often
a
test
case
will
be
declared
a
a
function
in
which
this
sequence
of
step
is
executed
for
that
test
case
including
the
checking
of
the
outcome
and
declaring
the
result—this
being
done
often
with
the
help
of
assert
statement
provided
by
the
framework
A
test
suite
is
then
a
collection
of
these
function
and
execution
of
a
test
suite
mean
each
of
the
function
is
executed
The
test
suite
succeeds
if
all
the
test
case
succeed
If
a
test
case
fails
then
the
test
framework
will
decide
whether
to
continue
executing
or
stop
A
function
for
a
test
case
can
be
easily
defined—it
will
execute
the
sequence
of
step
given
above
However
executing
the
module
under
test
with
the
test
case
is
not
straightforward
a
a
suitable
executable
will
have
to
be
formed
which
can
execute
the
function
representing
the
test
case
and
report
the
result
about
success
or
failure
And
one
would
like
to
do
this
without
changing
the
module
being
tested
or
the
file
in
which
this
module
is
contained
If
the
source
code
file
remain
unchanged
during
unit
testing
then
the
integration
process
remains
unaffected
by
unit
testing
Otherwise
after
unit
testing
any
change
made
to
the
file
will
have
to
be
removed
This
type
of
unit
testing
is
facilitated
by
testing
framework
For
a
procedural
language
like
C
some
common
unit
testing
framework
are
CuTest
CUnit
Cutest
Check
etc
With
these
test
framework
each
test
case
is
defined
a
a
function
The
function
end
with
some
assertion
provided
by
the
framework
which
test
for
some
condition
to
declare
whether
the
test
ha
failed
or
succeeded
Each
function
representing
a
unit
test
is
added
to
an
array
or
a
structure
which
is
the
test
suite
Multiple
test
suite
can
also
be
created
Often
there
is
one
driver
function
to
which
these
suite
are
passed
and
which
then
executes
all
the
test
case
In
object-oriented
program
the
unit
to
be
tested
is
usually
an
object
of
a
class
Testing
of
object
can
be
defined
a
the
process
of
exercising
the
routine
provided
by
an
object
with
the
goal
of
uncovering
error
in
the
implementation
of
the
routine
or
state
of
the
object
or
both
78
To
test
a
class
the
programmer
need
to
create
an
object
of
that
class
take
the
object
to
a
particular
state
invoke
a
method
on
it
and
then
check
whether
the
state
of
the
object
is
a
expected
This
sequence
ha
to
be
executed
many
time
for
a
method
and
ha
to
be
performed
for
all
the
method
All
this
is
facilitated
if
we
use
framework
like
the
Junit
www.junit.org
Though
Junit
itself
is
for
Java
similar
framework
have
been
developed
for
other
language
like
C++
and
C
#
Here
we
briefly
describe
how
Junit
can
be
used
for
testing
a
class
and
give
an
example
For
testing
of
a
class
CUT
class
under
test
with
Junit
the
tester
ha
to
create
another
class
which
inherits
from
Junit
e.g.
class
CUTtest
extends
Junit
The
Junit
framework
ha
to
be
imported
by
this
class
This
class
is
the
driver
for
testing
CUT
It
must
have
a
constructor
in
which
the
object
that
are
needed
for
the
test
case
are
created
a
setUp
method
which
is
typically
used
for
creating
any
object
and
setting
up
value
before
executing
a
test
case
a
suite
and
a
main
that
executes
the
suite
using
a
TestRunner
provided
by
Junit
Besides
these
method
all
other
method
are
actually
test
case
Most
of
these
method
are
often
named
testxxxx
Such
a
method
typ-
ically
focus
on
testing
a
method
under
some
state
typically
the
name
of
the
method
and/or
the
state
is
contained
in
xxx
This
method
first
set
up
the
state
if
not
already
setup
by
setup
and
then
executes
the
method
to
be
tested
To
check
the
result
Junit
provides
two
special
method
Assert-
True
boolean
expression
and
AssertFalse
boolean
expression
By
using
func-
tions
and
having
a
logical
expression
on
the
state
of
the
object
the
tester
can
test
if
the
state
is
correct
or
not
If
all
the
assertion
in
all
the
method
succeed
then
Junit
declares
that
the
test
ha
passed
If
any
assert
statement
fail
Junit
declares
that
testing
ha
failed
and
specifies
the
assertion
that
ha
failed
To
get
an
idea
of
how
it
work
consider
the
testing
of
a
class
Matrix.java
which
provides
standard
operation
on
matrix
The
main
attribute
of
the
class
and
the
main
method
are
given
in
Figure
For
unit
testing
the
Matrix
class
clearly
we
need
to
test
standard
operation
like
creation
of
a
matrix
setting
of
value
etc
We
also
need
to
test
whether
the
operation
like
add
subtract
multiply
determinant
are
performing
a
expected
Each
test
case
we
want
to
execute
is
programmed
by
setting
the
value
and
then
performing
the
operation
The
result
of
the
operation
is
checked
through
the
assert
statement
For
example
for
testing
add
we
create
a
method
testAdd
private
double
matrix
//Matrix
element
private
int
row
col
//Order
of
Matrix
public
Matrix
int
i
int
j
//
Sets
#
row
and
#
col
public
Matrix
int
i
int
j
double
a
//
Sets
from
2D
array
public
Matrix
Matrix
a
//Constructs
matrix
from
another
public
void
read
//read
elts
from
console
and
set
up
matrix
public
void
setElement
int
i
int
j
double
value
//
set
elt
i
j
public
int
noOfRows
//
return
no
of
row
public
int
noOfCols
//
return
no
of
col
public
Matrix
add
Matrix
a
//
add
a
to
matrix
public
Matrix
sub
Matrix
a
//
subtracts
a
from
matrix
public
Matrix
mul
Matrix
b
//
multiplies
b
to
matrix
public
Matrix
transpose
//
transpose
the
matrix
public
Matrix
minor
int
a
int
b
//
return
a
x
b
sub-matrix
public
double
determinant
//
determinant
of
the
matrix
public
Matrix
inverse
throw
Exception
//
inverse
of
the
matrix
public
void
print
//
print
matrix
on
console
public
boolean
equal
Matrix
m
//
check
for
equality
with
m
in
which
a
matrix
is
added
to
another
The
correct
result
is
stored
a
priori
in
another
matrix
After
addition
it
is
checked
if
the
result
obtained
by
performing
add
is
equal
to
the
correct
result
The
method
for
this
is
shown
in
Figure
The
programmer
may
want
to
perform
more
test
for
add
for
which
more
test
method
will
be
needed
Similarly
method
are
written
for
testing
other
method
Some
of
these
test
are
also
shown
in
Figure
As
we
can
see
Junit
encourages
automated
testing
Not
only
is
the
execu-
tion
of
test
case
automated
the
checking
of
the
result
is
also
automated
This
make
running
test
fully
automatic
By
building
testing
script
and
continu-
ously
updating
them
a
the
class
evolves
we
always
have
a
test
script
which
can
be
run
quickly
So
whenever
we
make
any
change
to
the
code
we
can
quickly
check
if
the
past
test
case
are
running
on
the
click
of
a
button
This
becomes
almost
essential
if
incremental
coding
or
test-driven
development
discussed
earlier
in
the
chapter
is
to
be
practiced
public
class
MatrixTest
extends
TestCase
Matrix
A
B
C
D
E
re
/
test
matrix
/
public
MatrixTest
String
testcase
double
a
=
new
double
9
,6
7
,5
A
=
new
Matrix
2
,2
a
double
b
=
new
double
16
,21
3
,12
B
=
new
Matrix
2
,2
b
double
d
=
new
double
2
,2
,3
4
,8
,6
7
,8
,9
res=new
Matrix
double
c
=
new
double
25
,27
10
,17
C
=
new
Matrix
2
,2
c
res=A
add
B
assertTrue
re
=
null
assertTrue
C.
equal
re
C=new
Matrix
2
,2
for
int
i=0
i
<
2
i++
for
int
j=0
j
<
2
j++
C.
setElement
i
j
A.
getElement
i
j
double
c
=
new
double
162
,261
127
,207
C
=
new
Matrix
2
,2
c
res=A
mul
B
assertTrue
re
=
null
assertTrue
C.
equal
re
res=A
transpose
res=res
transpose
assertTrue
re
equal
A
double
dd
=
new
double
1
,0
0
,1
Matrix
DD=new
Matrix
2
,2
dd
assertTrue
re
equal
DD
Code
inspection
is
another
technique
that
is
often
applied
at
the
unit
level
It
can
be
viewed
a
static
testing
in
which
defect
are
detected
in
the
code
not
by
executing
the
code
but
through
a
manual
process
Code
inspection
unlike
testing
is
applied
almost
entirely
at
the
unit
level
i.e.
only
program
unit
are
subjected
to
inspection
Hence
we
consider
it
a
another
form
of
unit
testing
However
in
practice
often
both
are
employed
particularly
for
critical
module
It
should
be
pointed
out
that
inspection
is
a
general
verification
approach
that
can
be
applied
for
detecting
defect
in
any
document
However
it
wa
first
utilized
for
detecting
defect
in
the
code
and
code
inspection
remain
even
today
an
industry
best
practice
which
is
widely
employed
Inspections
have
been
found
to
help
in
improving
not
only
quality
but
also
productivity
see
report
in
39
44
83
Code
inspection
were
first
proposed
by
Fagan
31
32
Now
there
are
book
on
the
topic
which
describe
in
great
detail
how
Code
inspection
is
a
review
of
code
by
a
group
of
peer
following
a
clearly
defined
process
The
basic
goal
of
inspection
is
to
improve
the
quality
of
code
by
finding
defect
Some
of
the
key
characteristic
of
inspection
are
Code
inspection
is
conducted
by
programmer
and
for
programmer
It
is
a
structured
process
with
defined
role
for
the
participant
The
focus
is
on
identifying
defect
not
fixing
them
Inspection
data
is
recorded
and
used
for
monitoring
the
effectiveness
of
the
inspection
process
Inspections
are
performed
by
a
team
of
reviewer
or
inspector
including
the
author
with
one
of
them
being
the
moderator
The
moderator
ha
the
overall
responsibility
to
ensure
that
the
review
is
done
in
a
proper
manner
and
all
step
in
the
review
process
are
followed
Most
method
for
inspection
are
similar
with
minor
variation
Here
we
discus
the
inspection
process
employed
by
a
commercial
organization
58
The
different
stage
in
this
process
are
planning
self-review
group
review
meeting
and
rework
and
follow-up
These
stage
are
generally
executed
in
a
linear
order
We
discus
each
of
these
phase
now
The
objective
of
the
planning
phase
is
to
prepare
for
inspection
An
inspection
team
is
formed
which
should
include
the
programmer
whose
code
is
to
be
reviewed
The
team
should
consist
of
at
least
three
people
though
sometimes
four-or-five
member
team
are
also
formed
A
moderator
is
appointed
The
author
of
the
code
ensures
that
the
code
is
ready
for
inspection
and
that
the
entry
criterion
are
satisfied
Commonly
used
entry
criterion
are
that
the
code
compiles
correctly
and
the
available
static
analysis
tool
have
been
applied
The
moderator
check
that
the
entry
criterion
are
satisfied
by
the
code
A
package
is
prepared
that
is
to
be
distributed
to
the
inspection
team
The
package
typically
consists
of
code
to
be
inspected
the
specification
for
which
the
code
wa
developed
and
the
checklist
that
should
be
used
for
inspection
The
package
for
review
is
given
to
the
reviewer
The
moderator
may
ar-
range
an
opening
meeting
if
needed
in
which
the
author
may
provide
a
brief
overview
of
the
product
and
any
special
area
that
need
to
be
looked
at
care-
fully
A
checklist
that
should
be
used
during
the
inspection
may
be
prepared
or
some
ready
checklist
may
be
used
As
the
aim
of
code
inspection
is
to
improve
quality
in
addition
to
coding
defect
there
are
other
quality
issue
which
code
inspection
usually
look
for
like
efficiency
compliance
to
coding
standard
etc
The
type
of
defect
the
code
inspection
should
focus
on
is
contained
in
a
checklist
that
is
provided
to
the
inspector
In
this
phase
each
reviewer
doe
a
self-review
of
the
code
During
the
self-
review
a
reviewer
go
through
the
entire
code
and
log
all
the
potential
defect
he
or
she
find
in
the
self-preparation
log
Often
the
reviewer
will
mark
the
defect
on
the
work
product
itself
and
may
provide
a
summary
of
the
self-review
in
the
log
The
reviewer
also
record
the
time
they
spent
in
the
self-review
A
standard
form
may
be
used
for
the
self-preparation
log
an
example
form
is
shown
in
Figure
58
Ideally
the
self-review
should
be
done
in
one
continuous
time
span
The
recommended
time
is
le
than
two
hours—that
is
the
work
product
is
small
enough
that
it
can
be
fully
examined
in
le
than
two
hour
This
phase
of
the
review
process
end
when
all
reviewer
have
properly
performed
their
self-
review
and
filled
the
self-review
log
The
basic
purpose
of
the
group
review
meeting
is
to
come
up
with
the
final
defect
list
based
on
the
initial
list
of
defect
reported
by
the
reviewer
and
the
new
one
found
during
the
discussion
in
the
meeting
The
entry
criterion
for
this
step
is
that
the
moderator
is
satisfied
that
all
the
reviewer
are
ready
for
the
meeting
The
main
output
of
this
phase
are
the
defect
log
and
the
defect
summary
report
The
moderator
first
check
to
see
if
all
the
reviewer
are
prepared
This
is
done
by
a
brief
examination
of
the
effort
and
defect
data
in
the
self-review
log
to
confirm
that
sufficient
time
and
attention
ha
gone
into
the
prepara-
tion
When
preparation
is
not
adequate
the
group
review
is
deferred
until
all
participant
are
fully
prepared
If
everything
is
ready
the
group
review
meeting
is
held
The
moderator
is
in
charge
of
the
meeting
and
ha
to
make
sure
that
the
meeting
stay
focused
on
it
basic
purpose
of
defect
identification
and
doe
not
degenerate
into
a
general
brainstorming
session
or
personal
attack
on
the
author
The
meeting
is
conducted
a
follows
A
team
member
called
the
reader
go
over
the
code
line
by
line
or
any
other
convenient
small
unit
and
para-
phrase
each
line
to
the
team
Sometimes
no
paraphrasing
is
done
and
the
team
just
go
over
the
code
line
by
line
At
any
line
if
any
reviewer
ha
any
issue
from
before
or
find
any
new
issue
in
the
meeting
while
listening
to
others
the
reviewer
raise
the
issue
There
could
be
a
discussion
on
the
issue
raised
The
author
accepts
the
issue
a
a
defect
or
clarifies
why
it
is
not
a
defect
After
discussion
an
agreement
is
reached
and
one
member
of
the
review
team
called
the
scribe
record
the
identified
defect
in
the
defect
log
At
the
end
of
the
meeting
the
scribe
read
out
the
defect
recorded
in
the
defect
log
for
a
final
review
by
the
team
member
Note
that
during
the
entire
process
of
review
defect
are
only
identified
It
is
not
the
purpose
of
the
group
to
identify
solutions—that
is
done
later
by
the
author
The
final
defect
log
is
the
official
record
of
the
defect
identified
in
the
inspection
and
may
also
be
used
to
track
the
defect
to
closure
For
analyzing
the
effectiveness
of
an
inspection
however
only
summary-level
information
is
needed
for
which
a
summary
report
is
prepared
The
summary
report
describes
the
code
the
total
effort
spent
and
it
breakup
in
the
different
review
process
activity
total
number
of
defect
found
for
each
category
and
size
If
type
of
defect
were
also
recorded
then
the
number
of
defect
in
each
category
can
also
be
recorded
in
the
summary
A
partially
filled
summary
report
is
shown
in
Figure
58
The
summary
report
is
self-explanatory
Total
number
of
minor
defect
found
wa
8
and
the
total
number
of
major
defect
found
wa
3
That
is
the
defect
density
found
is
8/0.250
=
3
and
3/0.25
=
1
From
experience
both
of
these
rate
are
within
the
range
seen
in
the
past
hence
it
can
be
assumed
that
the
review
wa
conducted
properly
The
review
team
had
and
each
had
spent
about
1
..
This
mean
that
the
preparation
rate
wa
about
180
LOC
per
hour
and
the
group
review
rate
wa
250
LOC
per
hour
both
of
which
from
past
experience
also
seem
acceptable
If
the
modification
required
for
fixing
the
defect
and
addressing
the
is-
sue
are
few
then
the
group
review
status
is
accepted.
If
the
modification
required
are
many
a
follow-up
meeting
by
the
moderator
or
a
re-review
might
be
necessary
to
verify
whether
the
change
have
been
incorporated
correctly
The
moderator
recommends
what
is
to
be
done
In
addition
recommendation
regarding
review
in
the
next
stage
may
also
be
made
e.g.
in
a
detailed-
design
review
it
may
be
recommended
code
of
which
module
should
undergo
Traditionally
work
on
metric
ha
focused
on
the
final
product
namely
the
code
In
a
sense
all
metric
for
intermediate
product
of
requirement
and
design
are
basically
used
to
ensure
that
the
final
product
ha
a
high
quality
and
the
productivity
of
the
project
stay
high
That
is
the
basic
goal
of
metric
for
intermediate
product
is
to
predict
or
get
some
idea
about
the
metric
of
the
final
product
For
the
code
the
most
commonly
used
metric
are
size
and
complexity
Here
we
discus
a
few
size
and
complexity
measure
Size
of
a
product
is
a
simple
measure
which
can
be
easy
to
calculate
The
main
reason
for
interest
in
size
measure
is
that
size
is
the
major
factor
that
affect
the
cost
of
a
project
Size
in
itself
is
of
little
use
it
is
the
relationship
of
size
with
the
cost
and
quality
that
make
size
an
important
metric
It
is
also
used
to
measure
productivity
during
the
project
e.g.
KLOC
per
person-month
Final
quality
delivered
by
a
process
is
also
frequently
normalized
with
respect
to
size
number
of
defect
per
KLOC
For
these
reason
size
is
one
of
the
most
important
and
frequently
used
metric
The
most
common
measure
of
size
is
delivered
line
of
source
code
or
the
number
of
line
of
code
LOC
finally
delivered
The
trouble
with
LOC
is
that
the
number
of
line
of
code
for
a
project
depends
heavily
on
the
language
used
For
example
a
program
written
in
assembly
language
will
be
large
compared
to
the
same
program
written
in
a
higher-level
language
if
LOC
is
used
a
a
size
measure
Even
for
the
same
language
the
size
can
vary
considerably
depending
on
how
line
are
counted
Despite
these
deficiency
LOC
remains
a
handy
and
reasonable
size
measure
that
is
used
extensively
Currently
perhaps
the
most
widely
used
counting
method
for
determining
the
size
is
to
count
noncomment
nonblank
line
only
Halstead
46
ha
proposed
metric
for
length
and
volume
of
a
program
based
on
the
number
of
operator
and
operand
In
a
program
we
define
the
following
measurable
quantity
f1
j
is
the
number
of
occurrence
of
the
jth
most
frequent
operator
f2
j
is
the
number
of
occurrence
of
the
jth
most
frequent
operand
Then
the
vocabulary
n
of
a
program
is
defined
a
With
the
measurable
parameter
listed
earlier
two
new
parameter
are
defined
From
the
length
and
the
vocabulary
the
volume
V
of
the
program
is
defined
a
This
definition
of
the
volume
of
a
program
represents
the
minimum
number
of
bit
necessary
to
represent
the
program
log2
n
is
the
number
of
bit
needed
to
represent
every
element
in
the
program
uniquely
and
N
is
the
total
occur-
rences
of
the
different
element
Volume
is
used
a
a
size
metric
for
a
program
Experiments
have
shown
that
the
volume
of
a
program
is
highly
correlated
with
the
size
in
LOC
The
productivity
if
measured
only
in
term
of
line
of
code
per
unit
time
can
vary
a
lot
depending
on
the
complexity
of
the
system
to
be
developed
Clearly
a
programmer
will
produce
a
lesser
amount
of
code
for
highly
complex
system
program
a
compared
to
a
simple
application
program
Similarly
complexity
ha
great
impact
on
the
cost
of
maintaining
a
program
To
quantify
complexity
beyond
the
fuzzy
notion
of
the
ease
with
which
a
program
can
be
constructed
or
comprehended
some
metric
to
measure
the
complexity
of
a
program
are
needed
A
number
of
metric
have
been
proposed
for
quantifying
the
complexity
of
a
program
47
and
study
have
been
done
to
correlate
the
complexity
with
maintenance
effort
Here
we
discus
a
few
of
the
complexity
measure
that
have
been
proposed
Cyclomatic
Complexity
Based
on
the
capability
of
the
human
mind
and
the
experience
of
people
it
is
generally
recognized
that
condition
and
control
statement
add
complexity
to
a
program
Given
two
program
with
the
same
size
the
program
with
the
larger
number
of
decision
statement
is
likely
to
be
more
complex
The
simplest
measure
of
complexity
then
is
the
number
of
construct
that
represent
branch
in
the
control
flow
of
the
program
like
if
then
else
while
do
repeat
until
and
goto
statement
A
more
refined
measure
is
the
cyclomatic
complexity
measure
proposed
by
McCabe
which
is
a
graph-theoretic–based
concept
For
a
graph
G
with
n
node
e
edge
and
p
connected
component
the
cyclomatic
number
V
G
is
defined
a
To
use
this
to
define
the
cyclomatic
complexity
of
a
module
the
control
flow
graph
G
of
the
module
is
first
drawn
To
construct
a
control
flow
graph
of
a
program
module
break
the
module
into
block
delimited
by
statement
that
affect
the
control
flow
like
if
while
repeat
and
goto
These
block
form
the
node
of
the
graph
If
the
control
from
a
block
i
can
branch
to
a
block
j
then
draw
an
arc
from
node
i
to
node
j
in
the
graph
The
control
flow
of
a
program
can
be
constructed
mechanically
As
an
example
consider
the
C-like
function
for
bubble
sorting
given
next
The
control
flow
graph
for
this
is
given
in
Figure
The
graph
of
a
module
ha
an
entry
node
and
an
exit
node
corresponding
to
the
first
and
last
block
of
statement
or
we
can
create
artificial
node
for
simplicity
a
in
the
example
In
such
graph
there
will
be
a
path
from
the
entry
node
to
any
node
and
a
path
from
any
node
to
the
exit
node
assuming
the
program
ha
no
anomaly
like
unreachable
code
For
such
a
graph
the
cyclomatic
number
can
be
0
if
the
code
is
a
linear
sequence
of
statement
without
any
control
statement
If
we
draw
an
arc
from
the
exit
node
to
the
entry
node
the
graph
will
be
strongly
connected
because
there
is
a
path
between
any
two
node
The
cyclomatic
number
of
a
graph
for
any
program
will
then
be
nonzero
and
it
is
desirable
to
have
a
nonzero
complexity
for
a
simple
program
without
any
condition
after
all
there
is
some
complexity
in
such
a
program
Hence
for
computing
the
cyclomatic
complexity
of
a
program
an
arc
is
added
from
the
exit
node
to
the
start
node
which
make
it
a
strongly
connected
graph
For
a
module
the
cyclomatic
complexity
is
defined
to
be
the
cyclomatic
number
of
such
a
graph
for
the
module
As
it
turn
out
the
cyclomatic
complexity
of
a
module
or
cyclomatic
num-
ber
of
it
graph
is
equal
to
the
maximum
number
of
linearly
independent
circuit
in
the
graph
A
set
of
circuit
is
linearly
independent
if
no
circuit
is
totally
contained
in
another
circuit
or
is
a
combination
of
other
circuit
So
for
calculating
the
cyclomatic
number
of
a
module
we
can
draw
the
graph
make
it
connected
by
drawing
an
arc
from
the
exit
node
to
the
entry
node
and
then
either
count
the
number
of
circuit
or
compute
it
by
counting
the
number
of
edge
and
node
In
the
graph
shown
in
Figure
the
cyclomatic
complexity
is
ckt
1
b
c
e
b
ckt
2
b
c
d
e
b
ckt
3
a
b
f
a
ckt
4
a
g
a
It
can
also
be
shown
that
the
cyclomatic
complexity
of
a
module
is
the
number
of
decision
in
the
module
plus
one
where
a
decision
is
effectively
any
condi-
tional
statement
in
the
module
26
Hence
we
can
also
compute
the
cyclomatic
complexity
simply
by
counting
the
number
of
decision
in
the
module
For
this
example
a
we
can
see
we
get
the
same
cyclomatic
complexity
for
the
module
if
we
add
The
module
ha
three
decision
two
in
the
two
while
statement
and
one
in
the
if
statement
The
cyclomatic
number
is
one
quantitative
measure
of
module
complexity
It
can
be
extended
to
compute
the
complexity
of
the
whole
program
though
it
is
more
suitable
at
the
module
level
McCabe
proposed
that
the
cyclomatic
complexity
of
module
should
in
general
be
kept
below
10
The
cyclomatic
number
can
also
be
used
a
a
number
of
path
that
should
be
tested
during
testing
Cyclomatic
complexity
is
one
of
the
most
widely
used
complexity
mea-
sures
Experiments
indicate
that
the
cyclomatic
complexity
is
highly
correlated
to
the
size
of
the
module
in
LOC
after
all
the
more
line
of
code
the
greater
the
number
of
decision
It
ha
also
been
found
to
be
correlated
to
the
number
of
fault
found
in
module
Halstead
’
s
Measure
Halstead
also
proposed
a
number
of
other
measure
based
on
his
software
science
46
Some
of
these
can
be
considered
complexity
mea-
sures
As
given
earlier
a
number
of
variable
are
defined
in
software
science
These
are
n1
number
of
unique
operator
n2
number
of
unique
operand
N1
total
frequency
of
operator
and
N2
total
frequency
of
operand
As
any
program
must
have
at
least
two
operators—one
for
function
call
and
one
for
end
of
statement—the
ratio
n1/-
ficulty
due
to
the
larger
number
of
operator
in
the
program
The
ratio
N2/n
In
a
program
in
which
variable
are
changed
more
frequently
this
ratio
will
be
larger
As
such
program
are
harder
to
understand
ease
of
reading
or
writing
is
defined
a
Halstead
’
s
complexity
measure
focused
on
the
internal
complexity
of
a
module
a
doe
McCabe
’
s
complexity
measure
Thus
the
complexity
of
the
module
’
s
connection
with
it
environment
is
not
given
much
importance
In
Halstead
’
s
measure
a
module
’
s
connection
with
it
environment
is
reflected
in
term
of
operand
and
operator
A
call
to
another
module
is
considered
an
operator
and
all
the
parameter
are
considered
operand
of
this
operator
Live
Variables
In
a
computer
program
a
typical
assignment
statement
us
and
modifies
only
a
few
variable
However
in
general
the
statement
have
a
much
larger
context
That
is
to
construct
or
understand
a
statement
a
programmer
must
keep
track
of
a
number
of
variable
other
than
those
directly
used
in
the
statement
For
a
statement
such
data
item
are
called
live
variable
Intuitively
the
more
live
variable
for
statement
the
harder
it
will
be
to
understand
a
program
Hence
the
concept
of
live
variable
can
be
used
a
a
metric
for
program
complexity
First
let
u
define
live
variable
more
precisely
A
variable
is
considered
live
from
it
first
to
it
last
reference
within
a
module
including
all
statement
between
the
first
and
last
statement
where
the
variable
is
referenced
Using
this
definition
the
set
of
live
variable
for
each
statement
can
be
computed
easily
by
analysis
of
the
module
’
s
code
The
procedure
of
determining
the
live
variable
can
easily
be
automated
For
a
statement
the
number
of
live
variable
represents
the
degree
of
dif-
ficulty
of
the
statement
This
notion
can
be
extended
to
the
entire
module
by
defining
the
average
number
of
live
variable
The
average
number
of
live
vari-
ables
is
the
sum
of
the
count
of
live
variable
for
all
executable
statement
divided
by
the
number
of
executable
statement
This
is
a
complexity
measure
for
the
module
Live
variable
are
defined
from
the
point
of
view
of
data
usage
The
logic
of
a
module
is
not
explicitly
included
The
logic
is
used
only
to
determine
the
first
and
last
statement
of
reference
for
a
variable
Hence
this
concept
of
complexity
is
quite
different
from
cyclomatic
complexity
which
is
based
entirely
on
the
logic
and
considers
data
a
secondary
Another
data
usage-oriented
concept
is
span
the
number
of
statement
between
two
successive
us
of
a
variable
If
a
variable
is
referenced
at
n
different
place
in
a
module
then
for
that
variable
there
are
n
1
span
The
average
span
size
is
the
average
number
of
executable
statement
between
two
successive
reference
of
a
variable
A
large
span
implies
that
the
reader
of
the
program
ha
to
remember
a
definition
of
a
variable
for
a
larger
period
of
time
or
for
more
statement
In
other
word
span
can
be
considered
a
complexity
measure
the
larger
the
span
the
more
complex
the
module
Knot
Count
A
method
for
quantifying
complexity
based
on
the
location
of
the
control
transfer
of
the
program
ha
been
proposed
in
85
It
wa
designed
largely
for
FORTRAN
program
where
explicit
transfer
of
control
is
shown
by
the
use
of
goto
statement
A
programmer
to
understand
a
given
program
typically
draw
arrow
from
the
point
of
control
transfer
to
it
destination
helping
to
create
a
mental
picture
of
the
program
and
the
control
transfer
in
it
According
to
this
metric
the
more
intertwined
these
arrow
become
the
more
complex
the
program
This
notion
is
captured
in
the
concept
of
a
knot.
A
knot
is
essentially
the
intersection
of
two
such
control
transfer
arrow
If
each
statement
in
the
program
is
written
on
a
separate
line
this
notion
can
be
formalized
a
follows
A
jump
from
line
a
to
line
b
is
represented
by
the
pair
a
b
Two
jump
a
b
and
p
q
give
rise
to
a
knot
if
either
min
a
b
<
min
p
q
<
max
a
b
and
max
p
q
>
max
a
b
or
min
a
b
<
max
p
Problems
can
arise
while
determining
the
knot
count
of
program
using
structured
construct
One
method
is
to
convert
such
a
program
into
one
that
explicitly
show
control
transfer
and
then
compute
the
knot
count
The
basic
scheme
can
be
generalized
to
flow
graph
though
with
flow
graph
only
bound
can
be
obtained
Topological
Complexity
A
complexity
measure
that
is
sensitive
to
the
nesting
of
structure
ha
been
proposed
in
20
Like
cyclomatic
complexity
it
is
based
on
the
flow
graph
of
a
module
or
program
The
complexity
of
a
program
is
considered
it
maximal
intersect
number
min
To
compute
the
maximal
intersect
a
flow
graph
is
converted
into
a
strongly
connected
graph
by
drawing
an
arrow
from
the
terminal
node
to
the
initial
node
A
strongly
connected
graph
divide
the
graph
into
a
finite
number
of
region
The
number
of
region
is
edge
node
+
2
If
we
draw
a
line
that
enters
each
region
exactly
once
then
the
number
of
time
this
line
intersects
the
arc
in
the
graph
is
the
maximal
intersect
min
which
is
taken
to
be
the
complexity
of
the
program
As
reading
program
is
a
much
more
common
activity
than
writing
program
the
goal
of
the
coding
activity
is
to
produce
program
that
are
besides
being
free
of
defect
easy
to
understand
and
modify
Use
of
structured
programming
in
which
the
program
is
a
sequence
of
suitable
single-entry
single-exit
construct
make
program
easy
to
understand
and
verify
Other
practice
like
using
information
hiding
suitable
coding
stan-
dard
and
good
programming
practice
also
help
improve
code
readability
and
quality
For
a
developer
it
is
most
effective
to
develop
code
incrementally
This
can
be
done
by
writing
code
in
small
increment
and
testing
and
debugging
each
increment
before
writing
more
code
Alternatively
test-driven
development
may
be
followed
in
which
test
case
are
written
first
and
then
code
is
written
to
pas
these
test
case
Though
coding
of
a
module
is
generally
done
by
individual
programmer
an
alternative
is
pair
programming
in
which
coding
is
done
by
a
pair
of
programmers—both
together
evolving
strategy
data
structure
algorithm
etc
Evolving
code
need
to
be
properly
managed
This
can
be
done
through
proper
source
code
control
tool
which
allow
easy
management
of
the
different
version
that
get
created
a
well
a
easy
undoing
of
change
that
need
to
be
rolled
back
As
code
change
with
time
to
ensure
that
the
code
quality
doe
not
continue
to
degrade
due
to
evolution
refactoring
may
be
done
During
refactoring
no
new
functionality
is
added—only
improvement
is
done
so
that
the
design
of
the
code
improves
by
reduction
of
coupling
increase
in
cohesion
and
better
use
of
hierarchy
Unit
testing
is
a
very
popular
and
most
often
used
practice
by
programmer
for
verifying
the
code
they
have
written
In
unit
testing
the
programmer
test
his/her
code
in
isolation
For
procedural
language
this
is
often
a
small
set
of
procedure
or
function
and
for
object-oriented
language
this
is
generally
a
class
or
a
small
set
of
class
Unit
testing
requires
driver
and
stub
and
can
be
facilitated
by
the
use
of
framework
which
allow
automated
test
script
execution
Good
framework
like
CUnit
and
Junit
exist
for
both
procedural
language
and
object-oriented
language
A
number
of
metric
exist
for
quantifying
different
quality
of
code
The
most
commonly
used
are
size
metric
because
they
are
used
to
ass
the
produc-
tivity
of
people
and
are
often
used
in
cost
estimation
The
most
common
size
measure
is
line
of
code
LOC
which
is
also
used
in
most
cost
model
The
goal
of
complexity
metric
is
to
quantify
the
complexity
of
software
a
com-
plexity
is
an
important
factor
affecting
the
productivity
of
project
and
is
a
factor
in
cost
estimation
A
number
of
different
metric
exist
most
common
being
the
cyclomatic
complexity
which
is
based
on
the
internal
logic
of
the
program
and
defines
complexity
a
the
number
of
independent
cycle
in
the
flow
graph
of
the
program
What
is
structured
programming
and
how
doe
it
help
improve
code
readability
How
doe
the
use
of
information
hiding
and
coding
standard
help
improve
the
readability
Suggest
some
possibility
on
how
TDD
will
function
if
programming
is
being
done
in
pair
Write
a
class/procedure
using
TDD
and
some
testing
framework
Have
another
programmer
do
the
same
using
the
incremental
code
first
approach
Then
com-
pare
the
code
for
the
class/procedure
with
respect
to
size
of
the
code
effort
required
and
the
number
of
test
case
used
Consider
the
code
for
a
class
Describe
two
situation
for
this
code
which
will
suggest
to
you
that
refactoring
may
be
desired
For
each
of
these
suggest
the
nature
of
refactoring
that
you
will
perform
What
is
the
role
of
testing
framework
and
automated
script
in
refactoring
Use
your
favorite
unit
testing
framework
and
use
it
to
unit
test
a
procedure/class
which
requires
at
least
one
other
procedure/classe
Give
a
flowchart
describing
the
code
inspection
process
Write
some
rule
for
evaluating
the
summary
report
of
a
code
review
Consider
the
following
two
algorithm
for
searching
an
element
E
in
a
sorted
array
A
which
contains
n
integer
The
first
procedure
implement
a
simple
linear
search
algorithm
The
second
performs
a
binary
search
Binary
search
is
generally
much
more
efficient
in
term
of
execution
time
compared
to
the
linear
search
while
not
found
and
i
≤
n
do
begin
if
A
i
=
E
then
found
=
true
Determine
the
cyclomatic
complexity
and
live
variable
complexity
for
these
two
function
Is
the
ratio
of
the
two
complexity
measure
similar
for
the
two
func-
tions
What
is
Halstead
’
s
size
measure
for
these
two
module
Compare
this
size
with
the
size
measured
in
LOC
In
a
software
development
project
error
can
be
introduced
at
any
stage
dur-
ing
development
Though
error
are
detected
after
each
phase
by
technique
like
inspection
some
error
remain
undetected
Ultimately
these
remaining
error
are
reflected
in
the
code
Hence
the
final
code
is
likely
to
have
some
re-
quirements
error
and
design
error
in
addition
to
error
introduced
during
the
coding
activity
To
ensure
quality
of
the
final
delivered
software
these
defect
will
have
to
be
removed
There
are
two
type
of
approach
for
identifying
defect
in
the
software—
static
and
dynamic
In
static
analysis
the
code
is
not
executed
but
is
evaluated
through
some
process
or
some
tool
for
locating
defect
Code
inspection
which
we
discussed
in
the
previous
chapter
is
one
static
approach
Another
is
static
analysis
of
code
through
the
use
of
tool
In
dynamic
analysis
code
is
executed
and
the
execution
is
used
for
determining
defect
Testing
is
the
most
common
dynamic
technique
that
is
employed
Indeed
testing
is
the
most
commonly
used
technique
for
detecting
defect
and
performs
a
very
critical
role
for
ensuring
quality
During
testing
the
software
under
test
SUT
is
executed
with
a
finite
set
of
test
case
and
the
behavior
of
the
system
for
these
test
case
is
evaluated
to
determine
if
the
system
is
performing
a
expected
The
basic
purpose
of
testing
is
to
increase
the
confidence
in
the
functioning
of
SUT
And
a
testing
is
extremely
expensive
and
can
consume
unlimited
amount
of
effort
an
additional
practical
goal
is
to
achieve
the
desired
confidence
a
efficiently
a
possible
Clearly
the
effectiveness
and
efficiency
of
testing
depends
critically
on
the
test
case
selected
Much
of
this
chapter
therefore
is
devoted
to
test
case
selection
P.
Jalote
A
Concise
Introduction
to
Software
Engineering
Basic
concept
and
definition
relating
to
testing
like
error
fault
failure
test
case
test
suite
test
harness
etc
The
testing
process—how
testing
is
planned
and
how
testing
of
a
unit
is
done
Test
case
selection
using
black-box
testing
approach
Some
metric
like
coverage
and
reliability
that
can
be
employed
during
test-
ing
In
this
section
we
will
first
define
some
of
the
term
that
are
commonly
used
when
discussing
testing
Then
we
will
discus
some
basic
issue
relating
to
how
testing
is
performed
and
the
importance
of
psychology
of
the
tester
While
discussing
testing
we
commonly
use
term
like
error
fault
failure
etc
Let
u
start
by
defining
these
concept
clearly
52
The
term
error
is
used
in
two
different
way
It
refers
to
the
discrepancy
between
a
computed
observed
or
measured
value
and
the
true
specified
or
theoretically
correct
value
That
is
error
refers
to
the
difference
between
the
actual
output
of
a
software
and
the
correct
output
In
this
interpretation
error
is
essentially
a
measure
of
the
difference
between
the
actual
and
the
ideal
Error
is
also
used
to
refer
to
human
action
that
result
in
software
containing
a
defect
or
fault
This
definition
is
quite
general
and
encompasses
all
the
phase
Fault
is
a
condition
that
cause
a
system
to
fail
in
performing
it
required
function
A
fault
is
the
basic
reason
for
software
malfunction
and
is
practically
synonymous
with
the
commonly
used
term
bug
or
the
somewhat
more
general
term
defect
The
term
error
is
also
often
used
to
refer
to
defect
taking
a
variation
of
the
second
definition
of
error
In
this
book
we
will
continue
to
use
the
term
in
the
manner
commonly
used
and
no
explicit
distinction
will
be
made
between
error
and
fault
unless
necessary
Failure
is
the
inability
of
a
system
or
component
to
perform
a
required
function
according
to
it
specification
A
software
failure
occurs
if
the
behavior
of
the
software
is
different
from
the
specified
behavior
Failures
may
be
caused
by
functional
or
performance
factor
Note
that
the
definition
doe
not
imply
that
a
failure
must
be
observed
It
is
possible
that
a
failure
may
occur
but
not
be
detected
There
are
some
implication
of
these
definition
Presence
of
an
error
in
the
state
implies
that
a
failure
must
have
occurred
and
the
observance
of
a
failure
implies
that
a
fault
must
be
present
in
the
system
However
the
presence
of
a
fault
doe
not
imply
that
a
failure
must
occur
The
presence
of
a
fault
in
a
system
only
implies
that
the
fault
ha
a
potential
to
cause
a
failure
Whether
a
fault
actually
manifest
itself
in
a
certain
time
duration
depends
on
how
the
software
is
executed
There
are
direct
consequence
of
this
on
testing
If
during
testing
we
do
not
observe
any
error
we
can
not
say
anything
about
the
presence
or
absence
of
fault
in
the
system
If
on
the
other
hand
we
observe
some
failure
we
can
say
that
there
are
some
fault
in
the
system
This
relationship
of
fault
and
failure
make
the
task
of
selecting
test
case
for
testing
very
challenging—an
objective
while
selecting
test
case
is
to
select
those
that
will
reveal
the
defect
if
it
exists
Ideally
we
would
like
the
set
of
test
case
to
be
such
that
if
there
are
any
defect
in
the
system
some
test
case
in
the
set
will
reveal
it—something
impossible
to
achieve
in
most
situation
It
should
be
noted
that
during
the
testing
process
only
failure
are
ob-
served
by
which
the
presence
of
fault
is
deduced
That
is
testing
only
reveals
the
presence
of
fault
The
actual
fault
are
identified
by
separate
activity
commonly
referred
to
a
debugging.
In
other
word
for
identifying
fault
af-
ter
testing
ha
revealed
the
presence
of
fault
the
expensive
task
of
debugging
ha
to
be
performed
This
is
one
of
the
reason
why
testing
is
an
expensive
method
for
identification
of
fault
So
far
we
have
used
the
term
test
case
or
set
of
test
case
informally
Let
u
define
them
more
precisely
A
test
case
often
called
a
test
can
be
considered
a
comprising
a
set
of
test
input
and
execution
condition
which
are
designed
to
exercise
the
SUT
in
a
particular
manner
52
Generally
a
test
case
also
specifies
the
expected
outcome
from
executing
the
SUT
under
the
specified
execution
condition
and
test
input
A
group
of
related
test
case
that
are
generally
executed
together
to
test
some
specific
behavior
or
aspect
of
the
SUT
is
often
referred
to
a
a
test
suite
Note
that
in
a
test
case
test
input
and
execution
condition
are
mentioned
separately
Test
input
are
the
specific
value
of
parameter
or
other
input
that
are
given
to
the
SUT
either
by
the
user
or
some
other
program
The
execution
condition
on
the
other
hand
reflect
the
state
of
the
system
and
environment
which
also
impact
the
behavior
of
the
SUT
So
for
example
while
testing
a
function
to
add
a
record
in
the
database
if
it
doe
not
already
exist
the
behavior
of
the
function
will
depend
both
on
the
value
of
the
input
record
a
well
a
the
state
of
the
database
And
a
test
case
need
to
specify
both
For
example
a
test
case
for
this
function
might
specify
a
record
r
a
input
and
might
specify
that
the
state
of
the
database
be
such
that
r
already
exists
in
it
Testing
can
be
done
manually
with
the
tester
executing
the
test
case
in
the
test
suite
and
then
checking
if
the
behavior
is
a
specified
in
the
test
case
This
is
a
very
cumbersome
process
particularly
when
the
test
suite
contains
a
large
number
of
test
case
It
becomes
even
more
cumbersome
since
the
test
suite
often
ha
to
be
executed
every
time
the
SUT
is
changed
Hence
the
current
With
automated
testing
a
test
case
is
typically
a
function
call
or
a
method
invocation
which
doe
all
the
activity
of
a
test
case—it
set
the
test
data
and
the
test
condition
invokes
the
SUT
a
per
the
test
case
compare
the
result
returned
with
expected
result
and
declares
to
the
tester
whether
the
SUT
failed
or
passed
the
test
case
In
other
word
with
automated
testing
executing
a
test
case
essentially
mean
executing
this
function
A
test
suite
will
then
be
a
set
of
such
function
each
representing
a
test
case
To
test
a
SUT
with
the
test
suite
generally
an
automated
test
script
will
be
written
which
will
invoke
the
test
case
in
the
desired
sequence
To
have
a
test
suite
executed
automatically
we
will
need
a
framework
in
which
test
input
can
be
defined
defined
input
can
be
used
by
function
representing
test
case
the
automated
test
script
can
be
written
the
SUT
can
be
executed
by
this
script
and
the
result
of
entire
testing
reported
to
the
tester
Many
testing
framework
now
exist
that
permit
all
this
to
be
done
in
a
simple
manner
A
testing
framework
is
also
sometimes
called
a
test
harness
A
test
harness
or
a
test
framework
make
the
life
of
a
tester
simpler
by
providing
easy
mean
of
defining
a
test
suite
executing
it
and
reporting
the
result
With
a
test
framework
a
test
suite
is
defined
once
and
then
whenever
needed
complete
testing
can
be
done
by
the
click
of
a
button
or
giving
a
command
As
mentioned
in
testing
the
software
under
test
SUT
is
executed
with
a
set
of
test
case
As
discussed
devising
a
set
of
test
case
that
will
guarantee
that
all
error
will
be
detected
is
not
feasible
Moreover
there
are
no
formal
or
precise
method
for
selecting
test
case
Even
though
there
are
a
number
of
heuristic
and
rule
of
thumb
for
deciding
the
test
case
selecting
test
case
is
still
a
creative
activity
that
relies
on
the
ingenuity
of
the
tester
Because
of
this
the
psychology
of
the
person
performing
the
testing
becomes
important
A
basic
purpose
of
testing
is
to
detect
the
error
that
may
be
present
in
the
program
Hence
one
should
not
start
testing
with
the
intent
of
showing
that
a
program
work
rather
the
intent
should
be
to
show
that
a
program
doe
not
work
to
reveal
any
defect
that
may
exist
Due
to
this
testing
ha
also
been
defined
a
the
process
of
executing
a
program
with
the
intent
of
finding
error
This
emphasis
on
proper
intent
of
testing
is
not
a
trivial
matter
because
test
case
are
designed
by
human
being
and
human
being
have
a
tendency
to
perform
action
to
achieve
the
goal
they
have
in
mind
So
if
the
goal
is
to
demonstrate
that
a
program
work
we
may
consciously
or
subconsciously
select
test
case
that
will
try
to
demonstrate
that
goal
and
that
will
beat
the
basic
purpose
of
testing
On
the
other
hand
if
the
intent
is
to
show
that
the
program
doe
not
work
we
will
challenge
our
intellect
to
find
test
case
toward
that
end
and
we
are
likely
to
detect
more
error
Testing
is
essentially
a
destructive
process
where
the
tester
ha
to
treat
the
program
a
an
adversary
that
must
be
beaten
by
the
tester
by
showing
the
presence
of
error
This
is
one
of
the
reason
why
many
organization
employ
independent
testing
in
which
testing
is
done
by
a
team
that
wa
not
involved
in
building
the
system
Testing
is
usually
relied
upon
to
detect
the
fault
remaining
from
earlier
stage
in
addition
to
the
fault
introduced
during
coding
itself
Due
to
this
different
level
of
testing
are
used
in
the
testing
process
each
level
of
testing
aim
to
test
different
aspect
of
the
system
The
basic
level
are
unit
testing
integration
testing
system
testing
and
acceptance
testing
These
different
level
of
testing
attempt
to
detect
different
type
of
fault
The
relation
of
the
fault
introduced
in
different
phase
and
the
different
level
of
testing
are
shown
in
Figure
The
first
level
of
testing
is
called
unit
testing
which
we
discussed
in
the
previous
chapter
Unit
testing
is
essentially
for
verification
of
the
code
produced
by
individual
programmer
and
is
typically
done
by
the
programmer
of
the
module
Generally
a
module
is
offered
by
a
programmer
for
integration
and
use
by
others
only
after
it
ha
been
unit
tested
satisfactorily
The
next
level
of
testing
is
often
called
integration
testing
In
this
many
unit
tested
module
are
combined
into
subsystem
which
are
then
tested
The
goal
here
is
to
see
if
the
module
can
be
integrated
properly
Hence
the
emphasis
is
on
testing
interface
between
module
This
testing
activity
can
be
considered
testing
the
design
The
next
level
are
system
testing
and
acceptance
testing
Here
the
entire
software
system
is
tested
The
reference
document
for
this
process
is
the
requirement
document
and
the
goal
is
to
see
if
the
software
meet
it
require-
ments
This
is
often
a
large
exercise
which
for
large
project
may
last
many
week
or
month
This
is
essentially
a
validation
exercise
and
in
many
situa-
tions
it
is
the
only
validation
activity
Acceptance
testing
is
often
performed
with
realistic
data
of
the
client
to
demonstrate
that
the
software
is
working
satisfactorily
It
may
be
done
in
the
setting
in
which
the
software
is
to
even-
tually
function
Acceptance
testing
essentially
test
if
the
system
satisfactorily
solves
the
problem
for
which
it
wa
commissioned
These
level
of
testing
are
performed
when
a
system
is
being
built
from
the
component
that
have
been
coded
There
is
another
level
of
testing
called
regression
testing
that
is
performed
when
some
change
are
made
to
an
existing
system
We
know
that
change
are
fundamental
to
software
any
software
must
undergo
change
However
when
modification
are
made
to
an
existing
system
testing
also
ha
to
be
done
to
make
sure
that
the
modification
ha
not
had
any
undesired
side
effect
of
making
some
of
the
earlier
service
faulty
That
is
besides
ensuring
the
desired
behavior
of
the
new
service
testing
ha
to
ensure
that
the
desired
behavior
of
the
old
service
is
maintained
This
is
the
task
of
regression
testing
For
regression
testing
some
test
case
that
have
been
executed
on
the
old
system
are
maintained
along
with
the
output
produced
by
the
old
system
These
test
case
are
executed
again
on
the
modified
system
and
it
output
compared
with
the
earlier
output
to
make
sure
that
the
system
is
working
a
before
on
these
test
case
This
frequently
is
a
major
task
when
modification
are
to
be
made
to
existing
system
Complete
regression
testing
of
large
system
can
take
a
considerable
amount
of
time
even
if
automation
is
used
If
a
small
change
is
made
to
the
system
often
practical
consideration
require
that
the
entire
test
suite
not
be
executed
but
regression
testing
be
done
with
only
a
subset
of
test
case
This
requires
suitably
selecting
test
case
from
the
suite
which
can
test
those
part
of
the
system
that
could
be
affected
by
the
change
Test
case
selection
for
regression
testing
is
an
active
research
area
and
many
different
approach
have
been
proposed
in
the
literature
for
this
We
will
not
discus
it
any
further
The
basic
goal
of
the
software
development
process
is
to
produce
software
that
ha
no
error
or
very
few
error
Testing
is
a
quality
control
activity
which
focus
on
identifying
defect
which
are
then
removed
We
have
seen
that
different
level
of
testing
are
needed
to
detect
the
defect
injected
during
the
various
task
in
the
project
And
at
a
level
multiple
SUTs
may
be
tested
And
for
testing
each
SUT
test
case
will
have
to
be
designed
and
then
executed
Overall
testing
in
a
project
is
a
complex
task
which
also
consumes
the
max-
imum
effort
Hence
testing
ha
to
be
done
properly
in
a
project
The
testing
process
for
a
project
consists
of
three
high-level
tasks—test
planning
test
case
design
and
test
execution
We
will
discus
these
in
the
rest
of
this
section
In
general
in
a
project
testing
commences
with
a
test
plan
and
terminates
with
successful
execution
of
acceptance
testing
A
test
plan
is
a
general
docu-
ment
for
the
entire
project
that
defines
the
scope
approach
to
be
taken
and
the
schedule
of
testing
a
well
a
identifies
the
test
item
for
testing
and
the
personnel
responsible
for
the
different
activity
of
testing
The
test
planning
can
be
done
well
before
the
actual
testing
commences
and
can
be
done
in
par-
allel
with
the
coding
and
design
activity
The
input
for
forming
the
test
plan
are
1
project
plan
2
requirement
document
and
3
architecture
or
design
document
The
project
plan
is
needed
to
make
sure
that
the
test
plan
is
consistent
with
the
overall
quality
plan
for
the
project
and
the
testing
schedule
match
that
of
the
project
plan
The
requirement
document
and
the
design
document
are
the
basic
document
used
for
selecting
the
test
unit
and
decid-
ing
the
approach
to
be
used
during
testing
A
test
plan
should
contain
the
following
As
seen
earlier
different
level
of
testing
have
to
be
performed
in
a
project
The
level
are
specified
in
the
test
plan
by
identifying
the
test
unit
for
the
project
A
test
unit
is
a
set
of
one
or
more
module
that
form
a
software
under
test
SUT
The
identification
of
test
unit
establishes
the
different
level
of
testing
that
will
be
performed
in
the
project
Generally
a
number
of
test
unit
are
formed
during
the
testing
starting
from
the
lower-level
module
which
have
to
be
unit-tested
That
is
first
the
module
that
have
to
be
tested
individually
are
specified
a
test
unit
Then
the
higher-level
unit
are
specified
which
may
be
a
combination
of
already
tested
unit
or
may
combine
some
already
tested
unit
with
some
untested
module
The
basic
idea
behind
forming
test
unit
is
to
make
sure
that
testing
is
being
performed
incrementally
with
each
increment
including
only
a
few
aspect
that
need
to
be
tested
An
important
factor
while
forming
a
unit
is
the
testability
of
a
unit
A
unit
should
be
such
that
it
can
be
easily
tested
In
other
word
it
should
be
possible
to
form
meaningful
test
case
and
execute
the
unit
without
much
effort
with
these
test
case
For
example
a
module
that
manipulates
the
complex
data
structure
formed
from
a
file
input
by
an
input
module
might
not
be
a
suitable
unit
from
the
point
of
view
of
testability
a
forming
meaningful
test
case
for
the
unit
will
be
hard
and
driver
routine
will
have
to
be
written
to
convert
input
from
file
or
terminal
that
are
given
by
the
tester
into
data
structure
suitable
for
the
module
In
this
case
it
might
be
better
to
form
the
unit
by
including
the
input
module
a
well
Then
the
file
input
expected
by
the
input
module
can
contain
the
test
case
Features
to
be
tested
include
all
software
feature
and
combination
of
fea-
tures
that
should
be
tested
A
software
feature
is
a
software
characteristic
spec-
ified
or
implied
by
the
requirement
or
design
document
These
may
include
functionality
performance
design
constraint
and
attribute
The
approach
for
testing
specifies
the
overall
approach
to
be
followed
in
the
current
project
The
technique
that
will
be
used
to
judge
the
testing
effort
should
also
be
specified
This
is
sometimes
called
the
testing
criterion
or
the
criterion
for
evaluating
the
set
of
test
case
used
in
testing
In
the
previous
section
we
discussed
many
criterion
for
evaluating
and
selecting
test
case
Testing
deliverable
should
be
specified
in
the
test
plan
before
the
actual
testing
begin
Deliverables
could
be
a
list
of
test
case
that
were
used
detailed
result
of
testing
including
the
list
of
defect
found
test
summary
report
and
data
about
the
code
coverage
The
test
plan
typically
also
specifies
the
schedule
and
effort
to
be
spent
on
different
activity
of
testing
and
the
tool
to
be
used
This
schedule
should
be
consistent
with
the
overall
project
schedule
The
detailed
plan
may
list
all
the
testing
task
and
allocate
them
to
test
resource
who
are
responsible
for
performing
them
Many
large
product
have
separate
testing
team
and
therefore
a
separate
test
plan
A
smaller
project
may
include
the
test
plan
a
part
of
it
quality
plan
in
the
project
management
plan
The
test
plan
focus
on
how
the
testing
for
the
project
will
proceed
which
unit
will
be
tested
and
what
approach
and
tool
are
to
be
used
during
the
various
stage
of
testing
However
it
doe
not
deal
with
the
detail
of
testing
a
unit
nor
doe
it
specify
which
test
case
are
to
be
used
Test
case
design
ha
to
be
done
separately
for
each
unit
Based
on
the
approach
specified
in
the
test
plan
and
the
feature
to
be
tested
the
test
case
are
designed
and
specified
for
testing
the
unit
Test
case
specification
give
for
each
unit
to
be
tested
all
test
case
input
to
be
used
in
the
test
case
condition
being
tested
by
the
test
case
and
output
expected
for
those
test
case
If
test
case
are
specified
in
a
document
the
specification
look
like
a
table
of
the
form
shown
in
Figure
Sometimes
a
few
column
are
also
provided
for
recording
the
outcome
of
different
round
of
testing
That
is
sometimes
the
test
case
specification
doc-
ument
is
also
used
to
record
the
result
of
testing
In
a
round
of
testing
the
outcome
of
all
the
test
case
is
recorded
i.e.
pas
or
fail
Hopefully
in
a
few
round
all
test
case
will
pas
With
testing
framework
and
automated
testing
the
testing
script
can
be
considered
a
test
case
specification
a
they
clearly
show
what
input
are
being
given
and
what
output
to
expect
With
suitable
comment
the
intent
of
the
test
case
can
also
be
easily
specified
Test
case
design
is
a
major
activity
in
the
testing
process
Careful
selection
of
test
case
that
satisfy
the
criterion
and
approach
specified
is
essential
for
proper
testing
We
will
later
consider
different
technique
for
designing
good
test
case
There
are
some
good
reason
why
test
case
are
specified
before
they
are
used
for
testing
It
is
known
that
testing
ha
severe
limitation
and
the
effec-
tiveness
of
testing
depends
very
heavily
on
the
exact
nature
of
the
test
case
It
is
therefore
important
to
ensure
that
the
set
of
test
case
used
is
of
high
quality
Evaluation
of
test
case
is
often
done
through
test
case
review
As
for
any
review
a
formal
document
or
work
product
is
needed
for
review
of
test
case
the
test
case
specification
document
is
required
This
is
the
primary
reason
for
documenting
the
test
case
The
test
case
specification
document
is
reviewed
using
a
formal
review
process
to
make
sure
that
the
test
case
are
consistent
with
the
policy
specified
in
the
plan
satisfy
the
chosen
criterion
and
cover
the
various
aspect
of
the
unit
to
be
tested
By
reviewing
the
condition
being
tested
by
the
test
case
the
reviewer
can
also
check
if
all
the
important
condition
are
being
tested
Another
reason
for
specifying
the
test
case
in
a
document
or
a
script
is
that
by
doing
this
the
tester
can
see
the
testing
of
the
unit
in
totality
and
the
effect
of
the
total
set
of
test
case
This
type
of
evaluation
is
hard
to
do
in
on-the-fly
testing
where
test
case
are
determined
a
testing
proceeds
It
also
allows
optimizing
the
number
of
test
case
a
evaluation
of
the
test
suite
may
show
that
some
test
case
are
redundant
With
the
specification
of
test
case
the
next
step
in
the
testing
process
is
to
execute
them
This
step
is
also
not
straightforward
The
test
case
specification
only
specify
the
set
of
test
case
for
the
unit
to
be
tested
However
executing
the
test
case
may
require
construction
of
driver
module
or
stub
It
may
also
require
module
to
set
up
the
environment
a
stated
in
the
test
plan
and
test
case
specification
Only
after
all
these
are
ready
can
the
test
case
be
executed
If
test
framework
are
being
used
then
the
setting
of
the
environment
a
well
a
input
for
a
test
case
is
already
done
in
the
test
script
and
execution
is
straightforward
During
test
case
execution
defect
are
found
These
defect
are
then
fixed
and
tesing
is
done
again
to
verify
the
fix
To
facilitate
reporting
and
tracking
of
defect
found
during
testing
and
other
quality
control
activity
defect
found
are
often
logged
Defect
logging
is
particularly
important
in
a
large
software
project
which
may
have
hundred
or
thousand
of
defect
that
are
found
by
different
people
at
different
stage
of
the
project
Often
the
person
who
fix
a
defect
is
not
the
person
who
find
or
report
the
defect
For
example
a
tester
may
find
the
defect
while
the
developer
of
the
code
may
actually
fix
it
In
such
a
scenario
defect
reporting
and
closing
can
not
be
done
informally
The
use
of
informal
mechanism
may
easily
lead
to
defect
being
found
but
later
forgotten
resulting
in
defect
not
getting
removed
or
in
extra
effort
in
finding
the
defect
again
Hence
defect
found
must
be
properly
logged
in
a
system
and
their
closure
tracked
Defect
logging
and
tracking
is
considered
one
of
the
best
practice
for
managing
a
project
17
and
is
followed
by
most
software
organization
Let
u
understand
the
life
cycle
of
a
defect
A
defect
can
be
found
by
anyone
at
anytime
When
a
defect
is
found
it
is
logged
in
a
defect
control
system
along
with
sufficient
information
about
the
defect
The
defect
is
then
in
the
state
submitted
essentially
implying
that
it
ha
been
logged
along
with
information
about
it
The
job
of
fixing
the
defect
is
then
assigned
to
some
person
who
is
generally
the
author
of
the
document
or
code
in
which
the
defect
is
found
The
assigned
person
doe
the
debugging
and
fix
the
reported
defect
and
the
defect
then
enters
the
fixed
state
However
a
defect
that
is
fixed
is
still
not
considered
a
fully
done
The
successful
fixing
of
the
defect
is
verified
This
verification
may
be
done
by
another
person
often
the
submitter
or
by
a
test
team
and
typically
involves
running
some
test
Once
the
defect
fixing
is
verified
then
the
defect
can
be
marked
a
closed.
In
other
word
the
general
life
cycle
of
a
defect
ha
three
states—submitted
fixed
and
closed
a
shown
in
Figure
A
defect
that
is
not
closed
is
also
called
open
This
is
a
typical
life
cycle
of
a
defect
which
is
used
in
many
organization
e.g.
58
Ideally
at
the
end
of
the
project
no
open
defect
should
remain
However
this
ideal
situation
is
often
not
practical
for
most
large
system
Besides
using
the
log
for
tracking
defect
the
data
in
the
log
can
also
be
used
for
analysis
purpose
We
will
discus
some
possible
analysis
later
in
the
chapter
As
we
have
seen
good
test
case
design
is
the
key
to
suitable
testing
of
the
SUT
The
goal
while
testing
a
SUT
is
to
detect
most
hopefully
all
of
the
defect
through
a
small
a
set
of
test
case
a
possible
Due
to
this
basic
goal
it
is
important
to
select
test
case
carefully—best
are
those
test
case
that
have
a
high
probability
of
detecting
a
defect
if
it
exists
and
also
whose
execution
will
give
a
confidence
that
no
failure
during
testing
implies
that
there
are
few
hopefully
none
defect
in
the
software
There
are
two
basic
approach
to
designing
the
test
case
to
be
used
in
testing
black-box
and
white-box
In
black-box
testing
the
structure
of
the
program
is
not
considered
Test
case
are
decided
solely
on
the
basis
of
the
requirement
or
specification
of
the
program
or
module
and
the
internals
of
the
module
or
the
program
are
not
considered
for
selection
of
test
case
In
this
section
we
will
present
some
technique
for
generating
test
case
for
black-box
testing
White-box
testing
is
discussed
in
the
next
section
In
black-box
testing
the
tester
only
know
the
input
that
can
be
given
to
the
system
and
what
output
the
system
should
give
In
other
word
the
basis
for
deciding
test
case
is
the
requirement
or
specification
of
the
system
or
module
This
form
of
testing
is
also
called
functional
or
behavioral
testing
The
most
obvious
functional
testing
procedure
is
exhaustive
testing
which
is
impractical
One
criterion
for
generating
test
case
is
to
generate
them
ran-
domly
This
strategy
ha
little
chance
of
resulting
in
a
set
of
test
case
that
is
close
to
optimal
i.e.
that
detects
the
maximum
error
with
minimum
test
case
Hence
we
need
some
other
criterion
or
rule
for
selecting
test
case
There
are
no
formal
rule
for
designing
test
case
for
functional
testing
How-
ever
there
are
a
number
of
technique
or
heuristic
that
can
be
used
to
select
test
case
that
have
been
found
to
be
very
successful
in
detecting
error
Here
we
mention
some
of
these
technique
Because
we
can
not
do
exhaustive
testing
the
next
natural
approach
is
to
divide
the
input
domain
into
a
set
of
equivalence
class
so
that
if
the
program
work
correctly
for
a
value
then
it
will
work
correctly
for
all
the
other
value
in
that
class
If
we
can
indeed
identify
such
class
then
testing
the
program
with
one
value
from
each
equivalence
class
is
equivalent
to
doing
an
exhaustive
test
of
the
program
However
without
looking
at
the
internal
structure
of
the
program
it
is
impossible
to
determine
such
ideal
equivalence
class
even
with
the
internal
structure
it
usually
can
not
be
done
The
equivalence
class
partitioning
method
68
try
to
approximate
this
ideal
An
equivalence
class
is
formed
of
the
input
for
which
the
behavior
of
the
system
is
specified
or
expected
to
be
similar
Each
group
of
input
for
which
the
behavior
is
expected
to
be
different
from
others
is
considered
a
separate
equivalence
class
The
rationale
of
forming
equivalence
class
like
this
is
the
assumption
that
if
the
specification
require
the
same
behavior
for
each
element
in
a
class
of
value
then
the
program
is
likely
to
be
constructed
so
that
it
either
succeeds
or
fails
for
each
of
the
value
in
that
class
For
example
the
specification
of
a
module
that
determines
the
absolute
value
for
integer
specify
one
behavior
for
positive
integer
and
another
for
negative
integer
In
this
case
we
will
form
two
equivalence
classes—one
consisting
of
positive
integer
and
the
other
consisting
of
negative
integer
For
robust
software
we
must
also
consider
invalid
input
That
is
we
should
define
equivalence
class
for
invalid
input
also
Equivalence
class
are
usually
formed
by
considering
each
condition
speci-
fied
on
an
input
a
specifying
a
valid
equivalence
class
and
one
or
more
invalid
equivalence
class
For
example
if
an
input
condition
specifies
a
range
of
val-
ues
say
0
<
count
<
Max
then
form
a
valid
equivalence
class
with
that
range
and
two
invalid
equivalence
class
one
with
value
le
than
the
lower
bound
of
the
range
i.e.
count
<
0
and
the
other
with
value
higher
than
the
higher
bound
count
>
Max
If
the
input
specifies
a
set
of
value
and
the
require-
ments
specify
different
behavior
for
different
element
in
the
set
then
a
valid
equivalence
class
is
formed
for
each
of
the
element
in
the
set
and
an
invalid
class
for
an
entity
not
belonging
to
the
set
One
common
approach
for
determining
equivalence
class
is
a
follows
If
there
is
reason
to
believe
that
the
entire
range
of
an
input
will
not
be
treated
in
the
same
manner
then
the
range
should
be
split
into
two
or
more
equivalence
class
each
consisting
of
value
for
which
the
behavior
is
expected
to
be
sim-
ilar
For
example
for
a
character
input
if
we
have
reason
to
believe
that
the
program
will
perform
different
action
if
the
character
is
a
letter
a
number
or
a
special
character
then
we
should
split
the
input
into
three
valid
equivalence
Another
approach
for
forming
equivalence
class
is
to
consider
any
special
value
for
which
the
behavior
could
be
different
a
an
equivalence
class
For
example
the
value
0
could
be
a
special
value
for
an
integer
input
Also
for
each
valid
equivalence
class
one
or
more
invalid
equivalence
class
should
be
identified
It
is
often
useful
to
consider
equivalence
class
in
the
output
For
an
output
equivalence
class
the
goal
is
to
have
input
such
that
the
output
for
that
test
case
lie
in
the
output
equivalence
class
As
an
example
consider
a
program
for
determining
rate
of
return
for
some
investment
There
are
three
clear
output
equivalence
classes—positive
rate
of
return
negative
rate
of
return
and
zero
rate
of
return
During
testing
it
is
important
to
test
for
each
of
these
that
is
give
input
such
that
each
of
these
three
output
is
generated
Determining
test
case
for
output
class
may
be
more
difficult
but
output
class
have
been
found
to
reveal
error
that
are
not
revealed
by
just
considering
the
input
class
Once
equivalence
class
are
selected
for
each
of
the
input
then
the
issue
is
to
select
test
case
suitably
There
are
different
way
to
select
the
test
case
One
strategy
is
to
select
each
test
case
covering
a
many
valid
equivalence
class
a
it
can
and
one
separate
test
case
for
each
invalid
equivalence
class
A
somewhat
better
strategy
which
requires
more
test
case
is
to
have
a
test
case
cover
at
most
one
valid
equivalence
class
for
each
input
and
have
one
separate
test
case
for
each
invalid
equivalence
class
In
the
latter
case
the
number
of
test
case
for
valid
equivalence
class
is
equal
to
the
largest
number
of
equivalence
class
for
any
input
plus
the
total
number
of
invalid
equivalence
class
As
an
example
consider
a
program
that
take
two
inputs—a
string
s
of
length
up
to
N
and
an
integer
n.
The
program
is
to
determine
the
top
n
highest
occurring
character
in
s.
The
tester
belief
that
the
programmer
may
deal
with
different
type
of
character
separately
One
set
of
valid
and
invalid
equivalence
class
for
this
is
shown
in
Table
With
these
a
the
equivalence
class
we
have
to
select
the
test
case
A
test
case
for
this
is
a
pair
of
value
for
s
and
n.
With
the
first
strategy
for
deciding
test
case
one
test
case
could
be
s
a
a
string
of
length
le
than
N
containing
lowercase
uppercase
number
and
special
character
and
n
a
the
number
5
This
one
test
case
cover
all
the
valid
equivalence
class
EQ6
Then
we
will
have
one
test
case
each
for
covering
IEQ1
IEQ2
and
IEQ3
That
is
a
total
of
four
test
case
is
needed
With
the
second
approach
in
one
test
case
we
can
cover
one
equivalence
class
for
one
input
only
So
one
test
case
could
be
a
string
of
number
and
the
number
5
This
cover
EQ6
Then
we
will
need
test
case
for
EQ5
and
separate
test
case
for
IEQ3
It
ha
been
observed
that
program
that
work
correctly
for
a
set
of
value
in
an
equivalence
class
fail
on
some
special
value
These
value
often
lie
on
the
boundary
of
the
equivalence
class
Test
case
that
have
value
on
the
boundary
of
equivalence
class
are
therefore
likely
to
be
high-yield
test
case
and
selecting
such
test
case
is
the
aim
of
boundary
value
analysis
In
boundary
value
analysis
68
we
choose
an
input
for
a
test
case
from
an
equivalence
class
such
that
the
input
lie
at
the
edge
of
the
equivalence
class
Boundary
value
for
each
equivalence
class
including
the
equivalence
class
of
the
output
should
be
covered
Boundary
value
test
case
are
also
called
extreme
cases.
Hence
we
can
say
that
a
boundary
value
test
case
is
a
set
of
input
data
that
lie
on
the
edge
or
boundary
of
a
class
of
input
data
or
that
generates
output
that
lie
at
the
boundary
of
a
class
of
output
data
In
case
of
range
for
boundary
value
analysis
it
is
useful
to
select
the
boundary
element
of
the
range
and
an
invalid
value
just
beyond
the
two
end
for
the
two
invalid
equivalence
class
So
if
the
range
is
0.0
x
1.0
then
the
test
case
are
0.0
1.0
valid
input
and
0.1
and
1.1
for
invalid
input
Similarly
if
the
input
is
a
list
attention
should
be
focused
on
the
first
and
last
element
of
the
list
We
should
also
consider
the
output
for
boundary
value
analysis
If
an
equivalence
class
can
be
identified
in
the
output
we
should
try
to
generate
test
case
that
will
produce
the
output
that
lie
at
the
boundary
of
the
equivalence
class
Furthermore
we
should
try
to
form
test
case
that
will
produce
an
output
that
doe
not
lie
in
the
equivalence
class
If
we
can
produce
an
input
case
that
produce
the
output
outside
the
equivalence
class
we
have
detected
an
error
Like
in
equivalence
class
partitioning
in
boundary
value
analysis
we
first
determine
value
for
each
of
the
variable
that
should
be
exercised
during
test-
ing
If
there
are
multiple
input
then
how
should
the
set
of
test
case
be
formed
covering
the
boundary
value
Suppose
each
input
variable
ha
a
defined
range
Then
there
are
six
boundary
values—the
extreme
end
of
the
range
just
be-
yond
the
end
and
just
before
the
end
If
an
integer
range
is
min
to
max
then
the
six
value
are
min
1
min
min
+
1
max
1
max
max
+
1
Suppose
there
are
n
such
input
variable
There
are
two
strategy
for
combining
the
boundary
value
for
the
different
variable
in
test
case
In
the
first
strategy
we
select
the
different
boundary
value
for
one
variable
and
keep
the
other
variable
at
some
nominal
value
And
we
select
one
test
case
consisting
of
nominal
value
of
all
the
variable
In
this
case
we
will
have
6n+
For
two
variable
X
and
Y
the
1
Another
strategy
would
be
to
try
all
possible
combination
for
the
value
for
the
different
variable
As
there
are
seven
value
for
each
variable
six
boundary
value
and
one
nominal
value
if
there
are
n
variable
there
will
be
a
total
of
7n
test
cases—too
large
for
practical
testing
There
are
generally
many
parameter
that
determine
the
behavior
of
a
software
system
These
parameter
could
be
direct
input
to
the
software
or
implicit
setting
like
those
for
device
These
parameter
can
take
different
value
and
for
some
of
them
the
software
may
not
work
correctly
Many
of
the
defect
in
software
generally
involve
one
condition
that
is
some
special
value
of
one
of
the
parameter
Such
a
defect
is
called
a
single-mode
fault
70
Simple
example
of
single-mode
fault
are
a
software
not
able
to
print
for
a
particular
type
of
printer
a
software
that
can
not
compute
fare
properly
when
the
traveler
is
a
minor
and
a
telephone
billing
software
that
doe
not
compute
the
bill
properly
for
a
particular
country
Single-mode
fault
can
be
detected
by
testing
for
different
value
of
dif-
ferent
parameter
So
if
there
are
n
parameter
for
a
system
and
each
one
of
them
can
take
m
different
value
or
m
different
class
of
value
each
class
being
considered
a
the
same
for
purpose
of
testing
a
in
equivalence
class
partitioning
then
with
each
test
case
we
can
test
one
different
value
of
each
parameter
In
other
word
we
can
test
for
all
the
different
value
in
m
test
case
However
all
fault
are
not
single-mode
and
there
are
combination
of
input
that
reveal
the
presence
of
fault
for
example
a
telephone
billing
software
that
doe
not
compute
correctly
for
nighttime
calling
one
parameter
to
a
particular
country
another
parameter
or
an
airline
ticketing
system
that
ha
incorrect
behavior
when
a
minor
one
parameter
is
traveling
business
class
another
parameter
and
not
staying
over
the
weekend
third
parameter
These
multi-
mode
fault
can
be
revealed
during
testing
by
trying
different
combination
of
the
parameter
values—an
approach
called
combinatorial
testing
Unfortunately
full
combinatorial
testing
is
often
not
feasible
For
a
system
with
n
parameter
each
having
m
value
the
number
of
different
combination
is
nm
For
a
simple
system
with
each
having
the
total
number
of
combination
is
3,125
And
if
testing
each
combination
take
it
will
take
over
Clearly
for
complex
system
that
have
many
parameter
and
each
parameter
may
have
many
value
a
full
combinatorial
testing
is
not
feasible
and
practical
technique
are
needed
to
reduce
the
number
of
test
Some
research
ha
suggested
that
most
software
fault
are
revealed
on
some
special
single
value
or
by
interaction
of
a
pair
of
value
25
That
it
most
fault
tend
to
be
either
single-mode
or
double-mode
For
testing
for
double-
mode
fault
we
need
not
test
the
system
with
all
the
combination
of
parameter
value
but
need
to
test
such
that
all
combination
of
value
for
each
pair
of
parameter
are
exercised
This
is
called
pairwise
testing
In
pairwise
testing
all
pair
of
value
have
to
be
exercised
during
testing
If
there
are
n
parameter
each
with
m
value
then
between
each
two
parameter
we
have
m
∗
m
pair
The
first
parameter
will
have
these
many
pair
with
each
of
the
remaining
n
−
the
second
one
will
have
new
pair
with
n
−
a
it
pair
with
the
first
are
already
included
in
the
first
parameter
pair
the
third
will
have
pair
with
n
−
and
so
on
That
is
the
total
number
of
pair
is
m
∗
m
∗
n
∗
n
−
1
/2
The
objective
of
pairwise
testing
is
to
have
a
set
of
test
case
that
cover
all
the
pair
As
there
are
n
parameter
a
test
case
is
a
combination
of
value
of
these
parameter
and
will
cover
n
1
+
n
2
+
...
=
n
n
1
/
In
the
best
case
when
each
pair
is
covered
exactly
once
by
one
test
case
m
As
an
example
consider
a
software
product
being
developed
for
multiple
platform
that
us
the
browser
a
it
interface
Suppose
the
software
is
be-
ing
designed
to
work
for
three
different
operating
system
and
three
different
browser
In
addition
a
the
product
is
memory
intensive
there
is
a
desire
to
test
it
performance
under
different
level
of
memory
So
we
have
the
following
three
parameter
with
their
different
value
Operating
System
Windows
Solaris
Linux
Memory
Size
128M
256M
512M
For
discussion
we
can
say
that
the
system
ha
three
parameter
A
op-
erating
system
B
memory
size
and
C
browser
Each
of
them
can
have
three
value
which
we
will
refer
to
a
a1
a2
a3
b1
b2
b3
and
c1
c2
c3
The
total
number
of
pairwise
combination
is
9
3
=
27
The
number
of
test
case
however
to
cover
all
the
pair
is
much
le
A
test
case
consisting
of
value
of
the
three
parameter
cover
three
combination
of
A-B
B-C
and
A-C
Hence
in
the
best
case
we
can
cover
all
227/3=
These
test
case
are
shown
in
Table
along
with
the
pair
they
cover
As
should
be
clear
generating
test
case
to
cover
all
the
pair
is
not
a
simple
task
The
minimum
set
of
test
case
is
that
in
which
each
pair
is
covered
by
exactly
one
test
case
Often
it
will
not
be
possible
to
generate
the
minimum
set
of
test
case
particularly
when
the
number
of
value
for
different
parameter
is
different
Various
algorithm
have
been
proposed
and
some
program
are
available
online
to
generate
the
test
case
to
cover
all
the
pair
For
situation
where
manual
generation
is
feasible
the
following
approach
can
be
followed
Start
with
initial
test
case
formed
by
all
combination
of
value
for
the
two
parameter
which
have
the
largest
number
of
value
a
we
must
have
at
least
this
many
test
case
to
test
all
the
pair
for
these
two
parameter
Then
complete
each
of
these
test
case
by
adding
value
for
other
parameter
such
that
they
add
pair
that
have
not
yet
been
covered
by
any
test
case
When
all
are
completed
form
additional
test
case
by
combining
a
many
uncovered
pair
a
possible
Essentially
we
are
generating
test
case
such
that
a
test
case
cover
a
many
new
pair
a
possible
By
avoiding
covering
pair
multiple
time
we
can
produce
a
small
set
of
test
case
that
cover
all
pair
Efficient
algorithm
of
generating
the
smallest
number
of
test
case
for
pairwise
testing
exist
In
25
an
example
is
given
in
which
for
1
each
having
three
distinct
value
all
pair
are
covered
in
merely
1
while
the
total
number
of
combination
is
over
Pairwise
testing
is
a
practical
way
of
testing
large
software
system
that
have
many
different
parameter
with
distinct
functioning
expected
for
different
value
An
example
would
be
a
billing
system
for
telephone
hotel
airline
etc
which
ha
different
rate
for
different
parameter
value
It
is
also
a
practical
approach
for
testing
general-purpose
software
product
that
are
expected
to
run
on
different
platform
and
configuration
or
a
system
that
is
expected
to
work
with
different
type
of
system
It
ha
been
seen
that
program
often
produce
incorrect
behavior
when
input
form
some
special
case
The
reason
is
that
in
program
some
combination
of
input
need
special
treatment
and
providing
proper
handling
for
these
special
case
is
easily
overlooked
For
example
in
an
arithmetic
routine
if
there
is
a
division
and
the
divisor
is
zero
some
special
action
ha
to
be
taken
which
could
easily
be
forgotten
by
the
programmer
These
special
case
form
particularly
good
test
case
which
can
reveal
error
that
will
usually
not
be
detected
by
other
test
case
Special
case
will
often
depend
on
the
data
structure
and
the
function
of
the
module
There
are
no
rule
to
determine
special
case
and
the
tester
ha
to
use
his
intuition
and
experience
to
identify
such
test
case
Consequently
determining
special
case
is
also
called
error
guessing
Psychology
is
particularly
important
for
error
guessing
The
tester
should
play
the
devil
’
s
advocate
and
try
to
guess
the
incorrect
assumption
the
programmer
could
have
made
and
the
situation
the
programmer
could
have
overlooked
or
handled
incorrectly
Essentially
the
tester
is
trying
to
identify
error-prone
situation
Then
test
case
are
written
for
these
situation
For
example
in
the
problem
of
finding
the
number
of
different
word
in
a
file
discussed
in
earlier
chapter
some
of
the
special
case
can
be
file
is
empty
only
one
word
in
the
file
only
one
word
in
a
line
some
empty
line
in
the
input
file
presence
of
more
than
one
blank
between
word
all
word
are
the
same
the
word
are
already
sorted
and
blank
at
the
start
and
end
of
the
file
Incorrect
assumption
are
usually
made
because
the
specification
are
not
complete
or
the
writer
of
specification
may
not
have
stated
some
property
assuming
them
to
be
obvious
Whenever
there
is
reliance
on
tacit
understanding
rather
than
explicit
statement
of
specification
there
is
scope
for
making
wrong
assumption
Frequently
wrong
assumption
are
made
about
the
environment
However
it
should
be
pointed
out
that
special
case
depend
heavily
on
the
problem
and
the
tester
should
really
try
to
get
into
the
shoe
of
the
designer
and
coder
to
determine
these
case
There
are
some
system
that
are
essentially
stateless
in
that
for
the
same
input
they
always
give
the
same
output
or
exhibit
the
same
behavior
Many
batch
processing
system
computational
system
and
server
fall
in
this
category
In
hardware
combinatorial
circuit
fall
in
this
category
At
a
smaller
level
most
function
are
supposed
to
behave
in
this
manner
There
are
however
many
system
whose
behavior
is
state-based
in
that
for
identical
input
they
behave
differently
at
different
time
and
may
produce
different
output
The
reason
for
different
behavior
is
that
the
state
of
the
system
may
be
different
In
other
word
the
behavior
and
output
of
the
system
depend
not
only
on
the
input
provided
but
also
on
the
state
of
the
system
The
state
of
the
system
depends
on
the
past
input
the
system
ha
received
In
other
word
the
state
represents
the
cumulative
impact
of
all
the
past
input
on
the
system
In
hardware
the
sequential
system
fall
in
this
category
In
software
many
large
system
fall
in
this
category
a
past
state
is
captured
in
database
or
file
and
used
to
control
the
behavior
of
the
system
For
such
system
another
approach
for
selecting
test
case
is
the
state-based
testing
approach
22
Theoretically
any
software
that
save
state
can
be
modeled
a
a
state
ma-
chine
However
the
state
space
of
any
reasonable
program
is
almost
infinite
a
it
is
a
cross
product
of
the
domain
of
all
the
variable
that
form
the
state
For
many
system
the
state
space
can
be
partitioned
into
a
few
state
each
representing
a
logical
combination
of
value
of
different
state
variable
which
share
some
property
of
interest
9
If
the
set
of
state
of
a
system
is
manage-
able
a
state
model
of
the
system
can
be
built
A
state
model
for
a
system
ha
four
component
States
Represent
the
impact
of
the
past
input
to
the
system
Transitions
Represent
how
the
state
of
the
system
change
from
one
state
to
another
in
response
to
some
event
The
state
model
show
what
state
transition
occur
and
what
action
are
performed
in
a
system
in
response
to
event
When
a
state
model
is
built
from
the
requirement
of
a
system
we
can
only
include
the
state
transition
and
action
that
are
stated
in
the
requirement
or
can
be
inferred
from
them
If
more
information
is
available
from
the
design
specification
then
a
richer
state
model
can
be
built
For
example
consider
the
student
survey
example
discussed
in
Chapter
5
According
to
the
requirement
a
system
is
to
be
created
for
taking
a
student
survey
The
student
take
a
survey
and
is
returned
the
current
result
of
the
survey
The
survey
result
can
be
up
to
five
survey
old
We
consider
the
archi-
tecture
which
had
a
cache
between
the
server
and
the
database
and
in
which
the
survey
and
result
are
cached
and
updated
only
after
five
survey
on
arrival
of
a
request
The
proposed
architecture
ha
a
database
at
the
back
which
may
go
down
To
create
a
state
machine
model
of
this
system
we
notice
that
of
a
series
of
the
first
Hence
we
divide
into
two
state
one
representing
the
the
receiving
of
1
state
1
and
the
other
representing
the
receiving
of
request
5
state
2
Next
we
see
that
the
database
can
be
up
or
down
and
it
can
go
down
in
any
of
these
two
state
However
the
behavior
of
request
if
the
database
is
down
may
be
different
Hence
we
create
another
pair
of
state
state
4
Once
the
database
ha
failed
then
the
first
When
a
request
is
received
after
receiving
the
system
enters
a
failed
state
state
5
in
which
it
doe
not
give
any
response
When
the
system
recovers
from
the
failed
state
it
must
update
it
cache
immediately
hence
go
to
state
2
The
state
model
for
this
system
is
shown
in
Figure
i
represents
an
input
from
the
user
for
taking
the
survey
Note
that
we
are
assuming
that
the
state
model
of
the
system
can
be
created
from
it
specification
or
design
This
is
how
most
state
modeling
is
done
and
that
is
how
the
model
wa
built
in
the
example
Once
the
state
model
is
built
we
can
use
it
to
select
test
case
When
the
design
is
implemented
these
test
case
can
be
used
for
testing
the
code
It
is
because
of
this
we
treat
state-based
testing
a
a
black
box
testing
strategy
However
the
state
model
often
requires
information
about
the
design
of
the
system
In
the
example
above
some
knowledge
of
the
architecture
is
utilized
Sometimes
making
the
state
model
may
require
detailed
information
about
the
design
of
the
system
For
example
for
a
class
we
have
seen
that
the
state
modeling
is
done
during
design
and
when
a
lot
is
already
known
about
the
class
it
attribute
and
it
method
Due
to
this
the
state-based
testing
may
be
considered
a
somewhat
between
black-box
and
white-box
testing
Such
strategy
are
sometimes
called
gray-box
testing
Given
a
state
model
of
a
system
how
should
test
case
be
generated
Many
coverage
criterion
have
been
proposed
69
We
discus
only
a
few
here
Suppose
the
set
of
test
case
is
T.
Some
of
the
criterion
are
All
transition
coverage
AT
T
must
ensure
that
every
transition
in
the
state
graph
is
exercised
All
transition
pair
coverage
ATP
T
must
execute
all
pair
of
adja-
cent
transition
An
adjacent
transition
pair
comprises
two
transition
an
incoming
transition
to
a
state
and
an
outgoing
transition
from
that
state
Transition
tree
coverage
TT
T
must
execute
all
simple
path
where
a
simple
path
is
one
which
start
from
the
start
state
and
reach
a
state
that
it
ha
already
visited
in
this
path
or
a
final
state
The
first
criterion
state
that
during
testing
all
transition
get
fired
This
will
also
ensure
that
all
state
are
visited
The
transition
pair
coverage
is
a
stronger
criterion
requiring
that
all
combination
of
incoming
and
outgoing
transition
for
each
state
must
be
exercised
by
T.
If
a
state
ha
two
incoming
transition
t2
and
two
outgoing
transition
t4
then
a
set
of
test
case
T
that
executes
t1
t2
t.
However
to
satisfy
ATP
T
must
also
ensure
execution
of
t1
t2
t3
The
transition
tree
coverage
is
named
in
this
manner
a
a
transition
tree
can
be
constructed
from
the
graph
and
then
used
to
identify
the
path
In
ATP
we
are
going
beyond
transition
and
stating
that
different
path
in
the
state
diagram
should
be
exercised
during
testing
ATP
will
generally
include
AT
For
the
example
above
the
set
of
test
case
for
AT
are
given
below
in
Table
Here
req
mean
that
a
request
for
taking
the
survey
should
be
given
fail
mean
that
the
database
should
be
failed
and
recover
mean
that
the
failed
database
should
be
recovered
As
we
can
see
state-based
testing
draw
attention
to
the
state
and
transi-
tions
Even
in
the
above
simple
case
we
can
see
different
scenario
get
tested
e.g.
system
behavior
when
the
database
fails
and
system
behavior
when
it
fails
and
recovers
thereafter
Many
of
these
scenario
are
easy
to
overlook
if
test
case
are
designed
only
by
looking
at
the
input
domain
The
set
of
test
case
is
richer
if
the
other
criterion
are
used
For
this
example
we
leave
it
a
an
exercise
to
determine
the
test
case
for
other
criterion
In
the
previous
section
we
discussed
black-box
testing
which
is
concerned
with
the
function
that
the
tested
program
is
supposed
to
perform
and
doe
not
deal
with
the
internal
structure
of
the
program
responsible
for
actually
imple-
menting
that
function
Thus
black-box
testing
is
concerned
with
functionality
rather
than
implementation
of
the
program
White-box
testing
on
the
other
hand
is
concerned
with
testing
the
implementation
of
the
program
The
intent
of
this
testing
is
not
to
exercise
all
the
different
input
or
output
condition
al-
though
that
may
be
a
by-product
but
to
exercise
the
different
programming
structure
and
data
structure
used
in
the
program
White-box
testing
is
also
called
structural
testing
and
we
will
use
the
two
term
interchangeably
To
test
the
structure
of
a
program
structural
testing
aim
to
achieve
test
case
that
will
force
the
desired
coverage
of
different
structure
Various
criterion
have
been
proposed
for
this
Unlike
the
criterion
for
functional
testing
which
are
frequently
imprecise
the
criterion
for
structural
testing
are
generally
quite
precise
a
they
are
based
on
program
structure
which
are
formal
and
precise
Here
we
will
discus
one
approach
to
structural
testing
control
flow-based
testing
which
is
most
commonly
used
in
practice
Most
common
structure-based
criterion
are
based
on
the
control
flow
of
the
program
In
these
criterion
the
control
flow
graph
of
a
program
is
considered
and
coverage
of
various
aspect
of
the
graph
are
specified
a
criterion
Hence
before
we
consider
the
criterion
let
u
precisely
define
a
control
flow
graph
for
a
program
Let
the
control
flow
graph
or
simply
flow
graph
of
a
program
P
be
G.
A
node
in
this
graph
represents
a
block
of
statement
that
is
always
executed
together
i.e.
whenever
the
first
statement
is
executed
all
other
statement
are
also
executed
An
edge
i
j
from
node
i
to
node
j
represents
a
possible
transfer
of
control
after
executing
the
last
statement
of
the
block
represented
by
node
i
to
the
first
statement
of
the
block
represented
by
node
j
A
node
corresponding
to
a
block
whose
first
statement
is
the
start
statement
of
P
is
called
the
start
node
of
G
and
a
node
corresponding
to
a
block
whose
last
statement
is
an
exit
statement
is
called
an
exit
node
73
A
path
is
a
finite
sequence
of
node
n1
n2
...
nk
k
>
1
such
that
there
is
an
edge
ni
ni+1
for
all
node
ni
in
the
sequence
except
the
last
node
nk
A
complete
path
is
a
path
whose
first
node
is
the
start
node
and
the
last
node
is
an
exit
node
Now
let
u
consider
control
flow-based
criterion
Perhaps
the
simplest
cover-
age
criterion
is
statement
coverage
which
requires
that
each
statement
of
the
program
be
executed
at
least
once
during
testing
In
other
word
it
requires
that
the
path
executed
during
testing
include
all
the
node
in
the
graph
This
is
also
called
the
all-nodes
criterion
73
This
coverage
criterion
is
not
very
strong
and
can
leave
error
undetected
For
example
if
there
is
an
if
statement
in
the
program
without
having
an
else
clause
the
statement
coverage
criterion
for
this
statement
will
be
satisfied
by
a
test
case
that
evaluates
the
condition
to
true
No
test
case
is
needed
that
ensures
that
the
condition
in
the
if
statement
evaluates
to
false
This
is
a
serious
shortcoming
because
decision
in
program
are
potential
source
of
error
As
an
example
consider
the
following
function
to
compute
the
absolute
value
of
a
number
This
program
is
clearly
wrong
Suppose
we
execute
the
function
with
the
set
of
test
case
x=0
i.e.
the
set
ha
only
one
test
case
The
statement
coverage
criterion
will
be
satisfied
by
testing
with
this
set
but
the
error
will
not
be
revealed
A
more
general
coverage
criterion
is
branch
coverage
which
requires
that
each
edge
in
the
control
flow
graph
be
traversed
at
least
once
during
testing
In
other
word
branch
coverage
requires
that
each
decision
in
the
program
be
evaluated
to
true
and
false
value
at
least
once
during
testing
Testing
based
on
branch
coverage
is
often
called
branch
testing
The
100
%
branch
coverage
criterion
is
also
called
the
all-edges
criterion
73
Branch
coverage
implies
state-
ment
coverage
a
each
statement
is
a
part
of
some
branch
In
the
preceding
example
a
set
of
test
case
satisfying
this
criterion
will
detect
the
error
The
trouble
with
branch
coverage
come
if
a
decision
ha
many
condition
in
it
consisting
of
a
Boolean
expression
with
Boolean
operator
and
and
or
In
such
situation
a
decision
can
evaluate
to
true
and
false
without
actually
exercising
all
the
condition
For
example
consider
the
following
function
that
check
the
validity
of
a
data
item
The
data
item
is
valid
if
it
lie
between
0
and
100
The
module
is
incorrect
a
it
is
checking
for
x
200
instead
of
100
perhaps
a
typing
error
made
by
the
programmer
Suppose
the
module
is
tested
with
the
following
set
of
test
case
x
=
5
x
=
-
The
branch
coverage
criterion
will
be
satisfied
for
this
module
by
this
set
However
the
error
will
not
be
revealed
and
the
behavior
of
the
module
is
consistent
with
it
specification
for
all
test
case
in
this
set
Thus
the
coverage
criterion
is
satisfied
but
the
error
is
not
detected
This
occurs
because
the
decision
is
evaluating
to
true
and
false
because
of
the
condition
x
0
The
condition
x
200
never
evaluates
to
false
during
this
test
hence
the
error
in
this
condition
is
not
revealed
This
problem
can
be
resolved
by
requiring
that
all
condition
evaluate
to
true
and
false
However
situation
can
occur
where
a
decision
may
not
get
both
true
and
false
value
even
if
each
individual
condition
evaluates
to
true
and
false
An
obvious
solution
to
this
problem
is
to
require
decision/condition
coverage
where
all
the
decision
and
all
the
condition
in
the
decision
take
both
true
and
false
value
during
the
course
of
testing
Studies
have
indicated
that
there
are
many
error
whose
presence
is
not
de-
tected
by
branch
testing
because
some
error
are
related
to
some
combination
of
branch
and
their
presence
is
revealed
by
an
execution
that
follows
the
path
that
includes
those
branch
Hence
a
more
general
coverage
criterion
is
one
that
requires
all
possible
path
in
the
control
flow
graph
be
executed
during
testing
This
is
called
the
path
coverage
criterion
or
the
all-paths
criterion
and
the
testing
based
on
this
criterion
is
often
called
path
testing
The
difficulty
with
this
criterion
is
that
program
that
contain
loop
can
have
an
infinite
number
of
possible
path
Furthermore
not
all
path
in
a
graph
may
be
fea-
sible
in
the
sense
that
there
may
not
be
any
input
for
which
the
path
can
be
executed
As
the
path
coverage
criterion
lead
to
a
potentially
infinite
number
of
path
some
effort
have
been
made
to
suggest
criterion
between
the
branch
coverage
and
path
coverage
The
basic
aim
of
these
approach
is
to
select
a
set
of
path
that
ensure
branch
coverage
criterion
and
try
some
other
path
that
may
help
reveal
error
One
method
to
limit
the
number
of
path
is
to
consider
two
path
the
same
if
they
differ
only
in
their
subpaths
that
are
caused
due
to
the
loop
Even
with
this
restriction
the
number
of
path
can
be
extremely
large
It
should
be
pointed
out
that
none
of
these
criterion
is
sufficient
to
detect
all
kind
of
error
in
program
For
example
if
a
program
is
missing
some
control
flow
path
that
are
needed
to
check
for
a
special
value
like
pointer
equal
nil
and
divisor
equal
zero
then
even
executing
all
the
path
will
not
necessarily
detect
the
error
Similarly
if
the
set
of
path
is
such
that
they
satisfy
the
all-
path
criterion
but
exercise
only
one
part
of
a
compound
condition
then
the
set
will
not
reveal
any
error
in
the
part
of
the
condition
that
is
not
exercised
Hence
even
the
path
coverage
criterion
which
is
the
strongest
of
the
criterion
we
have
discussed
is
not
strong
enough
to
guarantee
detection
of
all
the
error
Once
a
coverage
criterion
is
decided
two
problem
have
to
be
solved
to
use
the
chosen
criterion
for
testing
The
first
is
to
decide
if
a
set
of
test
case
satisfy
the
criterion
and
the
second
is
to
generate
a
set
of
test
case
for
a
given
criterion
Deciding
whether
a
set
of
test
case
satisfy
a
criterion
without
the
aid
of
any
tool
is
a
cumbersome
task
though
it
is
theoretically
possible
to
do
manually
For
almost
all
the
structural
testing
technique
tool
are
used
to
determine
whether
the
criterion
ha
been
satisfied
Generally
these
tool
will
provide
feedback
regarding
what
need
to
be
tested
to
fully
satisfy
the
criterion
To
generate
the
test
case
tool
are
not
that
easily
available
and
due
to
the
nature
of
the
problem
i.e.
undecidability
of
feasibility
of
a
path
a
fully
automated
tool
for
selecting
test
case
to
satisfy
a
criterion
is
generally
not
possible
Hence
tool
can
at
best
aid
the
tester
One
method
for
generating
test
case
is
to
randomly
select
test
data
until
the
desired
criterion
is
satisfied
which
is
determined
by
a
tool
This
can
result
in
a
lot
of
redundant
test
case
a
many
test
case
will
exercise
the
same
path
As
test
case
generation
can
not
be
fully
automated
frequently
the
test
case
selection
is
done
manually
by
the
tester
by
performing
structural
testing
in
an
iterative
manner
starting
with
an
initial
test
case
set
and
selecting
more
test
case
based
on
the
feedback
provided
by
the
tool
for
test
case
evaluation
The
test
case
evaluation
tool
can
tell
which
path
need
to
be
executed
or
which
mutant
need
to
be
killed
This
information
can
be
used
to
select
further
test
case
Even
with
the
aid
of
tool
selecting
test
case
is
not
a
simple
process
Selecting
test
case
to
execute
some
part
of
a
yet
unexecuted
code
is
often
very
difficult
Because
of
this
and
for
other
reason
the
criterion
are
often
weakened
For
example
instead
of
requiring
100
%
coverage
of
statement
and
branch
the
goal
might
be
to
achieve
some
acceptably
high
percentage
but
le
than
100
%
There
are
many
tool
available
for
statement
and
branch
coverage
the
crite-
ria
that
are
used
most
often
Both
commercial
and
freeware
tool
are
available
for
different
source
language
These
tool
often
also
give
higher-level
coverage
data
like
function
coverage
method
coverage
and
class
coverage
To
get
the
coverage
data
the
execution
of
the
program
during
testing
ha
to
be
closely
monitored
This
requires
that
the
program
be
instrumented
so
that
required
data
can
be
collected
A
common
method
of
instrumenting
is
to
insert
some
statement
called
probe
in
the
program
The
sole
purpose
of
the
probe
is
to
generate
data
about
program
execution
during
testing
that
can
be
used
to
compute
the
coverage
With
this
we
can
identify
three
phase
in
generating
coverage
data
Probe
insertion
can
be
done
automatically
by
a
preprocessor
The
execution
of
the
program
is
done
by
the
tester
After
testing
the
coverage
data
is
displayed
by
the
tool—sometimes
graphical
representation
are
also
shown
We
have
seen
that
during
testing
the
software
under
test
is
executed
with
a
set
of
test
case
As
the
quality
of
delivered
software
depends
substantially
on
the
quality
of
testing
a
few
natural
question
arise
while
testing
What
is
the
quality
or
reliability
of
software
after
testing
is
completed
During
testing
the
primary
purpose
of
metric
is
to
try
to
answer
these
and
other
related
question
We
will
discus
some
metric
that
may
be
used
for
this
purpose
One
of
the
most
commonly
used
approach
for
evaluating
the
thoroughness
of
testing
is
to
use
some
coverage
measure
We
have
discussed
above
some
of
the
common
coverage
measure
that
are
used
in
practice—statement
coverage
and
branch
coverage
To
use
these
coverage
measure
for
evaluating
the
quality
of
testing
proper
coverage
analysis
tool
will
have
to
be
employed
which
can
inform
not
only
the
coverage
achieved
during
testing
but
also
which
portion
are
not
yet
covered
Often
organization
build
guideline
for
the
level
of
coverage
that
must
be
achieved
during
testing
Generally
the
coverage
requirement
will
be
higher
for
unit
testing
but
lower
for
system
testing
a
it
is
much
more
difficult
to
ensure
execution
of
identified
block
when
the
entire
system
is
being
executed
Often
the
coverage
requirement
at
unit
level
can
be
90
%
to
100
%
keep
in
mind
that
100
%
may
not
be
always
possible
a
there
may
be
unreachable
code
Besides
the
coverage
of
program
construct
coverage
of
requirement
is
also
often
examined
It
is
for
facilitating
this
evaluation
that
in
test
case
specifica-
tion
the
requirement
or
condition
being
tested
is
mentioned
This
coverage
is
generally
established
by
evaluating
the
set
of
test
case
to
ensure
that
sufficient
number
of
test
case
with
suitable
data
are
included
for
all
the
requirement
The
coverage
measure
here
is
the
percentage
of
requirement
or
their
clauses/-
condition
for
which
at
least
one
test
case
exists
Often
a
full
coverage
may
be
required
at
requirement
level
before
testing
is
considered
a
acceptable
After
testing
is
done
and
the
software
is
delivered
the
development
is
con-
sidered
over
It
will
clearly
be
desirable
to
know
in
quantifiable
term
the
reliability
of
the
software
being
delivered
As
reliability
of
software
depends
considerably
on
the
quality
of
testing
by
assessing
reliability
we
can
also
judge
the
quality
of
testing
Alternatively
reliability
estimation
can
be
used
to
decide
whether
enough
testing
ha
been
done
In
other
word
besides
characterizing
an
important
quality
property
of
the
product
being
delivered
reliability
esti-
mation
ha
a
direct
role
in
project
management—it
can
be
used
by
the
project
manager
to
decide
whether
enough
testing
ha
been
done
and
when
to
stop
testing
Reliability
of
a
product
specifies
the
probability
of
failure-free
operation
of
that
product
for
a
given
time
duration
Most
reliability
model
require
that
the
occurrence
of
failure
be
a
random
phenomenon
In
software
even
though
failure
occur
due
to
preexisting
bug
this
assumption
will
generally
hold
for
larger
system
but
may
not
hold
for
small
program
that
have
bug
in
which
case
one
might
be
able
to
predict
the
failure
Hence
reliability
modeling
is
more
meaningful
for
larger
system
Let
X
be
the
random
variable
that
represents
the
life
of
a
system
Reliability
of
a
system
is
the
probability
that
the
system
ha
not
failed
by
time
t.
In
other
word
The
reliability
of
a
system
can
also
be
specified
a
the
mean
time
to
failure
MTTF
MTTF
represents
the
expected
lifetime
of
the
system
From
the
re-
liability
function
it
can
be
obtained
a
80
Reliability
can
also
be
defined
in
term
of
failure
intensity
which
is
the
failure
rate
i.e.
number
of
failure
per
unit
time
of
the
software
at
time
t
From
the
measurement
perspective
during
testing
measuring
failure
rate
is
the
easiest
if
defect
are
being
logged
A
simple
way
to
do
this
is
to
compute
the
number
of
failure
every
week
or
every
day
during
the
last
stage
of
testing
And
number
of
failure
can
be
approximated
by
the
number
of
defect
logged
Though
failure
and
defect
are
different
in
the
last
stage
of
testing
it
is
assumed
that
defect
that
cause
failure
are
fixed
soon
enough
and
therefore
do
not
cause
multiple
failure
Generally
this
failure
rate
increase
in
the
start
of
testing
a
more
and
more
defect
are
found
peak
somewhere
in
the
middle
of
testing
and
then
continues
to
drop
a
fewer
defect
are
reported
For
a
given
test
suite
if
all
defect
are
fixed
then
there
should
be
almost
no
failure
toward
the
end
And
that
could
be
considered
a
proper
time
for
release
of
this
software
That
is
a
release
criterion
could
be
that
the
failure
rate
at
release
time
is
zero
failure
in
some
time
duration
or
zero
failure
while
executing
a
test
suite
Though
failure
rate
tracking
give
a
rough
sense
of
reliability
in
term
of
failure
per
day
or
per
week
for
more
accurate
reliability
estimation
better
model
have
to
be
used
Software
reliability
modeling
is
a
complex
task
requir-
ing
rigorous
model
and
sophisticated
statistical
analysis
Many
model
have
been
proposed
for
software
reliability
assessment
and
a
survey
of
many
of
the
model
is
given
in
33
67
It
should
be
mentioned
that
a
failure
of
software
also
depends
critically
on
the
environment
in
which
it
is
executing
failure
rate
experienced
in
testing
will
reflect
the
ultimate
reliability
experienced
by
the
user
after
software
release
only
if
testing
closely
mimic
the
user
behavior
This
may
not
be
the
case
particularly
with
lower
level
of
testing
However
often
at
higher
level
active
effort
is
made
to
have
the
final
test
suite
mimic
the
actual
usage
If
this
is
the
case
then
reliability
estimation
can
be
applied
with
a
higher
confidence
Another
analysis
of
interest
is
defect
removal
efficiency
though
this
can
only
be
determined
sometime
after
the
software
ha
been
released
The
purpose
of
this
analysis
is
to
evaluate
the
effectiveness
of
the
testing
process
being
employed
not
the
quality
of
testing
for
a
project
This
analysis
is
useful
for
improving
the
testing
process
in
the
future
Usually
after
the
software
ha
been
released
to
the
client
the
client
will
find
defect
which
have
to
be
fixed
generally
by
the
original
developer
a
this
is
often
part
of
the
contract
This
defect
data
is
also
generally
logged
Within
a
few
month
most
of
the
defect
would
be
uncovered
by
the
client
often
the
warranty
period
is
Once
the
total
number
of
defect
or
a
close
approximation
to
the
total
is
known
the
defect
removal
efficiency
DRE
of
testing
can
be
computed
The
defect
removal
efficiency
of
a
quality
control
activity
is
defined
a
the
percentage
reduction
in
the
number
of
defect
by
executing
that
activity
61
As
an
example
suppose
the
total
number
of
defect
logged
is
500
out
of
which
20
were
found
after
delivery
and
200
were
found
during
the
system
testing
The
defect
removal
efficiency
of
system
testing
is
200/220
just
about
90
%
a
the
total
number
of
defect
present
in
the
system
when
testing
started
wa
220
The
defect
removal
efficiency
of
the
overall
quality
process
is
480/500
which
is
96
%
Incidentally
this
level
of
DRE
is
decent
and
is
what
many
commercial
organization
achieve
It
should
be
clear
that
DRE
is
a
general
concept
which
can
be
applied
to
any
defect
removal
activity
For
example
we
can
compute
the
DRE
of
design
review
or
unit
testing
This
can
be
done
if
for
each
defect
besides
logging
when
and
where
the
defect
is
found
the
phase
in
which
the
defect
wa
introduced
is
also
analyzed
and
logged
With
this
information
when
all
the
defect
are
logged
the
DRE
of
the
main
quality
control
task
can
be
determined
This
information
is
extremely
useful
in
improving
the
overall
quality
process
Testing
is
a
dynamic
method
for
verification
and
validation
where
the
soft-
ware
to
be
tested
is
executed
with
carefully
designed
test
case
and
the
behavior
of
the
software
system
is
observed
A
test
case
is
a
set
of
input
and
test
condition
along
with
the
expected
outcome
of
testing
A
test
suite
is
a
set
of
test
case
that
are
generally
executed
together
to
test
some
spe-
cific
behavior
During
testing
only
the
failure
of
the
system
are
observed
from
which
the
presence
of
fault
is
deduced
separate
activity
have
to
be
performed
to
identify
the
fault
and
remove
them
The
intent
of
testing
is
to
increase
confidence
in
the
correctness
of
the
soft-
ware
For
this
the
set
of
test
case
used
for
testing
should
be
such
that
for
any
defect
in
the
system
there
is
likely
to
be
a
test
case
that
will
reveal
it
To
ensure
this
it
is
important
that
the
test
case
are
carefully
designed
with
the
intent
of
revealing
defect
Due
to
the
limitation
of
the
verification
method
for
early
phase
design
and
requirement
fault
also
appear
in
the
code
Testing
is
used
to
detect
these
error
also
in
addition
to
the
error
introduced
during
the
coding
phase
Hence
different
level
of
testing
are
often
used
for
detecting
defect
injected
during
different
stage
The
commonly
employed
testing
level
are
unit
testing
integration
testing
system
testing
and
acceptance
testing
For
testing
a
software
product
overall
testing
should
be
planned
and
for
testing
each
unit
identified
in
the
plan
test
case
should
be
carefully
designed
to
reveal
error
and
specified
in
a
document
or
a
test
script
There
are
two
approach
for
designing
test
case
black-box
and
white-box
In
black-box
testing
the
internal
logic
of
the
system
under
testing
is
not
con-
sidered
and
the
test
case
are
decided
from
the
specification
or
the
require-
ments
Equivalence
class
partitioning
boundary
value
analysis
and
cause-
effect
graphing
are
example
of
method
for
selecting
test
case
for
black-box
testing
State-based
testing
is
another
approach
in
which
the
system
is
mod-
eled
a
a
state
machine
and
then
this
model
is
used
to
select
test
case
using
some
transition
or
path-based
coverage
criterion
State-based
testing
can
also
be
viewed
a
gray-box
testing
in
that
it
often
requires
more
information
than
just
the
requirement
In
white-box
testing
the
test
case
are
decided
based
on
the
internal
logic
of
the
program
being
tested
Often
a
criterion
is
specified
but
the
procedure
for
selecting
test
case
to
satisfy
the
criterion
is
left
to
the
tester
The
most
common
criterion
are
statement
coverage
and
branch
coverage
The
main
metric
of
interest
during
testing
is
the
reliability
of
the
software
under
testing
If
defect
are
being
logged
reliability
can
be
assessed
in
term
of
failure
rate
per
week
or
day
though
better
model
for
estimation
exist
Coverage
achieved
during
testing
and
defect
removal
efficiency
are
other
metric
of
interest
Suppose
you
have
to
test
a
procedure
that
take
two
input
parameter
doe
some
computation
with
them
and
then
manipulates
a
global
table
the
manipulation
itself
depending
on
the
state
of
the
table
What
will
the
complete
specification
of
a
test
case
for
this
procedure
contain
What
are
the
different
level
of
testing
and
the
goal
of
the
different
level
Suppose
for
logging
defect
each
defect
will
be
treated
a
an
object
of
a
class
Defect
Give
the
definition
of
this
class
Suppose
a
software
ha
three
input
each
having
a
defined
valid
range
How
many
test
case
will
you
need
to
test
all
the
boundary
value
For
boundary
value
analysis
if
the
strategy
for
generating
test
case
is
to
consider
all
possible
combination
for
the
different
value
what
will
be
the
set
of
test
case
for
a
software
that
ha
three
input
X
Y
and
Z
Suppose
a
software
ha
five
different
configuration
variable
that
are
set
indepen-
dently
If
three
of
them
are
binary
have
two
possible
value
and
the
rest
have
three
value
how
many
test
case
will
be
needed
if
pairwise
testing
method
is
used
Consider
a
vending
machine
that
take
quarter
and
when
it
ha
received
two
quarter
give
a
can
of
soda
Develop
a
state
model
of
this
system
and
then
generate
set
of
test
case
for
the
various
criterion
Consider
a
simple
text
formatter
problem
Given
a
text
consisting
of
word
sepa-
rated
by
blank
BL
or
newline
NL
character
the
text
formatter
ha
to
covert
it
into
line
so
that
no
line
ha
more
than
MAXPOS
character
break
between
line
occur
at
BL
or
NL
and
the
maximum
possible
number
of
word
are
in
each
line
The
following
program
ha
been
written
for
this
text
formatter
41
outchar
buffer
k
fill
=
fill
+
bufpos
bufpos
=
0
end
Select
a
set
of
test
case
using
the
black-box
testing
approach
Use
a
many
technique
a
possible
and
select
test
case
for
special
case
using
the
error
guessing
method
Select
a
set
of
test
case
that
will
provide
100
%
branch
coverage
Suppose
that
the
last
round
of
testing
in
which
all
the
test
suite
were
executed
but
no
fault
were
fixed
took
2
And
in
this
testing
the
number
of
failure
that
were
logged
every
day
were
2
0
1
2
1
1
0
If
it
is
expected
that
an
average
user
will
use
the
software
for
two
hour
each
day
in
a
manner
that
is
similar
to
what
wa
done
in
testing
what
is
the
expected
reliability
of
this
software
for
the
user
F.
B.
Abreu
and
R.
Carapuca
Candidate
metric
for
object-oriented
soft-
ware
wihin
a
taxonomy
framework
Journal
of
Systems
and
Software
26
1
:87–96
Jan.
1994
V.
R.
Basili
Tutorial
on
model
and
metric
for
software
management
and
engineering
IEEE
Press
1980
V.
R.
Basili
L.
Briand
and
W.
L.
Melo
A
validation
of
object-oriented
design
metric
a
quality
indicator
IEEE
Transactions
on
Software
En-
gineering
22
10
:751–761
Oct.
1996
V.
R.
Basili
and
A.
Turner
Iterative
enhancement
a
practical
technique
for
software
development
IEEE
Transactions
on
Software
Engineering
SE-1
4
Dec.
1975
V.
R.
Basili
and
D.
M.
Weiss
Evaluation
of
a
software
requirement
docu-
ment
by
analysis
of
change
data
In
5th
Int
Conf
on
Software
Engineering
page
314–323
IEEE
1981
L.
Bass
P.
Clements
and
Rick
Kazman
Software
Architecture
in
Practice
Second
Edition
Addison-Wesley
Professional
2003
K.
Beck
Extreme
Programming
Explained
Addison-Wesley
2000
K.
Beck
Test
Driven
Development
by
Example
Addison-Wesley
Profes-
sional
2002
R.V
Binder
Testing
Object-Oriented
Systems—Model
Patterns
and
Tools
Addison-Wesley
1999
B.
Boehm
Software
engineering
IEEE
Transactions
on
Computers
25
12
Dec.
1976
B.
Boehm
Tutorial
software
risk
management
IEEE
Computer
Society
1989
B.
W.
Boehm
Software
Engineering
Economics
Prentice
Hall
Englewood
Cliffs
NJ
1981
B.
W.
Boehm
Software
engineering
economics
IEEE
Transactions
on
Software
Engineering
10
1
:135–152
Jan.
1984
B.
W.
Boehm
Improving
software
productivity
IEEE
Computer
page
43–57
Sept.
1987
G.
Booch
Object-Oriented
Analysis
and
Design
The
Benjamin/Cum-
ming
Publishing
Company
1994
F.
Brooks
The
Mytical
Man
Month
Addison-Wesley
1975
N.
Brown
Industrial-strength
management
strategy
IEEE
Software
July
1996
R.N
Charette
Software
Engineering
Risk
Analysis
and
Management
Mc-
Graw
Hill
1989
R.N
Charette
Large-scale
project
management
is
risk
management
IEEE
Software
July
1996
E.
Chen
Program
complexity
and
programmer
productivity
IEEE
Trans-
action
on
Software
Engineering
SE-4:187–194
May
1978
S.
R.
Chidamber
and
C.
F.
Kemerer
A
metric
suite
for
object-oriented
design
IEEE
Transactions
on
Software
Engineering
20
6
:476–493
June
1994
T.S
Chow
Testing
software
design
modeled
by
finite
state
machine
IEEE
Transactions
on
Software
Engineering
SE-4
3
:178–187
1978
P.
Clements
F.
Bachmann
L.
Bass
D.
Garlan
J.
Ivers
R.
Little
R.
Nord
and
J.
Stafford
Documenting
Software
Architectures
Views
and
Beyond
Addison-Wesley
2003
A.
Cockburn
Writing
Effective
Use
Cases
Addison-Wesley
2001
D.M
Cohen
S.R
Dalal
M.L
Fredman
and
G.C
Patton
The
AETG
system
An
approach
to
testing
based
on
combinatorial
design
IEEE
Transactions
on
Software
Engineering
23
7
:437–443
1997
S.
D.
Conte
H.
E.
Dunsmore
and
V.
Y.
Shen
Software
Engineering
Met-
rics
and
Models
The
Benjamin/Cummings
Publishing
Company
1986
J.
S.
Davis
Identification
of
error
in
software
requirement
through
use
of
automated
requirement
tool
Information
and
Software
Technology
31
9
:472–476
Nov.
1989
T.
DeMarco
Structured
Analysis
and
System
Specification
Yourdon
Press
1979
L.
Dobrica
and
E.
Niemela
A
survey
on
software
architecture
analy-
si
method
IEEE
Transactions
on
Software
Engineering
28
7
:638–653
2002
J.
Eder
G.
Kappel
and
M.
Schrefl
Coupling
and
cohesion
in
object-
oriented
system
Technical
report
University
of
Klagenfurt
1994
M.
E.
Fagan
Design
and
code
inspection
to
reduce
error
in
program
development
IBM
System
Journal
3
:182–211
1976
M.
E.
Fagan
Advances
in
software
inspection
IEEE
Transactions
on
Software
Engineering
12
7
:744–751
July
1986
W.
Farr
Software
reliability
modeling
survey
In
M.
R.
Lyu
editor
Software
Reliability
Engineering
page
71–117
McGraw
Hill
and
IEEE
Computer
Society
1996
S.
I.
Feldman
Make—a
program
for
maintaining
computer
program
Software
Practice
and
Experience
9
3
:255–265
March
1979
M.
Fowler
UML
Distilled—A
Brief
Guide
to
the
Standard
Object
Modeling
Language
Addison-Wesley
Professional
2003
M.
Fowler
K.
Beck
J.
Brant
W.
Opdyke
and
D.
Roberts
Refactoring
Improving
the
Design
of
Existing
Code
Addison-Wesley
1999
D.
P.
Freedman
and
G.
M.
Weinberg
Handbook
of
Walkthroughs
Inspec-
tions
and
Technical
Reviews—Evaluating
Programs
Projects
and
Prod-
ucts
Dorset
House
1990
E.
Gamma
R.
Helm
R.
Johnson
and
J.
Vlissides
Design
Patterns—
Elements
of
Reusable
Object-Oriented
Software
Addison-Wesley
Profes-
sional
1995
T.
Gilb
and
D.
Graham
Software
Inspection
Addison-Wesley
1993
H.
Gomma
and
D.
B.
H.
Scott
Prototyping
a
a
tool
in
the
specification
of
user
requirement
In
Fifth
Int
Conf
on
Software
Engineering
page
333–341
1981
J.
Goodenough
and
S.
L.
Gerhart
Towards
a
theory
of
test
data
selection
IEEE
Transactions
on
Software
Engineering
SE-1:156–173
1975
S.
E.
Goodman
and
S.
T.
Hedetniemi
Introduction
to
the
Design
and
Analysis
of
Algorithms
McGraw-Hill
1977
R.
Grady
and
D.
Caswell
Software
Metrics
Establishing
a
Company-wide
Program
Prentice
Hall
1987
R.
B.
Grady
and
T.
V.
Slack
Key
lesson
learned
in
achieving
widespread
inspection
use
IEEE
Software
page
48–57
July
1994
E.M.
Hall
Managing
Risk
Methods
for
Software
Development
and
En-
hancement
Addison-Wesley
1998
M.
Halstead
Elements
of
Software
Science
Elsevier
North-Holland
1977
W.
Harrison
K.
Magel
R.
Kluczny
and
A.
DeKock
Applying
software
complexity
metric
to
program
maintenance
IEEE
Computer
page
65–
79
Sept.
1982
S.
Henry
and
D.
Kafura
Software
structure
metric
based
on
information
flow
IEEE
Transactions
on
Software
Engineering
7
5
:510–518
1981
S.
Henry
and
D.
Kafura
The
evaluation
of
software
system
’
structure
using
quantitative
software
metric
Software
Practice
and
Experience
14
6
:561–573
June
1984
C.
A.
R.
Hoare
An
axiomatic
basis
for
computer
programming
Commu-
nications
of
the
ACM
12
3
:335–355
1969
IBM-Rational
Rational
unified
process
best
practice
for
software
devel-
opment
team
Technical
report
IBMwebsite
1993
IEEE
IEEE
standard
glossary
of
software
engineering
terminology
Tech-
nical
report
1990
IEEE
IEEE
recommended
practice
for
software
requirement
specifica-
tions
Technical
report
1998
IEEE
IEEE
recommended
practice
for
architectural
description
of
software-intensive
system
Technical
Report
1471-2000
2000
International
Standards
Organization
Software
engineering—product
quality
part
1
Quality
model
Technical
Report
ISO9126-1
2001
I.
Jacobson
Object-oriented
Software
Engineering—A
Use
Case
Driven
Approach
Addison-Wesley
1992
P.
Jalote
CMM
in
Practice—Processes
for
Executing
Software
Projects
at
Infosys
Addison-Wesley
1999
P.
Jalote
Software
Project
Management
in
Practice
Addison-Wesley
2002
P.
Jalote
A.
Palit
and
P.
Kurien
The
timeboxing
process
model
for
iterative
software
development
In
Advances
in
Computers
Vol
62
page
67–103
Academic
Press
2004
P.
Jalote
A.
Palit
P.
Kurien
and
V.
T.
Peethamber
Timeboxing
A
process
model
for
iterative
software
development
The
Journal
of
Systems
and
Software
70:117–127
S.H
Kan.
Metrics
and
Models
in
Software
Quality
Engineering
Addison-
Wesley
1995
T.
Korson
and
J.
D.
Gregor
Understanding
object-oriented
A
unifying
paradigm
Communications
of
the
ACM
33
9
:40–60
Sept.
1990
P.
Kruchten
The
Rational
Unified
Process
Addison-Wesley
1999
W.
Lie
and
S.
Henry
Object-oriented
metric
that
predict
maintainability
Journal
of
Systems
and
Software
23
2
:111–122
1993
B.
Liskov
Data
abstraction
and
hierarchy
SIGPLAN
Notices
23
5
May
1988
B.
Meyer
Object
Oriented
Software
Construction
Prentice
Hall
1988
J.
D.
Musa
A.
Iannino
and
K.
Okumoto
Software
Reliability—
Measurement
Prediction
Application
McGraw
Hill
1987
G.
Myers
The
Art
of
Software
Testing
Wiley-Interscience
New
York
1979
J.
Offutt
S.
Liu
A.
Abdurazik
and
P.
Ammann
Generating
test
data
from
state-based
specification
The
Journal
of
Software
Testing
Verifi-
cation
and
Reliability
13
1
:25–53
March
2003
M.S
Phadke
Planning
efficient
software
test
Crosstalk
Oct
1997
L.
H.
Putnam
A
general
empirical
solution
to
the
macro
software
sizing
and
estimation
problem
IEEE
Transactions
on
Software
Engineering
SE-4:345–361
July
1978
L.
H.
Putnam
and
W.
Myers
Industrial
Strength
Software
Effective
Man-
agement
Using
Measurement
IEEE
Computer
Society
1997
S.
Rapps
and
E.
J.
Weyuker
Selecting
software
test
data
using
data
flow
information
IEEE
Transactions
on
Software
Engineering
11
4
:367–375
Apr
1985
W.
W.
Royce
Managing
the
development
of
large
software
system
In
Proc
9th
Int
Conf
on
Software
Engineering
ICSE-9
originally
in
IEEE
Wescon
Aug
1970
page
328–338
IEEE
1987
SEI
Software
Engineering
Institute
The
Capability
Maturity
Model
Guidelines
for
Improving
the
Software
Process
Addison-Wesley
1995
M.
Shaw
and
D.
Garlan
Software
Architecture
Perspectives
on
an
Emerg-
ing
Discipline
Prentice
Hall
1996
M.
D.
Smith
and
D.
J.
Robson
Object
oriented
programming
The
prob-
lem
of
validation
Proc
of
6th
Int
IEEE
Conference
on
Software
Main-
tenance
page
272–282
Nov.
1990
M.
D.
Smith
and
D.
J.
Robson
A
framework
for
testing
object-oriented
program
Journal
of
Object-Oriented
Programming
page
45–53
June
1992
W.
P.
Stevens
G.
J.
Myers
and
L.
Constantine
Structured
design
IBM
Systems
Journal
13
2
1974
K.
S.
Trivedi
Probability
and
Statistics
with
Reliability
Queuing
and
Computer
Science
Applications
Second
Edition
Wiley-Interscience
2002
C.
Watson
and
C.
Felix
A
method
of
programming
measurement
and
estimation
IBM
Systems
Journal
16
1
Jan.
1977
G.
M.
Weinberg
and
E.
L.
Schulman
Goals
and
performance
in
computer
programming
Human
Factors
16
1
:70–77
1974
E.
F.
Weller
Lessons
learned
from
three
year
of
inspection
data
IEEE
Software
page
38–53
Sept.
1993
N.
Wirth
Program
development
by
stepwise
refinement
Communications
of
the
ACM
14
4
:221–227
April
1971
M.
Woodward
M.
Hennell
and
D.
Hedley
A
measure
of
control
flow
complexity
in
program
text
IEEE
Transactions
on
Software
Engineering
SE-5:45–50
Jan.
1979
R.
T.
Yeh
and
P.
Zave
Specifying
software
requirement
Proceedings
of
the
IEEE
68
9
:1077–1088
Sept.
1980
B.
H.
Yin
and
J.
W.
Winchester
The
establishment
and
use
of
measure
to
evaluate
the
quality
of
design
Software
Engineering
Notes
3:45–52
1978
E.
Yourdon
and
L.
Constantine
Structured
Design
Prentice
Hall
1979
W.
M.
Zage
and
D.
M.
Zage
Evaluating
design
metric
on
large-scale
software
IEEE
Software
page
75–81
July
1993
transition
to
specification
4
see
Software
architecture
Architecture
description
language
117
an
example
240
Branch
testing
24
see
Fault
Error
Build
process
200
Client-server
interaction
of
object
14-server
style
112
Code
inspection
see
Inspection
process
Coding
181
Configuration
management
see
Software
configuration
management
waterfall
model
see
Waterfall
model
Dynamic
binding
147
Effort
estimation
see
Estimation
Encapsulation
143
Functional
testing
see
Black-box
testing
Functionality
4
test-driven
development
19-strength
software
17
17
186
187
Object
modeling
technique
15-oriented
analysis
15-oriented
design
142
word
counting
example
16-closed
principle
12
174
Pre-condition
of
program
18
see
Analysis
Problem
partitioning
58
monitoring
and
control
see
Project
monitoring
and
control
scheduling
see
Project
scheduling
Project
monitoring
and
control
33
86
defect
injection
and
removal
cycle
7
Q
&
P
3
4
mean
time
to
failure
25
see
Analysis
Requirement
change
6
use
case
see
Use
case
Requirement
validation
63
Requirements
see
Software
requirement
Requirements
review
65
Reviews
see
Inspection
process
Risk
management
80
92
Single-entry
single-exit
construct
18-mode
fault
241
Software
engineering
process
group
1
see
Inspection
Software
reliability
see
Reliability
Software
requirement
process
3
see
Requirement
specification
Software
requirement
37
41
66
version
maintenance
200
Specification
language
46
Structural
testing
see
White-box
testing
Structure
chart
132
137
object-oriented
design
see
Object-
oriented
design
verification
see
Design
verification
System
testing
230
Unified
modeling
language
see
UML
Unit
testing
204
229